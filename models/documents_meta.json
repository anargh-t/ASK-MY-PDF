{
  "62bd5715-6e2e-48a9-90f2-8ec97700257f": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "7220e5c0-2333-4632-88b3-1cb5b518d47c": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "121e909d-5014-43d5-91d4-86212417c19f": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "05b0b6a4-40c5-4122-b244-6a576cf56c5d": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "e49aed1e-22dc-4673-913e-8ecf9d4d2dc3": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "0a23b352-8d1b-442a-bafa-813e66456fbe": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "304f1742-839a-424f-98ac-678e019ab83f": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "9c8158ec-6b5e-4586-9f45-6cdfbabe9113": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "fa1296ac-73e5-405a-8b16-80939340cf83": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "af1c896c-d0b5-4007-bcc2-5a2da427768f": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "a8152c1b-0a8a-4f8d-9c87-847e1493bbb1": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "da27028a-17ea-4390-87f5-d22f092a5449": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "6d0683d7-fb10-412a-97d5-f26cc89b1629": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "02fe0b67-844c-43de-a521-82d9346e2ccd": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "d05fe906-8e0c-4b50-99e0-64c929c81be9": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "94843540-ea96-444c-a198-20032c7b9b30": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "d8a148c9-7a08-4d61-9cd9-2daf0ddce6f0": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "cbb02ffd-c1b3-4479-b0e2-23dd978beb57": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "27ff32aa-0488-4996-80ba-67b974137918": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "4fb084bd-edf8-4c38-b53e-f887e4c68154": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "b147d7ef-1520-435c-b506-6b8b69efde27": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "4f7f63c8-b8f2-42ff-9a87-5315f4b4e20d": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "2beb6d48-08a0-465b-a365-c6b7c547d6ae": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "c0f73802-a53a-453c-a49f-0ea649af6814": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "fbd189fc-b475-469e-8bac-e7f160646dce": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "1ecc6cf7-1781-42d9-8cc8-5a887b21b1fe": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "befbb262-ec7f-44c3-a3ce-cf02427fc80c": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "4f2b4e39-3266-4fbf-9c0b-fff21b2d1ad7": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "0d7e5960-0377-433f-ab59-7db357a97367": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "c1b39f51-c7f9-45f8-9339-08577c2d7827": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "d87c8f74-4b38-4d5c-beac-e916941a1043": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "3a6bac03-43eb-48c6-929e-8bae9a810bff": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "514ab5b9-e896-45fe-aa0f-3c1f83f37c01": {
    "filename": "MobSF_Assignment 2.pdf",
    "text": "Mobile Security Framework (MobSF) To perform of a mobile application (APK/IPA) using the and interpret the results. I clone Mobile Security Framework (MobSF): https://github.com/MobSF/Mobile- Security-Framework-MobSF and install and configure it with docker Login Screen of MobSF Default Username and password – mobsf:mobsf Next we upload an android apk file for analyze for that I already download a apk from the external source : SuperVPN Pro_1.9.2_APKPure.apk APP SCORES • Security Score: 52/100 • Trackers Detected: 3/32 • Button: MobiSF Scorecard FILE INFORMATION • File Name: SuperVPN_Pro_1.9.2_APKPure.apk • Size: 20.4 MB • MD5: bc2678292e0befdd1daa221558bf339 • SHA1: 760498738dfeb6d6a2064eaa68fb76fd65d614c9 • SHA256: 3965132062c85d5640fb9d02705dc85b813ca2a927892c16c7644c1b553edae APP INFORMATION • App Name: SuperVPN Pro • Package Name: com.jrzheng.supervnpayment • Main Activity: com.jrzheng.supervnpayment.activity.MainActivity • Target SDK: 34 • Min SDK: 23 • Max SDK: (blank / not listed) • Android Version Name: 1.9.2 • Android Version Code: 52 PLAYSTORE INFORMATION • Title: SuperVPN Pro • Score: 4.6 (113K reviews) • Installs: 10,000,000+ • Price: Android Version Support: Category Productivity • Play Store URL: com.jrzheng.supervnpayment • Developer: SuperSoftTech • Developer 2: SuperSoftTech • Developer Address: None • Developer Website: https://www.supervpnpro.app • Developer Email: goanalyticsgpp@gmail.com • Release Date: Apr 11, 2014 • Privacy Policy: Privacy link Description (as in screenshot) SuperVPN Pro, total free VPN service without any limitation. Easy to use, one click to connect to VPN server. Unlimited bandwidth and unlimited free time. Fast servers across the world. Features: • Protect your privacy, keep you safe from 3rd party tracking • Unblock geographically restricted websites • No registration required, no settings required • No speed limitation, no bandwidth limitation • One-click to connect to VPN • No root access needed • Encrypt your internet traffic • Top server speed & reliability • Using most secure VPN solution The app is free forever. There is no need to purchase if the free version is good enough for you. Purchasing VIP will remove ads, give you faster speed and better experience. You can cancel VIP at any time and get a full refund. EXPORTED COMPONENTS • Exported Activities: 0 / 20 • Exported Services: 2 / 14 • Exported Receivers: 2 / 11 • Exported Providers: 0 / 4 Application Permissions These are the permissions the app requests from Android. Most are normal network-related permissions, but a couple stand out: – It can query all installed apps. – It can post notifications. Overall, it leans heavily into advertising-related permissions. Android API Usage This shows which Android APIs the app actively uses. It uses crypto, Base64 encoding/decoding, certificate handling, dynamic class loading, OS command execution, and advertising ID access — a mix often seen in VPNs but also potentially in shady apps. Certificate Analysis The APK is signed using outdated or weak signature schemes. Two warnings appear: – Vulnerable to the Janus signing flaw. – Uses SHA-1 in parts of the signing chain, which is collision-prone. This doesn’t prove malicious intent, but it weakens trust in the app’s integrity. App Issues These are weaknesses found in the app’s configuration. It shows outdated Android support, clear-text traffic enabled, backup enabled, and several components that expose permissions incorrectly — all signs of sloppy security hygiene. Code Analysis These findings come from scanning the app’s source code. Issues include weak cryptography, insecure random numbers, logging sensitive data, hardcoded secrets, SQL injection risk, and MD5 usage. Only a couple of items are marked secure, like SSL pinning. Behaviour Analysis This lists what the app actually does during execution. It reads files, parses JSON, opens local files, connects to remote URLs, queries installed apps, uses reflection, triggers implicit intents, and retrieves SIM country data. The behaviors match a networked app with analytics and tracking capabilities. Conclusion The whole picture points to an app that works, but cuts corners everywhere that matters for security. The permissions are broad, the APIs used lean toward tracking and dynamic loading, the certificates are outdated, and the code base mixes weak crypto, insecure randomness, clear-text traffic, and sloppy logging. On top of that, several components expose permissions incorrectly, making them accessible to other apps. The behavior analysis shows it probes the device, reaches out to remote hosts, inspects installed packages, and pulls SIM data. None of these items alone guarantee malicious intent, but together they form a pattern: the app is built with minimal security discipline and maximum data access. It’s the kind of VPN product that may function on the surface but introduces more risk than it removes. A secure VPN should reduce your exposure; this one expands it. For someone studying or analyzing app security, it’s a solid example of how technical debt, weak crypto, and unnecessary permissions converge into a high-risk footprint.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "d3199ffd-8bd5-4280-a26b-81909529aa50": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "042b06e2-0ec1-42d7-8fbb-8eb014d0b2f3": {
    "filename": "MobSF_Assignment 3.pdf",
    "text": "Mobile Security Framework (MobSF) To perform of a mobile application (APK/IPA) using the and interpret the results. I clone Mobile Security Framework (MobSF): https://github.com/MobSF/Mobile- Security-Framework-MobSF and install and configure it with docker Login Screen of MobSF Default Username and password – mobsf:mobsf Next we upload an android apk file for analyze for that I already download a apk from the external source : SuperVPN Pro_1.9.2_APKPure.apk APP SCORES • Security Score: 52/100 • Trackers Detected: 3/32 • Button: MobiSF Scorecard FILE INFORMATION • File Name: SuperVPN_Pro_1.9.2_APKPure.apk • Size: 20.4 MB • MD5: bc2678292e0befdd1daa221558bf339 • SHA1: 760498738dfeb6d6a2064eaa68fb76fd65d614c9 • SHA256: 3965132062c85d5640fb9d02705dc85b813ca2a927892c16c7644c1b553edae APP INFORMATION • App Name: SuperVPN Pro • Package Name: com.jrzheng.supervnpayment • Main Activity: com.jrzheng.supervnpayment.activity.MainActivity • Target SDK: 34 • Min SDK: 23 • Max SDK: (blank / not listed) • Android Version Name: 1.9.2 • Android Version Code: 52 PLAYSTORE INFORMATION • Title: SuperVPN Pro • Score: 4.6 (113K reviews) • Installs: 10,000,000+ • Price: Android Version Support: Category Productivity • Play Store URL: com.jrzheng.supervnpayment • Developer: SuperSoftTech • Developer 2: SuperSoftTech • Developer Address: None • Developer Website: https://www.supervpnpro.app • Developer Email: goanalyticsgpp@gmail.com • Release Date: Apr 11, 2014 • Privacy Policy: Privacy link Description (as in screenshot) SuperVPN Pro, total free VPN service without any limitation. Easy to use, one click to connect to VPN server. Unlimited bandwidth and unlimited free time. Fast servers across the world. Features: • Protect your privacy, keep you safe from 3rd party tracking • Unblock geographically restricted websites • No registration required, no settings required • No speed limitation, no bandwidth limitation • One-click to connect to VPN • No root access needed • Encrypt your internet traffic • Top server speed & reliability • Using most secure VPN solution The app is free forever. There is no need to purchase if the free version is good enough for you. Purchasing VIP will remove ads, give you faster speed and better experience. You can cancel VIP at any time and get a full refund. EXPORTED COMPONENTS • Exported Activities: 0 / 20 • Exported Services: 2 / 14 • Exported Receivers: 2 / 11 • Exported Providers: 0 / 4 Application Permissions These are the permissions the app requests from Android. Most are normal network-related permissions, but a couple stand out: – It can query all installed apps. – It can post notifications. Overall, it leans heavily into advertising-related permissions. Android API Usage This shows which Android APIs the app actively uses. It uses crypto, Base64 encoding/decoding, certificate handling, dynamic class loading, OS command execution, and advertising ID access — a mix often seen in VPNs but also potentially in shady apps. Certificate Analysis The APK is signed using outdated or weak signature schemes. Two warnings appear: – Vulnerable to the Janus signing flaw. – Uses SHA-1 in parts of the signing chain, which is collision-prone. This doesn’t prove malicious intent, but it weakens trust in the app’s integrity. App Issues These are weaknesses found in the app’s configuration. It shows outdated Android support, clear-text traffic enabled, backup enabled, and several components that expose permissions incorrectly — all signs of sloppy security hygiene. Code Analysis These findings come from scanning the app’s source code. Issues include weak cryptography, insecure random numbers, logging sensitive data, hardcoded secrets, SQL injection risk, and MD5 usage. Only a couple of items are marked secure, like SSL pinning. Behaviour Analysis This lists what the app actually does during execution. It reads files, parses JSON, opens local files, connects to remote URLs, queries installed apps, uses reflection, triggers implicit intents, and retrieves SIM country data. The behaviors match a networked app with analytics and tracking capabilities. Conclusion The whole picture points to an app that works, but cuts corners everywhere that matters for security. The permissions are broad, the APIs used lean toward tracking and dynamic loading, the certificates are outdated, and the code base mixes weak crypto, insecure randomness, clear-text traffic, and sloppy logging. On top of that, several components expose permissions incorrectly, making them accessible to other apps. The behavior analysis shows it probes the device, reaches out to remote hosts, inspects installed packages, and pulls SIM data. None of these items alone guarantee malicious intent, but together they form a pattern: the app is built with minimal security discipline and maximum data access. It’s the kind of VPN product that may function on the surface but introduces more risk than it removes. A secure VPN should reduce your exposure; this one expands it. For someone studying or analyzing app security, it’s a solid example of how technical debt, weak crypto, and unnecessary permissions converge into a high-risk footprint.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "80315d2f-d7a0-4e2d-9801-5ab408a01bc9": {
    "filename": "MobSF_Assignment 3.pdf",
    "text": "Mobile Security Framework (MobSF) To perform of a mobile application (APK/IPA) using the and interpret the results. I clone Mobile Security Framework (MobSF): https://github.com/MobSF/Mobile- Security-Framework-MobSF and install and configure it with docker Login Screen of MobSF Default Username and password – mobsf:mobsf Next we upload an android apk file for analyze for that I already download a apk from the external source : SuperVPN Pro_1.9.2_APKPure.apk APP SCORES • Security Score: 52/100 • Trackers Detected: 3/32 • Button: MobiSF Scorecard FILE INFORMATION • File Name: SuperVPN_Pro_1.9.2_APKPure.apk • Size: 20.4 MB • MD5: bc2678292e0befdd1daa221558bf339 • SHA1: 760498738dfeb6d6a2064eaa68fb76fd65d614c9 • SHA256: 3965132062c85d5640fb9d02705dc85b813ca2a927892c16c7644c1b553edae APP INFORMATION • App Name: SuperVPN Pro • Package Name: com.jrzheng.supervnpayment • Main Activity: com.jrzheng.supervnpayment.activity.MainActivity • Target SDK: 34 • Min SDK: 23 • Max SDK: (blank / not listed) • Android Version Name: 1.9.2 • Android Version Code: 52 PLAYSTORE INFORMATION • Title: SuperVPN Pro • Score: 4.6 (113K reviews) • Installs: 10,000,000+ • Price: Android Version Support: Category Productivity • Play Store URL: com.jrzheng.supervnpayment • Developer: SuperSoftTech • Developer 2: SuperSoftTech • Developer Address: None • Developer Website: https://www.supervpnpro.app • Developer Email: goanalyticsgpp@gmail.com • Release Date: Apr 11, 2014 • Privacy Policy: Privacy link Description (as in screenshot) SuperVPN Pro, total free VPN service without any limitation. Easy to use, one click to connect to VPN server. Unlimited bandwidth and unlimited free time. Fast servers across the world. Features: • Protect your privacy, keep you safe from 3rd party tracking • Unblock geographically restricted websites • No registration required, no settings required • No speed limitation, no bandwidth limitation • One-click to connect to VPN • No root access needed • Encrypt your internet traffic • Top server speed & reliability • Using most secure VPN solution The app is free forever. There is no need to purchase if the free version is good enough for you. Purchasing VIP will remove ads, give you faster speed and better experience. You can cancel VIP at any time and get a full refund. EXPORTED COMPONENTS • Exported Activities: 0 / 20 • Exported Services: 2 / 14 • Exported Receivers: 2 / 11 • Exported Providers: 0 / 4 Application Permissions These are the permissions the app requests from Android. Most are normal network-related permissions, but a couple stand out: – It can query all installed apps. – It can post notifications. Overall, it leans heavily into advertising-related permissions. Android API Usage This shows which Android APIs the app actively uses. It uses crypto, Base64 encoding/decoding, certificate handling, dynamic class loading, OS command execution, and advertising ID access — a mix often seen in VPNs but also potentially in shady apps. Certificate Analysis The APK is signed using outdated or weak signature schemes. Two warnings appear: – Vulnerable to the Janus signing flaw. – Uses SHA-1 in parts of the signing chain, which is collision-prone. This doesn’t prove malicious intent, but it weakens trust in the app’s integrity. App Issues These are weaknesses found in the app’s configuration. It shows outdated Android support, clear-text traffic enabled, backup enabled, and several components that expose permissions incorrectly — all signs of sloppy security hygiene. Code Analysis These findings come from scanning the app’s source code. Issues include weak cryptography, insecure random numbers, logging sensitive data, hardcoded secrets, SQL injection risk, and MD5 usage. Only a couple of items are marked secure, like SSL pinning. Behaviour Analysis This lists what the app actually does during execution. It reads files, parses JSON, opens local files, connects to remote URLs, queries installed apps, uses reflection, triggers implicit intents, and retrieves SIM country data. The behaviors match a networked app with analytics and tracking capabilities. Conclusion The whole picture points to an app that works, but cuts corners everywhere that matters for security. The permissions are broad, the APIs used lean toward tracking and dynamic loading, the certificates are outdated, and the code base mixes weak crypto, insecure randomness, clear-text traffic, and sloppy logging. On top of that, several components expose permissions incorrectly, making them accessible to other apps. The behavior analysis shows it probes the device, reaches out to remote hosts, inspects installed packages, and pulls SIM data. None of these items alone guarantee malicious intent, but together they form a pattern: the app is built with minimal security discipline and maximum data access. It’s the kind of VPN product that may function on the surface but introduces more risk than it removes. A secure VPN should reduce your exposure; this one expands it. For someone studying or analyzing app security, it’s a solid example of how technical debt, weak crypto, and unnecessary permissions converge into a high-risk footprint.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "ad05d15c-6de1-4b75-865c-077ac8a672dd": {
    "filename": "Lab Exam 1 - Static Analysis of Malware .pdf",
    "text": "LAB EXAM MODEL 1 Static Analysis of Malware The Downloaded malware file is DanaBot.exe 1. File Information: a. file type : executable b. Size : 2795520 bytes c. MD5 hash : 4D 5A 90 00 03 00 00 00 04 00 00 00 FF FF 00 00 B8 00 00 00 00 00 00 00 40 00 00 00 00 00 00 00 00 d. SHA256 hash: DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 e. creation date : Thu Mar 14 19:36:48 2019 | UTC The analyzed file, DanaBot.exe , is a 32-bit Windows executable with a size of approximately 2.79 MB. The compilation timestamp indicates it was created on March 14, 2019. The SHA256 hash, which uniquely identifies the file, is DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 Tools used: Pestudio 2. String Analysis: a. Use a string extraction tool to identify readable text. Tool Used: FLOSS 3. Categories useful strings a. URLs b. IP addresses c. Domains d. file paths e. function names f. suspicious strings g. DLL information Analysis with the FLOSS tool revealed a large number of static strings (33,058), but a conspicuous absence of decoded strings. Further inspection of the output showed no clear-text URLs, IP addresses, or domains. The strings consist mostly of standard library and function names, with no immediately obvious suspicious indicators. This lack of meaningful, human-readable indicators is a strong sign that the malware's configuration and critical data (like C2 server addresses) are obfuscated or encrypted within the binary. 4. PE Header Analysis (for PE files): a. Check whether the sample is a Portable Executable (PE). The file is confirmed to be a valid Portable Executable (PE), as identified by the \"MZ\" magic bytes at the beginning of the file. The PE header defines five sections: .text, .rdata, .data, .tls, and .rsrc. Analysis of the section properties reveals significant anomalies. The .text, .rdata, and .data sections all show a Virtual Size that is substantially larger than their Raw Size. For example, the .text section has a raw size of F800 bytes on disk but expands to F7FD bytes in memory. This discrepancy is a classic indicator of a packed executable. The malware code is likely compressed or encrypted on disk and will be unpacked into its larger, true form in memory during execution. Tools Used: PE-Bear It loads, it's a Portable Executable (PE). The first two letters you see in the hex view should be \"MZ\"—the signature of a DOS header, which is part of the PE format. 5. Analyse the PE header. Identify sections a. .text : b. .data c. .rdata d. inspect their sizes and properties. A large difference where the VirtualSize is much bigger than the SizeOfRawData can be a red flag. It often means the section is compressed or un-initialized and will be expanded in memory when the program runs, a common sign of packing. 6. Extract information about imported and exported functions. Exports Imports The malware's export table contains three non-descript functions, which is unusual for a malicious executable and likely a byproduct of its packer. The import table, however, is highly revealing. It shows the program imports numerous functions from standard Windows libraries like KERNEL32.dll and ADVAPI32.dll. Pestudio flags over 88 imported functions as suspicious. These imports provide critical clues about the malware's capabilities, including reconnaissance (GetVersionExA), memory manipulation (HeapAlloc), and system information gathering (GetVolumeNameForVolumeMountPointA). These functions will be analyzed in greater detail to identify behavioral patterns. Tools: PE-bear for structure, Pestudio for a clean view of imports/exports. 7. Obfuscation/Encryption Detection: a. Identify any signs of code obfuscation or encryption within the binary. If the file is not packed (or DiE can see through the packing), it will identify the compiler used to create the program, such as Compiler: Microsoft Visual C++. 8. Section Entropy Analysis a. Use a PE analysis tool to calculate the entropy of each section (.text, .data, .rdata, etc.). b. Interpret the entropy values to determine whether any section indicates potential packing or encryption. A high, flat line indicates high entropy, which is a strong sign of packed or encrypted data, even if DiE can't name the specific packer. A high entropy value (generally above 7.0) suggests the data is compressed or encrypted. Normal code and data have lower entropy. If you see a section with high entropy, it's likely packed. Tools: Detect It Easy (DiE), Pestudio. While Detect It Easy (DiE) identifies the compiler as Microsoft Visual C++ and does not flag a specific packer, the entropy analysis provides definitive evidence of obfuscation. The overall entropy of the file as calculated by Pestudio is 7.950, which is extremely high. An entropy value above 7.0 strongly suggests that the data is not normal code but is instead compressed or encrypted. The entropy graph from PE-bear confirms this, showing a high and uniform entropy level across the majority of the file, which is characteristic of a packed binary. This confirms that the malware's true code is hidden 10.Export Table Review ● Analyze the Export Table to check if the malware exposes functions. If yes, identify their purpose and potential use in malicious operations. 11. Section Anomalies Detection ● Check for non-standard section names, irregular section alignment, or sections with both read/write/execute permissions, which may indicate packing or code injection. The PE sections have standard names. However, a critical anomaly is found in the section permissions. Both the .data and .tls sections have characteristics that allow them to be written to and executed. A section should never be both writable and executable at the same time. This Write + Execute permission is a major red flag and a common technique used by packers. The packer writes the unpacked, malicious code into this memory region and then instructs the CPU to execute it, a clear sign of code injection or in-memory unpacking 12. Suspicious API Pattern Identification Analysis of the imported functions reveals several combinations that indicate malicious functionality. The APIs can be grouped into distinct behavioral patterns: ● Process Injection: The malware imports OpenProcess, VirtualAllocEx, WriteProcessMemory, and CreateRemoteThread. This is the complete set of functions required to inject malicious code into another running process, a core technique for stealth. ● Persistence: Imports like RegCreateKeyExA and RegSetValueExA indicate the ability to manipulate the Windows Registry. This is commonly used to create startup entries to ensure the malware runs every time the system boots. ● Anti-Analysis & Evasion: The presence of IsDebuggerPresent is a direct check to see if the malware is being analyzed. Furthermore, the use of GetModuleHandleA and GetProcAddress suggests the malware dynamically resolves function addresses to make static analysis more difficult. 13. Timestamp and Compiler Information Analysis Examine the PE header’s timestamp and compiler information. Check for anomalies such as future dates, missing timestamps, or uncommon compiler signatures. The PE header's compiler stamp indicates the file was created on Thursday, March 14, 2019. While not as overtly suspicious as a future date or a generic 1992 timestamp, this date could still be manipulated by the author. The compiler is identified as Visual Studio 2013. This information suggests the malware was developed using a standard, albeit older, development environment. The most significant finding remains the evidence of packing, rather than any specific compiler anomaly.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 19
  },
  "f4184f95-be80-4f7e-bf85-f11f7f5d26c6": {
    "filename": "Lab Exam 1 - Static Analysis of Malware .pdf",
    "text": "LAB EXAM MODEL 1 Static Analysis of Malware The Downloaded malware file is DanaBot.exe 1. File Information: a. file type : executable b. Size : 2795520 bytes c. MD5 hash : 4D 5A 90 00 03 00 00 00 04 00 00 00 FF FF 00 00 B8 00 00 00 00 00 00 00 40 00 00 00 00 00 00 00 00 d. SHA256 hash: DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 e. creation date : Thu Mar 14 19:36:48 2019 | UTC The analyzed file, DanaBot.exe , is a 32-bit Windows executable with a size of approximately 2.79 MB. The compilation timestamp indicates it was created on March 14, 2019. The SHA256 hash, which uniquely identifies the file, is DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 Tools used: Pestudio 2. String Analysis: a. Use a string extraction tool to identify readable text. Tool Used: FLOSS 3. Categories useful strings a. URLs b. IP addresses c. Domains d. file paths e. function names f. suspicious strings g. DLL information Analysis with the FLOSS tool revealed a large number of static strings (33,058), but a conspicuous absence of decoded strings. Further inspection of the output showed no clear-text URLs, IP addresses, or domains. The strings consist mostly of standard library and function names, with no immediately obvious suspicious indicators. This lack of meaningful, human-readable indicators is a strong sign that the malware's configuration and critical data (like C2 server addresses) are obfuscated or encrypted within the binary. 4. PE Header Analysis (for PE files): a. Check whether the sample is a Portable Executable (PE). The file is confirmed to be a valid Portable Executable (PE), as identified by the \"MZ\" magic bytes at the beginning of the file. The PE header defines five sections: .text, .rdata, .data, .tls, and .rsrc. Analysis of the section properties reveals significant anomalies. The .text, .rdata, and .data sections all show a Virtual Size that is substantially larger than their Raw Size. For example, the .text section has a raw size of F800 bytes on disk but expands to F7FD bytes in memory. This discrepancy is a classic indicator of a packed executable. The malware code is likely compressed or encrypted on disk and will be unpacked into its larger, true form in memory during execution. Tools Used: PE-Bear It loads, it's a Portable Executable (PE). The first two letters you see in the hex view should be \"MZ\"—the signature of a DOS header, which is part of the PE format. 5. Analyse the PE header. Identify sections a. .text : b. .data c. .rdata d. inspect their sizes and properties. A large difference where the VirtualSize is much bigger than the SizeOfRawData can be a red flag. It often means the section is compressed or un-initialized and will be expanded in memory when the program runs, a common sign of packing. 6. Extract information about imported and exported functions. Exports Imports The malware's export table contains three non-descript functions, which is unusual for a malicious executable and likely a byproduct of its packer. The import table, however, is highly revealing. It shows the program imports numerous functions from standard Windows libraries like KERNEL32.dll and ADVAPI32.dll. Pestudio flags over 88 imported functions as suspicious. These imports provide critical clues about the malware's capabilities, including reconnaissance (GetVersionExA), memory manipulation (HeapAlloc), and system information gathering (GetVolumeNameForVolumeMountPointA). These functions will be analyzed in greater detail to identify behavioral patterns. Tools: PE-bear for structure, Pestudio for a clean view of imports/exports. 7. Obfuscation/Encryption Detection: a. Identify any signs of code obfuscation or encryption within the binary. If the file is not packed (or DiE can see through the packing), it will identify the compiler used to create the program, such as Compiler: Microsoft Visual C++. 8. Section Entropy Analysis a. Use a PE analysis tool to calculate the entropy of each section (.text, .data, .rdata, etc.). b. Interpret the entropy values to determine whether any section indicates potential packing or encryption. A high, flat line indicates high entropy, which is a strong sign of packed or encrypted data, even if DiE can't name the specific packer. A high entropy value (generally above 7.0) suggests the data is compressed or encrypted. Normal code and data have lower entropy. If you see a section with high entropy, it's likely packed. Tools: Detect It Easy (DiE), Pestudio. While Detect It Easy (DiE) identifies the compiler as Microsoft Visual C++ and does not flag a specific packer, the entropy analysis provides definitive evidence of obfuscation. The overall entropy of the file as calculated by Pestudio is 7.950, which is extremely high. An entropy value above 7.0 strongly suggests that the data is not normal code but is instead compressed or encrypted. The entropy graph from PE-bear confirms this, showing a high and uniform entropy level across the majority of the file, which is characteristic of a packed binary. This confirms that the malware's true code is hidden 10.Export Table Review ● Analyze the Export Table to check if the malware exposes functions. If yes, identify their purpose and potential use in malicious operations. 11. Section Anomalies Detection ● Check for non-standard section names, irregular section alignment, or sections with both read/write/execute permissions, which may indicate packing or code injection. The PE sections have standard names. However, a critical anomaly is found in the section permissions. Both the .data and .tls sections have characteristics that allow them to be written to and executed. A section should never be both writable and executable at the same time. This Write + Execute permission is a major red flag and a common technique used by packers. The packer writes the unpacked, malicious code into this memory region and then instructs the CPU to execute it, a clear sign of code injection or in-memory unpacking 12. Suspicious API Pattern Identification Analysis of the imported functions reveals several combinations that indicate malicious functionality. The APIs can be grouped into distinct behavioral patterns: ● Process Injection: The malware imports OpenProcess, VirtualAllocEx, WriteProcessMemory, and CreateRemoteThread. This is the complete set of functions required to inject malicious code into another running process, a core technique for stealth. ● Persistence: Imports like RegCreateKeyExA and RegSetValueExA indicate the ability to manipulate the Windows Registry. This is commonly used to create startup entries to ensure the malware runs every time the system boots. ● Anti-Analysis & Evasion: The presence of IsDebuggerPresent is a direct check to see if the malware is being analyzed. Furthermore, the use of GetModuleHandleA and GetProcAddress suggests the malware dynamically resolves function addresses to make static analysis more difficult. 13. Timestamp and Compiler Information Analysis Examine the PE header’s timestamp and compiler information. Check for anomalies such as future dates, missing timestamps, or uncommon compiler signatures. The PE header's compiler stamp indicates the file was created on Thursday, March 14, 2019. While not as overtly suspicious as a future date or a generic 1992 timestamp, this date could still be manipulated by the author. The compiler is identified as Visual Studio 2013. This information suggests the malware was developed using a standard, albeit older, development environment. The most significant finding remains the evidence of packing, rather than any specific compiler anomaly.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 19
  },
  "61aa5ed0-c336-4a33-8a34-eef2f8b344dc": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "6f71da35-5fdc-4ca8-a805-7fa9ba1dd99d": {
    "filename": "Lecture 12.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 12 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Unsupervised Learning Introduction • Regression and Classification problems belong to a class of problems called Supervised Learning Problems. • In both regression and classification, the historical or training data provided to build a predictive model includes data on the variable to be predicted, i.e., the outcome or target variables. • When the data is provided on the variable to be predicted then the problem is a Supervised Learning problem. • Unsupervised learning problems are those where the dataset does not have any target or outcome variables. If there is no data on what needs to be predicted, then what can such algorithms do? • Suppose a customer has selected a few products to purchase. • We would like to provide suggestions about other products that he should or could be buying. • The data we have is historical data of what customers have bought. • In the historical data, if we find repeat patterns of milk being bought with bread, and • If customer who is currently shopping has bought milk, then our algorithm can suggest buying bread. • We can not say that if milk is the value of a feature , then bread is the value to be predicted because vice versa, i.e., if the customer has bought bread, he could buy milk. Contd… • So, the role of feature and predictor quickly gets changed. • Rather, in this case, we have patterns in our historical data, and we define rules saying that if certain patterns occur very often, then they can be used for making certain recommendations. • This problem formulation goes by the name of Associate Rule Mining and is an Unsupervised Learning problem. • We do not discuss associate rule mining in this course • Rules in Associate rule mining are automatically inferred from the data (i.e machine learned) and are not created manually by human beings. Contd.. • In unsupervised learning , the models still perform predictions. • But there is no prior data on the outcomes. • We need to define the outcome or prediction we desire and accordingly come up with an algorithm for doing so. Clustering • Clustering is an unsupervised machine learning technique that groups similar data points together into clusters based on their characteristics, without using any labeled data. • The objective is to ensure that data points within the same cluster are more similar to each other than to those in different clusters, enabling the discovery of natural groupings and hidden patterns in complex datasets. • Goal: Discover the natural grouping or structure in unlabeled data without predefined categories. • How: Data points are assigned to clusters based on similarity or distance measures. • Similarity Measures: Can include Euclidean distance, cosine similarity or other metrics depending on data type and clustering method. • Output: Each group is assigned a cluster ID, representing shared characteristics within the cluster. • For example, if we have customer purchase data, clustering can group customers with similar shopping habits. • These clusters can then be used for targeted marketing, personalized recommendations or customer segmentation. Types of Clustering • Hard Clustering: In hard clustering, each data point strictly belongs to exactly one cluster, no overlap is allowed. This approach assigns a clear membership, making it easier to interpret and use for definitive segmentation tasks. • Example: If clustering customer data into 2 segments, each customer belongs fully to either Cluster 1 or Cluster 2 without partial memberships. • Use cases: Market segmentation, customer grouping, document clustering. • Limitations: Cannot represent ambiguity or overlap between groups; boundaries are crisp. • Soft Clustering: Soft clustering assigns each data point a probability or degree of membership to multiple clusters simultaneously, allowing data points to partially belong to several groups. • Example: A data point may have a 70% membership in Cluster 1 and 30% in Cluster 2, reflecting uncertainty or overlap in group characteristics. • Use cases: Situations with overlapping class boundaries, fuzzy categories like customer personas or medical diagnosis. • Benefits: Captures ambiguity in data, models gradual transitions between clusters. Types of Clustering Methods • Clustering methods can be classified on the basis of how they for clusters, 1. Centroid-based Clustering (Partitioning Methods) • Centroid-based clustering organizes data points around central prototypes called centroids, where each cluster is represented by the mean (or medoid) of its members. • The number of clusters is specified in advance and the algorithm allocates points to the nearest centroid, making this technique efficient for spherical and similarly sized clusters but sensitive to outliers and initialization. Algorithms • K-means: Iteratively assigns points to nearest centroid and recalculates centroids to minimize intra-cluster variance. • K-medoids : Similar to K-means but uses actual data points (medoids) as centers, robust to outliers. Pros: • Fast and scalable for large datasets. • Simple to implement and interpret. Cons: • Requires pre-knowledge of k. • Sensitive to initialization and outliers. • Not suitable for non-spherical clusters. Density-based Clustering (Model-based Methods) • Density-based clustering defines clusters as contiguous regions of high data density separated by areas of lower density. • This approach can identify clusters of arbitrary shapes, handles noise well and does not require predefining the number of clusters, though its effectiveness depends on chosen density parameters. Algorithms • DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points with sufficient neighbors; labels sparse points as noise. • OPTICS (Ordering Points To Identify Clustering Structure): Extends DBSCAN to handle varying densities. Pros/Cons Pros: • Handles clusters of varying shapes and sizes. • Does not require cluster count upfront. • Effective in noisy datasets. Cons: • Difficult to choose parameters like epsilon and min points. • Less effective for varying density clusters (except OPTICS). Connectivity-based Clustering (Hierarchical Clustering) • Connectivity-based (or hierarchical) clustering builds nested groupings of data by evaluating how data points are connected to their neighbors. • It creates a dendrogram—a tree-like structure—that reflects relationships at various granularity levels and does not require specifying cluster numbers in advance, but can be computationally intensive. Approaches: • Agglomerative (Bottom-up): Start with each point as a cluster; iteratively merge closest clusters. • Divisive (Top-down): Start with one cluster; iteratively split into smaller clusters. Pros • Provides a full hierarchy, easy to visualize. • No need to specify number of clusters upfront. Cons • Computationally intensive for large datasets. • Merging/splitting decisions are irreversible. Distribution-based Clustering • Distribution-based clustering assumes data is generated from a mixture of probability distributions, such as Gaussian distributions and assigns points to clusters based on statistical likelihood. • This method supports clusters with flexible shapes and overlaps, but usually requires specifying the number of distributions. Algorithm: • Gaussian Mixture Model (GMM): Fits data as a weighted mixture of Gaussian distributions; assigns data points based on likelihood. Pros: • Flexible cluster shapes. • Provides probabilistic memberships. • Suitable for overlapping clusters. Cons: • Requires specifying number of components. • Computationally more expensive. • Sensitive to initialization. Fuzzy Clustering • Fuzzy clustering extends traditional methods by allowing each data point to belong to multiple clusters with varying degrees of membership. • This approach captures ambiguity and soft boundaries in data and is particularly useful when the clusters overlap or boundaries are not clear-cut. Algorithm: • Fuzzy C-Means: Similar to K-means but with fuzzy memberships updated iteratively. Pros: • Models data ambiguity explicitly. • Useful for complex or imprecise data. Cons: • Choosing fuzziness parameter can be tricky. • Computational overhead compared to hard clustering. Use Cases • Customer Segmentation: Grouping customers based on behavior or demographics for targeted marketing and personalized services. • Anomaly Detection: Identifying outliers or fraudulent activities in finance, network security and sensor data. • Image Segmentation: Dividing images into meaningful parts for object detection, medical diagnostics or computer vision tasks. • Recommendation Systems: Clustering user preferences to recommend movies, products or content tailored to different groups. • Market Basket Analysis: Discovering products frequently bought together to optimize store layouts and promotions. K means Clustering • K-Means Clustering is an unsupervised machine learning algorithm that helps group data points into clusters based on their inherent similarity. • Unlike supervised learning, where we train models using labeled data, K-Means is used when we have data that is not labeled and the goal is to uncover hidden patterns or structures. • For example, an online store can use K-Means to segment customers into groups like \"Budget Shoppers,\" \"Frequent Buyers,\" and \"Big Spenders\" based on their purchase history Working of K-Means Clustering • Suppose we are given a data set of items with certain features and values for these features like a vector. • The task is to categorize those items into groups. • To achieve this, we will use the K-means algorithm. 𝑘 • \" \" represents the number of groups or clusters we want to classify our items into. K-Means Clustering Algorithm • The algorithm will categorize the items into \"𝑘 \" groups or clusters of similarity. • To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows: 1. Initialization: We begin by randomly selecting k cluster centroids. 2. Assignment Step: Each data point is assigned to the nearest centroid, forming clusters. 3. Update Step: After the assignment, we recalculate the centroid of each cluster by averaging the points within it. 4. Repeat: This process repeats until the centroids no longer change or the maximum number of iterations is reached. • The goal is to partition the dataset into 𝑘 clusters such that data points within each cluster are more similar to each other than to those in other clusters. Euclidean distance Elbow Method for optimal value of k in KMeans • Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means. • However, deciding the ideal k is not straightforward. • The Elbow Method helps by plotting the Within-Cluster Sum of Squares (WCSS) against increasing k values and looking for a point where the improvement slows down, this point is called the \"elbow.\" Working of Elbow Point The Elbow Method works in the following steps: 1) We iterate over a range of k values, typically from 1 to n (where n is a hyperparameter you choose). 2) For each k, we calculate a distance measure called WCSS (Within-Cluster Sum of Squares). • This tells us how spread out the data points are within each cluster. • WCSS measures how well the data points are clustered around their respective centroids. 3) We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS. 4) We plot a graph with k on the X-axis and WCSS on the Y-axis. 5) As we increase k, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph. Within-Cluster Sum of Squares(WCSS) 𝑘 𝑘 • Where (𝑥 , 𝑦 ) is the centroid of the points in the cluster 𝐶 , and | 𝐶 | is 𝑘 𝑘 the number of points in the cluster. • WCSS is essentially the sum of the square of the distances between the points in the cluster and its centroid. • It should be obvious that if the points are tightly clustered ,then WCSS will be a small number and vice versa. • Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability. • After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting. • The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. • This \"elbow\" point suggests the optimal number of clusters. Inertia • Inertia simply as the sum of the WCSs values for a group of clusters. • If there are K clusters ,then inertia of the clusters is given by, • Given K ,the number of clusters and n points , there are many ways to cluster the n points into K clusters. • The K-means algorithm creates K-Clusters such that the sum of the WCSS values for the K-clusters is minimal, i.e., it tries to create K clusters such that the inertia of the K clusters created is less than the inertia of clustering of the n points into some other K clusters. • It should be mentioned at the onset that K-means is based on heuristics and does not guarantee a clustering with minimum inertia. • It merely finds a clustering with low inertia • Lower Inertia suggests better clustering Why Use K-Means Clustering? K-Means is popular in a wide variety of applications due to its simplicity, efficiency and effectiveness. Data Segmentation: One of the most common uses of K-Means is segmenting data into distinct groups. For example, businesses use K-Means to group customers based on behavior, such as purchasing patterns or website interaction. Image Compression: K-Means can be used to reduce the complexity of images by grouping similar pixels into clusters, effectively compressing the image. This is useful for image storage and processing. Anomaly Detection: K-Means can be applied to detect anomalies or outliers by identifying data points that do not belong to any of the clusters. Document Clustering: In natural language processing (NLP), K-Means is used to group similar documents or articles together. It’s often used in applications like recommendation systems or news categorization. Organizing Large Datasets: When dealing with large datasets, K-Means can help in organizing the data into smaller, more manageable chunks based on similarities, improving the efficiency of data analysis. Challenges with K-Means Clustering • Choosing the Right Number of Clusters (𝑘 ): One of the biggest challenges is deciding how many clusters to use. • Sensitive to Initial Centroids: The final clusters can vary depending on the initial random placement of centroids. • Non-Spherical Clusters: K-Means assumes that the clusters are spherical and equally sized. This can be a problem when the actual clusters in the data are of different shapes or densities. • Outliers: K-Means is sensitive to outliers, which can distort the centroid and, ultimately, the clusters.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 39
  },
  "81c599dc-39ab-4bd5-b602-89f04f02625c": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "f5a27217-2c68-42de-94e8-7f6ba6ccfb9e": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "82dcd426-66c8-46b3-ba25-d0c8d16f8676": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "05aed1a5-9a3b-4963-956e-186af991b026": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "48ae5370-d72b-4d97-b961-eb6f6fb8fc3d": {
    "filename": "Lecture 12.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 12 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Unsupervised Learning Introduction • Regression and Classification problems belong to a class of problems called Supervised Learning Problems. • In both regression and classification, the historical or training data provided to build a predictive model includes data on the variable to be predicted, i.e., the outcome or target variables. • When the data is provided on the variable to be predicted then the problem is a Supervised Learning problem. • Unsupervised learning problems are those where the dataset does not have any target or outcome variables. If there is no data on what needs to be predicted, then what can such algorithms do? • Suppose a customer has selected a few products to purchase. • We would like to provide suggestions about other products that he should or could be buying. • The data we have is historical data of what customers have bought. • In the historical data, if we find repeat patterns of milk being bought with bread, and • If customer who is currently shopping has bought milk, then our algorithm can suggest buying bread. • We can not say that if milk is the value of a feature , then bread is the value to be predicted because vice versa, i.e., if the customer has bought bread, he could buy milk. Contd… • So, the role of feature and predictor quickly gets changed. • Rather, in this case, we have patterns in our historical data, and we define rules saying that if certain patterns occur very often, then they can be used for making certain recommendations. • This problem formulation goes by the name of Associate Rule Mining and is an Unsupervised Learning problem. • We do not discuss associate rule mining in this course • Rules in Associate rule mining are automatically inferred from the data (i.e machine learned) and are not created manually by human beings. Contd.. • In unsupervised learning , the models still perform predictions. • But there is no prior data on the outcomes. • We need to define the outcome or prediction we desire and accordingly come up with an algorithm for doing so. Clustering • Clustering is an unsupervised machine learning technique that groups similar data points together into clusters based on their characteristics, without using any labeled data. • The objective is to ensure that data points within the same cluster are more similar to each other than to those in different clusters, enabling the discovery of natural groupings and hidden patterns in complex datasets. • Goal: Discover the natural grouping or structure in unlabeled data without predefined categories. • How: Data points are assigned to clusters based on similarity or distance measures. • Similarity Measures: Can include Euclidean distance, cosine similarity or other metrics depending on data type and clustering method. • Output: Each group is assigned a cluster ID, representing shared characteristics within the cluster. • For example, if we have customer purchase data, clustering can group customers with similar shopping habits. • These clusters can then be used for targeted marketing, personalized recommendations or customer segmentation. Types of Clustering • Hard Clustering: In hard clustering, each data point strictly belongs to exactly one cluster, no overlap is allowed. This approach assigns a clear membership, making it easier to interpret and use for definitive segmentation tasks. • Example: If clustering customer data into 2 segments, each customer belongs fully to either Cluster 1 or Cluster 2 without partial memberships. • Use cases: Market segmentation, customer grouping, document clustering. • Limitations: Cannot represent ambiguity or overlap between groups; boundaries are crisp. • Soft Clustering: Soft clustering assigns each data point a probability or degree of membership to multiple clusters simultaneously, allowing data points to partially belong to several groups. • Example: A data point may have a 70% membership in Cluster 1 and 30% in Cluster 2, reflecting uncertainty or overlap in group characteristics. • Use cases: Situations with overlapping class boundaries, fuzzy categories like customer personas or medical diagnosis. • Benefits: Captures ambiguity in data, models gradual transitions between clusters. Types of Clustering Methods • Clustering methods can be classified on the basis of how they for clusters, 1. Centroid-based Clustering (Partitioning Methods) • Centroid-based clustering organizes data points around central prototypes called centroids, where each cluster is represented by the mean (or medoid) of its members. • The number of clusters is specified in advance and the algorithm allocates points to the nearest centroid, making this technique efficient for spherical and similarly sized clusters but sensitive to outliers and initialization. Algorithms • K-means: Iteratively assigns points to nearest centroid and recalculates centroids to minimize intra-cluster variance. • K-medoids : Similar to K-means but uses actual data points (medoids) as centers, robust to outliers. Pros: • Fast and scalable for large datasets. • Simple to implement and interpret. Cons: • Requires pre-knowledge of k. • Sensitive to initialization and outliers. • Not suitable for non-spherical clusters. Density-based Clustering (Model-based Methods) • Density-based clustering defines clusters as contiguous regions of high data density separated by areas of lower density. • This approach can identify clusters of arbitrary shapes, handles noise well and does not require predefining the number of clusters, though its effectiveness depends on chosen density parameters. Algorithms • DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points with sufficient neighbors; labels sparse points as noise. • OPTICS (Ordering Points To Identify Clustering Structure): Extends DBSCAN to handle varying densities. Pros/Cons Pros: • Handles clusters of varying shapes and sizes. • Does not require cluster count upfront. • Effective in noisy datasets. Cons: • Difficult to choose parameters like epsilon and min points. • Less effective for varying density clusters (except OPTICS). Connectivity-based Clustering (Hierarchical Clustering) • Connectivity-based (or hierarchical) clustering builds nested groupings of data by evaluating how data points are connected to their neighbors. • It creates a dendrogram—a tree-like structure—that reflects relationships at various granularity levels and does not require specifying cluster numbers in advance, but can be computationally intensive. Approaches: • Agglomerative (Bottom-up): Start with each point as a cluster; iteratively merge closest clusters. • Divisive (Top-down): Start with one cluster; iteratively split into smaller clusters. Pros • Provides a full hierarchy, easy to visualize. • No need to specify number of clusters upfront. Cons • Computationally intensive for large datasets. • Merging/splitting decisions are irreversible. Distribution-based Clustering • Distribution-based clustering assumes data is generated from a mixture of probability distributions, such as Gaussian distributions and assigns points to clusters based on statistical likelihood. • This method supports clusters with flexible shapes and overlaps, but usually requires specifying the number of distributions. Algorithm: • Gaussian Mixture Model (GMM): Fits data as a weighted mixture of Gaussian distributions; assigns data points based on likelihood. Pros: • Flexible cluster shapes. • Provides probabilistic memberships. • Suitable for overlapping clusters. Cons: • Requires specifying number of components. • Computationally more expensive. • Sensitive to initialization. Fuzzy Clustering • Fuzzy clustering extends traditional methods by allowing each data point to belong to multiple clusters with varying degrees of membership. • This approach captures ambiguity and soft boundaries in data and is particularly useful when the clusters overlap or boundaries are not clear-cut. Algorithm: • Fuzzy C-Means: Similar to K-means but with fuzzy memberships updated iteratively. Pros: • Models data ambiguity explicitly. • Useful for complex or imprecise data. Cons: • Choosing fuzziness parameter can be tricky. • Computational overhead compared to hard clustering. Use Cases • Customer Segmentation: Grouping customers based on behavior or demographics for targeted marketing and personalized services. • Anomaly Detection: Identifying outliers or fraudulent activities in finance, network security and sensor data. • Image Segmentation: Dividing images into meaningful parts for object detection, medical diagnostics or computer vision tasks. • Recommendation Systems: Clustering user preferences to recommend movies, products or content tailored to different groups. • Market Basket Analysis: Discovering products frequently bought together to optimize store layouts and promotions. K means Clustering • K-Means Clustering is an unsupervised machine learning algorithm that helps group data points into clusters based on their inherent similarity. • Unlike supervised learning, where we train models using labeled data, K-Means is used when we have data that is not labeled and the goal is to uncover hidden patterns or structures. • For example, an online store can use K-Means to segment customers into groups like \"Budget Shoppers,\" \"Frequent Buyers,\" and \"Big Spenders\" based on their purchase history Working of K-Means Clustering • Suppose we are given a data set of items with certain features and values for these features like a vector. • The task is to categorize those items into groups. • To achieve this, we will use the K-means algorithm. 𝑘 • \" \" represents the number of groups or clusters we want to classify our items into. K-Means Clustering Algorithm • The algorithm will categorize the items into \"𝑘 \" groups or clusters of similarity. • To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows: 1. Initialization: We begin by randomly selecting k cluster centroids. 2. Assignment Step: Each data point is assigned to the nearest centroid, forming clusters. 3. Update Step: After the assignment, we recalculate the centroid of each cluster by averaging the points within it. 4. Repeat: This process repeats until the centroids no longer change or the maximum number of iterations is reached. • The goal is to partition the dataset into 𝑘 clusters such that data points within each cluster are more similar to each other than to those in other clusters. Euclidean distance Elbow Method for optimal value of k in KMeans • Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means. • However, deciding the ideal k is not straightforward. • The Elbow Method helps by plotting the Within-Cluster Sum of Squares (WCSS) against increasing k values and looking for a point where the improvement slows down, this point is called the \"elbow.\" Working of Elbow Point The Elbow Method works in the following steps: 1) We iterate over a range of k values, typically from 1 to n (where n is a hyperparameter you choose). 2) For each k, we calculate a distance measure called WCSS (Within-Cluster Sum of Squares). • This tells us how spread out the data points are within each cluster. • WCSS measures how well the data points are clustered around their respective centroids. 3) We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS. 4) We plot a graph with k on the X-axis and WCSS on the Y-axis. 5) As we increase k, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph. Within-Cluster Sum of Squares(WCSS) 𝑘 𝑘 • Where (𝑥 , 𝑦 ) is the centroid of the points in the cluster 𝐶 , and | 𝐶 | is 𝑘 𝑘 the number of points in the cluster. • WCSS is essentially the sum of the square of the distances between the points in the cluster and its centroid. • It should be obvious that if the points are tightly clustered ,then WCSS will be a small number and vice versa. • Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability. • After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting. • The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. • This \"elbow\" point suggests the optimal number of clusters. Inertia • Inertia simply as the sum of the WCSs values for a group of clusters. • If there are K clusters ,then inertia of the clusters is given by, • Given K ,the number of clusters and n points , there are many ways to cluster the n points into K clusters. • The K-means algorithm creates K-Clusters such that the sum of the WCSS values for the K-clusters is minimal, i.e., it tries to create K clusters such that the inertia of the K clusters created is less than the inertia of clustering of the n points into some other K clusters. • It should be mentioned at the onset that K-means is based on heuristics and does not guarantee a clustering with minimum inertia. • It merely finds a clustering with low inertia • Lower Inertia suggests better clustering Why Use K-Means Clustering? K-Means is popular in a wide variety of applications due to its simplicity, efficiency and effectiveness. Data Segmentation: One of the most common uses of K-Means is segmenting data into distinct groups. For example, businesses use K-Means to group customers based on behavior, such as purchasing patterns or website interaction. Image Compression: K-Means can be used to reduce the complexity of images by grouping similar pixels into clusters, effectively compressing the image. This is useful for image storage and processing. Anomaly Detection: K-Means can be applied to detect anomalies or outliers by identifying data points that do not belong to any of the clusters. Document Clustering: In natural language processing (NLP), K-Means is used to group similar documents or articles together. It’s often used in applications like recommendation systems or news categorization. Organizing Large Datasets: When dealing with large datasets, K-Means can help in organizing the data into smaller, more manageable chunks based on similarities, improving the efficiency of data analysis. Challenges with K-Means Clustering • Choosing the Right Number of Clusters (𝑘 ): One of the biggest challenges is deciding how many clusters to use. • Sensitive to Initial Centroids: The final clusters can vary depending on the initial random placement of centroids. • Non-Spherical Clusters: K-Means assumes that the clusters are spherical and equally sized. This can be a problem when the actual clusters in the data are of different shapes or densities. • Outliers: K-Means is sensitive to outliers, which can distort the centroid and, ultimately, the clusters.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 39
  },
  "51ae9041-e41f-41e5-94b5-2ea635ecfec4": {
    "filename": "Lecture 12.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 12 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Unsupervised Learning Introduction • Regression and Classification problems belong to a class of problems called Supervised Learning Problems. • In both regression and classification, the historical or training data provided to build a predictive model includes data on the variable to be predicted, i.e., the outcome or target variables. • When the data is provided on the variable to be predicted then the problem is a Supervised Learning problem. • Unsupervised learning problems are those where the dataset does not have any target or outcome variables. If there is no data on what needs to be predicted, then what can such algorithms do? • Suppose a customer has selected a few products to purchase. • We would like to provide suggestions about other products that he should or could be buying. • The data we have is historical data of what customers have bought. • In the historical data, if we find repeat patterns of milk being bought with bread, and • If customer who is currently shopping has bought milk, then our algorithm can suggest buying bread. • We can not say that if milk is the value of a feature , then bread is the value to be predicted because vice versa, i.e., if the customer has bought bread, he could buy milk. Contd… • So, the role of feature and predictor quickly gets changed. • Rather, in this case, we have patterns in our historical data, and we define rules saying that if certain patterns occur very often, then they can be used for making certain recommendations. • This problem formulation goes by the name of Associate Rule Mining and is an Unsupervised Learning problem. • We do not discuss associate rule mining in this course • Rules in Associate rule mining are automatically inferred from the data (i.e machine learned) and are not created manually by human beings. Contd.. • In unsupervised learning , the models still perform predictions. • But there is no prior data on the outcomes. • We need to define the outcome or prediction we desire and accordingly come up with an algorithm for doing so. Clustering • Clustering is an unsupervised machine learning technique that groups similar data points together into clusters based on their characteristics, without using any labeled data. • The objective is to ensure that data points within the same cluster are more similar to each other than to those in different clusters, enabling the discovery of natural groupings and hidden patterns in complex datasets. • Goal: Discover the natural grouping or structure in unlabeled data without predefined categories. • How: Data points are assigned to clusters based on similarity or distance measures. • Similarity Measures: Can include Euclidean distance, cosine similarity or other metrics depending on data type and clustering method. • Output: Each group is assigned a cluster ID, representing shared characteristics within the cluster. • For example, if we have customer purchase data, clustering can group customers with similar shopping habits. • These clusters can then be used for targeted marketing, personalized recommendations or customer segmentation. Types of Clustering • Hard Clustering: In hard clustering, each data point strictly belongs to exactly one cluster, no overlap is allowed. This approach assigns a clear membership, making it easier to interpret and use for definitive segmentation tasks. • Example: If clustering customer data into 2 segments, each customer belongs fully to either Cluster 1 or Cluster 2 without partial memberships. • Use cases: Market segmentation, customer grouping, document clustering. • Limitations: Cannot represent ambiguity or overlap between groups; boundaries are crisp. • Soft Clustering: Soft clustering assigns each data point a probability or degree of membership to multiple clusters simultaneously, allowing data points to partially belong to several groups. • Example: A data point may have a 70% membership in Cluster 1 and 30% in Cluster 2, reflecting uncertainty or overlap in group characteristics. • Use cases: Situations with overlapping class boundaries, fuzzy categories like customer personas or medical diagnosis. • Benefits: Captures ambiguity in data, models gradual transitions between clusters. Types of Clustering Methods • Clustering methods can be classified on the basis of how they for clusters, 1. Centroid-based Clustering (Partitioning Methods) • Centroid-based clustering organizes data points around central prototypes called centroids, where each cluster is represented by the mean (or medoid) of its members. • The number of clusters is specified in advance and the algorithm allocates points to the nearest centroid, making this technique efficient for spherical and similarly sized clusters but sensitive to outliers and initialization. Algorithms • K-means: Iteratively assigns points to nearest centroid and recalculates centroids to minimize intra-cluster variance. • K-medoids : Similar to K-means but uses actual data points (medoids) as centers, robust to outliers. Pros: • Fast and scalable for large datasets. • Simple to implement and interpret. Cons: • Requires pre-knowledge of k. • Sensitive to initialization and outliers. • Not suitable for non-spherical clusters. Density-based Clustering (Model-based Methods) • Density-based clustering defines clusters as contiguous regions of high data density separated by areas of lower density. • This approach can identify clusters of arbitrary shapes, handles noise well and does not require predefining the number of clusters, though its effectiveness depends on chosen density parameters. Algorithms • DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points with sufficient neighbors; labels sparse points as noise. • OPTICS (Ordering Points To Identify Clustering Structure): Extends DBSCAN to handle varying densities. Pros/Cons Pros: • Handles clusters of varying shapes and sizes. • Does not require cluster count upfront. • Effective in noisy datasets. Cons: • Difficult to choose parameters like epsilon and min points. • Less effective for varying density clusters (except OPTICS). Connectivity-based Clustering (Hierarchical Clustering) • Connectivity-based (or hierarchical) clustering builds nested groupings of data by evaluating how data points are connected to their neighbors. • It creates a dendrogram—a tree-like structure—that reflects relationships at various granularity levels and does not require specifying cluster numbers in advance, but can be computationally intensive. Approaches: • Agglomerative (Bottom-up): Start with each point as a cluster; iteratively merge closest clusters. • Divisive (Top-down): Start with one cluster; iteratively split into smaller clusters. Pros • Provides a full hierarchy, easy to visualize. • No need to specify number of clusters upfront. Cons • Computationally intensive for large datasets. • Merging/splitting decisions are irreversible. Distribution-based Clustering • Distribution-based clustering assumes data is generated from a mixture of probability distributions, such as Gaussian distributions and assigns points to clusters based on statistical likelihood. • This method supports clusters with flexible shapes and overlaps, but usually requires specifying the number of distributions. Algorithm: • Gaussian Mixture Model (GMM): Fits data as a weighted mixture of Gaussian distributions; assigns data points based on likelihood. Pros: • Flexible cluster shapes. • Provides probabilistic memberships. • Suitable for overlapping clusters. Cons: • Requires specifying number of components. • Computationally more expensive. • Sensitive to initialization. Fuzzy Clustering • Fuzzy clustering extends traditional methods by allowing each data point to belong to multiple clusters with varying degrees of membership. • This approach captures ambiguity and soft boundaries in data and is particularly useful when the clusters overlap or boundaries are not clear-cut. Algorithm: • Fuzzy C-Means: Similar to K-means but with fuzzy memberships updated iteratively. Pros: • Models data ambiguity explicitly. • Useful for complex or imprecise data. Cons: • Choosing fuzziness parameter can be tricky. • Computational overhead compared to hard clustering. Use Cases • Customer Segmentation: Grouping customers based on behavior or demographics for targeted marketing and personalized services. • Anomaly Detection: Identifying outliers or fraudulent activities in finance, network security and sensor data. • Image Segmentation: Dividing images into meaningful parts for object detection, medical diagnostics or computer vision tasks. • Recommendation Systems: Clustering user preferences to recommend movies, products or content tailored to different groups. • Market Basket Analysis: Discovering products frequently bought together to optimize store layouts and promotions. K means Clustering • K-Means Clustering is an unsupervised machine learning algorithm that helps group data points into clusters based on their inherent similarity. • Unlike supervised learning, where we train models using labeled data, K-Means is used when we have data that is not labeled and the goal is to uncover hidden patterns or structures. • For example, an online store can use K-Means to segment customers into groups like \"Budget Shoppers,\" \"Frequent Buyers,\" and \"Big Spenders\" based on their purchase history Working of K-Means Clustering • Suppose we are given a data set of items with certain features and values for these features like a vector. • The task is to categorize those items into groups. • To achieve this, we will use the K-means algorithm. 𝑘 • \" \" represents the number of groups or clusters we want to classify our items into. K-Means Clustering Algorithm • The algorithm will categorize the items into \"𝑘 \" groups or clusters of similarity. • To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows: 1. Initialization: We begin by randomly selecting k cluster centroids. 2. Assignment Step: Each data point is assigned to the nearest centroid, forming clusters. 3. Update Step: After the assignment, we recalculate the centroid of each cluster by averaging the points within it. 4. Repeat: This process repeats until the centroids no longer change or the maximum number of iterations is reached. • The goal is to partition the dataset into 𝑘 clusters such that data points within each cluster are more similar to each other than to those in other clusters. Euclidean distance Elbow Method for optimal value of k in KMeans • Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means. • However, deciding the ideal k is not straightforward. • The Elbow Method helps by plotting the Within-Cluster Sum of Squares (WCSS) against increasing k values and looking for a point where the improvement slows down, this point is called the \"elbow.\" Working of Elbow Point The Elbow Method works in the following steps: 1) We iterate over a range of k values, typically from 1 to n (where n is a hyperparameter you choose). 2) For each k, we calculate a distance measure called WCSS (Within-Cluster Sum of Squares). • This tells us how spread out the data points are within each cluster. • WCSS measures how well the data points are clustered around their respective centroids. 3) We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS. 4) We plot a graph with k on the X-axis and WCSS on the Y-axis. 5) As we increase k, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph. Within-Cluster Sum of Squares(WCSS) 𝑘 𝑘 • Where (𝑥 , 𝑦 ) is the centroid of the points in the cluster 𝐶 , and | 𝐶 | is 𝑘 𝑘 the number of points in the cluster. • WCSS is essentially the sum of the square of the distances between the points in the cluster and its centroid. • It should be obvious that if the points are tightly clustered ,then WCSS will be a small number and vice versa. • Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability. • After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting. • The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. • This \"elbow\" point suggests the optimal number of clusters. Inertia • Inertia simply as the sum of the WCSs values for a group of clusters. • If there are K clusters ,then inertia of the clusters is given by, • Given K ,the number of clusters and n points , there are many ways to cluster the n points into K clusters. • The K-means algorithm creates K-Clusters such that the sum of the WCSS values for the K-clusters is minimal, i.e., it tries to create K clusters such that the inertia of the K clusters created is less than the inertia of clustering of the n points into some other K clusters. • It should be mentioned at the onset that K-means is based on heuristics and does not guarantee a clustering with minimum inertia. • It merely finds a clustering with low inertia • Lower Inertia suggests better clustering Why Use K-Means Clustering? K-Means is popular in a wide variety of applications due to its simplicity, efficiency and effectiveness. Data Segmentation: One of the most common uses of K-Means is segmenting data into distinct groups. For example, businesses use K-Means to group customers based on behavior, such as purchasing patterns or website interaction. Image Compression: K-Means can be used to reduce the complexity of images by grouping similar pixels into clusters, effectively compressing the image. This is useful for image storage and processing. Anomaly Detection: K-Means can be applied to detect anomalies or outliers by identifying data points that do not belong to any of the clusters. Document Clustering: In natural language processing (NLP), K-Means is used to group similar documents or articles together. It’s often used in applications like recommendation systems or news categorization. Organizing Large Datasets: When dealing with large datasets, K-Means can help in organizing the data into smaller, more manageable chunks based on similarities, improving the efficiency of data analysis. Challenges with K-Means Clustering • Choosing the Right Number of Clusters (𝑘 ): One of the biggest challenges is deciding how many clusters to use. • Sensitive to Initial Centroids: The final clusters can vary depending on the initial random placement of centroids. • Non-Spherical Clusters: K-Means assumes that the clusters are spherical and equally sized. This can be a problem when the actual clusters in the data are of different shapes or densities. • Outliers: K-Means is sensitive to outliers, which can distort the centroid and, ultimately, the clusters.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 39
  },
  "a5aa2397-f0d9-445c-ac46-797f7e357a52": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "2c1b1309-56ce-4f42-b3ab-f3614ffb3ed2": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "00e60013-b499-414d-a5e4-8d676fa9180f": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "3d96d4fe-d3ef-40a5-8da3-9f30bf41de4f": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "5f628564-bd78-4784-84ac-ff138b58de37": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "dd738292-9192-4f13-a1df-cd4002135e5d": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "b8fedb40-7804-4622-99fe-f66037fa6312": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "bfad01d0-e7e2-41a1-bf3c-626d85695a38": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "c26da295-d69c-482e-8ce5-664a9632d4a8": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "74271c0c-9082-4f27-9149-887262bf896c": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "4e3bf599-a588-4a9a-beae-79a58a2e3f83": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "fc73801a-8607-4bfd-8779-1d20a9d6d26c": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "963568d1-068b-448b-828e-f27f3590192b": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "58ce5870-fb40-40ac-a563-40b667fd6708": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "62f3ffc5-3137-4650-960b-ac5715ecc9dc": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "cc7d1ab3-6521-4241-9f40-84f8bf4284ab": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "1f1b76e1-2ac8-424c-b672-0feccf27b969": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "12c5aa9d-22ce-45c5-b736-d0b18b5acf5f": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "dc35d848-011c-44ab-ae39-e52e92ee4109": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "e93aa26c-d324-4346-a6bf-cfa94a0c86ae": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "031fd2cd-c419-4f9d-a75c-fce6496d4e17": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "13dc60e9-db81-4276-95ff-c31760134be9": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "d2bcb4b7-07ca-4e86-8bcb-bf7c31b47b8d": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "328167c1-c71a-49f9-86fc-c0b56645ae41": {
    "filename": "compliance-report-job_20251116_152702-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_152702 Search Term: tea powder Generated On: 16/11/2025, 3:31:37 pm Analysis Date: 16/11/2025, 3:28:49 pm Executive Summary Metric Value Total Products Analyzed 2 Compliant Products 0 Non-Compliant Products 2 Total Violations Found 4 Compliance Rate 0.0% Page 1 of 4 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: TATA TEA Kanan Devan Strong NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 4 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Brooke Bond 3 ROSES Tea NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 3 of 4 Generated by Product Compliance Checker <<PAGE_4>> Compliance Summary Table # Product Name Status Score Violations 1 TATA TEA Kanan Devan Strong Non-Compliant 71% 2 2 Brooke Bond 3 ROSES Tea Non-Compliant 71% 2 Page 4 of 4 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "7248caa4-ee1c-4e5c-9632-663ef06cc838": {
    "filename": "compliance-report-job_20251116_152702-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_152702 Search Term: tea powder Generated On: 16/11/2025, 3:31:37 pm Analysis Date: 16/11/2025, 3:28:49 pm Executive Summary Metric Value Total Products Analyzed 2 Compliant Products 0 Non-Compliant Products 2 Total Violations Found 4 Compliance Rate 0.0% Page 1 of 4 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: TATA TEA Kanan Devan Strong NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 4 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Brooke Bond 3 ROSES Tea NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 3 of 4 Generated by Product Compliance Checker <<PAGE_4>> Compliance Summary Table # Product Name Status Score Violations 1 TATA TEA Kanan Devan Strong Non-Compliant 71% 2 2 Brooke Bond 3 ROSES Tea Non-Compliant 71% 2 Page 4 of 4 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "9b10165b-05f0-41a3-b763-c2bc989ac15a": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker <<PAGE_3>> Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker <<PAGE_4>> Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker <<PAGE_5>> Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "856a36b9-3cec-46ce-a5f3-c668d94e7c5e": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker <<PAGE_3>> Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker <<PAGE_4>> Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker <<PAGE_5>> Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "ef029740-dd81-4e31-94ca-1cefd9bd1e04": {
    "filename": "compliance-report-job_20251116_103517-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_103517 Search Term: cookies Generated On: 16/11/2025, 12:43:00 pm Analysis Date: 16/11/2025, 10:36:57 am Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 8 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: Kikibix Whole Grain Cookies NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Kikibix Whole Grain Cookies NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker <<PAGE_4>> Product 3: Kikibix Whole Grain Cookies NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker <<PAGE_5>> Compliance Summary Table # Product Name Status Score Violations 1 Kikibix Whole Grain Cookies Non-Compliant 57% 3 2 Kikibix Whole Grain Cookies Non-Compliant 57% 3 3 Kikibix Whole Grain Cookies Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "437b723b-8423-4177-82bd-6ab21cfecaa5": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker <<PAGE_4>> Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker <<PAGE_5>> Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "4facbc41-3038-4548-bf20-35450c171eac": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker <<PAGE_3>> Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker <<PAGE_4>> Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker <<PAGE_5>> Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "84e323f4-60ed-4e5a-a384-2752ab33c891": {
    "filename": "compliance-report-job_20251116_152702-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_152702 Search Term: tea powder Generated On: 16/11/2025, 3:31:37 pm Analysis Date: 16/11/2025, 3:28:49 pm Executive Summary Metric Value Total Products Analyzed 2 Compliant Products 0 Non-Compliant Products 2 Total Violations Found 4 Compliance Rate 0.0% Page 1 of 4 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: TATA TEA Kanan Devan Strong NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 4 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Brooke Bond 3 ROSES Tea NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 3 of 4 Generated by Product Compliance Checker <<PAGE_4>> Compliance Summary Table # Product Name Status Score Violations 1 TATA TEA Kanan Devan Strong Non-Compliant 71% 2 2 Brooke Bond 3 ROSES Tea Non-Compliant 71% 2 Page 4 of 4 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "0ab7cc83-a900-41e7-8262-1094e31a3dc0": {
    "filename": "compliance-report-job_20251116_152702-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_152702 Search Term: tea powder Generated On: 16/11/2025, 3:31:37 pm Analysis Date: 16/11/2025, 3:28:49 pm Executive Summary Metric Value Total Products Analyzed 2 Compliant Products 0 Non-Compliant Products 2 Total Violations Found 4 Compliance Rate 0.0% Page 1 of 4 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: TATA TEA Kanan Devan Strong NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 4 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Brooke Bond 3 ROSES Tea NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 3 of 4 Generated by Product Compliance Checker <<PAGE_4>> Compliance Summary Table # Product Name Status Score Violations 1 TATA TEA Kanan Devan Strong Non-Compliant 71% 2 2 Brooke Bond 3 ROSES Tea Non-Compliant 71% 2 Page 4 of 4 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "e49ca400-56d9-4550-957a-941ba08733de": {
    "filename": "compliance-report-job_20251116_152702-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_152702 Search Term: tea powder Generated On: 16/11/2025, 3:31:37 pm Analysis Date: 16/11/2025, 3:28:49 pm Executive Summary Metric Value Total Products Analyzed 2 Compliant Products 0 Non-Compliant Products 2 Total Violations Found 4 Compliance Rate 0.0% Page 1 of 4 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: TATA TEA Kanan Devan Strong NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 4 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Brooke Bond 3 ROSES Tea NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 3 of 4 Generated by Product Compliance Checker <<PAGE_4>> Compliance Summary Table # Product Name Status Score Violations 1 TATA TEA Kanan Devan Strong Non-Compliant 71% 2 2 Brooke Bond 3 ROSES Tea Non-Compliant 71% 2 Page 4 of 4 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "c1f94ee9-064a-4abc-9e56-0b5e98c26763": {
    "filename": "Receipt-2451-3164.pdf",
    "text": "<<PAGE_1>> Receipt Invoice number HZ7SNSI00001 Date paid July 17, 2025 Perplexity AI, Inc Bill to 115 Sansome St. anarght@gmail.com Suite 900 San Francisco, California 94104 United States support@perplexity.ai US EIN 883550462 GB VAT GB474336086 EU OSS VAT EU372071072 $0.00 paid on July 17, 2025 Thanks for subscribing to Perplexity Pro! Unit price Amount Description Qty (excl. tax) (excl. tax) Time on Perplexity Pro (with 100.0% off) after 17 Jul 2025 1 $0.00 $0.00 Jul 17, 2025 – Jul 17, 2026 Subtotal $0.00 Airtel 1 year free 100% off) $0.00 Total $0.00 Payment history Payment method Date Amount paid Receipt number Page 1 of 1",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "28b9a2fa-507f-488c-a630-1725c8b14142": {
    "filename": "compliance-report-job_20251116_152702-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_152702 Search Term: tea powder Generated On: 16/11/2025, 3:31:37 pm Analysis Date: 16/11/2025, 3:28:49 pm Executive Summary Metric Value Total Products Analyzed 2 Compliant Products 0 Non-Compliant Products 2 Total Violations Found 4 Compliance Rate 0.0% Page 1 of 4 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: TATA TEA Kanan Devan Strong NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 4 Generated by Product Compliance Checker <<PAGE_3>> Product 2: Brooke Bond 3 ROSES Tea NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 3 of 4 Generated by Product Compliance Checker <<PAGE_4>> Compliance Summary Table # Product Name Status Score Violations 1 TATA TEA Kanan Devan Strong Non-Compliant 71% 2 2 Brooke Bond 3 ROSES Tea Non-Compliant 71% 2 Page 4 of 4 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "bef29600-fce2-461f-9352-64739231f8f4": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "<<PAGE_1>> Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker <<PAGE_2>> Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker <<PAGE_3>> Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker <<PAGE_4>> Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker <<PAGE_5>> Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "a4830a07-b223-4886-81ec-fdfd99003d8d": {
    "filename": "Lec27.pdf",
    "text": "<<PAGE_1>> Introduction to Large Language Models (LLMs) Prof. Tanmoy Chakraborty, Prof. Soumen Chakraborti Department of Computer Science & Engineering Indian Institute of Technology, Delhi Lecture 27 Knowledge and Retrieval: Knowledge Graph Completion and Evaluation Hello, today we will continue with knowledge graph completion and its evaluation. As we mentioned recently, knowledge graphs are very incomplete and we need to complete them either as hard decisions or as soft probabilistic judgments. In this setting, an incomplete knowledge graph is provided and a modeling or machine learning system first fits embedding representations of the entities and relations in the knowledge graph. Within this paradigm, there are two possibilities. One is that these representations are based on the topology or the shape of the knowledge graph alone. <<PAGE_2>> And then there is the more realistic and beneficial setting where we supplement the knowledge of the graph topology with the text aliases of the entities and relations that we briefly viewed in the last part. After these embedding representations are fitted, these will be applied back to the knowledge graph to infer missing fact triples in the knowledge graph with a certain level of confidence. So in short, knowledge graph completion consists of proposing a function f. The inputs to this function are the subject, the relation, and the object. While training, we would like to provide all the three inputs, but while testing one of these inputs may be withheld or unknown. The output of F is the belief in the fact, which connects the subject relation and object. Now, most knowledge graph representation and completion algorithms will represent each entity, which may be a subject or an object, and each relation as some kind of continuous geometric artifacts. Such an artifact may be a point in space, some kind of high dimensional space where the KG artifacts are living, or it could be a vector or a displacement. It can be a projection or a hyperplane onto which these points are being projected. It can be a rotation. <<PAGE_3>> It can be all kinds of real or complex artifacts. Without getting into the detail of that yet, the purpose of the scoring function f is to take these representations as input and output some measure of belief in subject relation object as a triple. For example, if I say Barack Obama, president of US, as the three inputs to f, f should output a high level of belief. If instead I say something like Barack Obama, discovered theory of relativity, then the output of F should be very low. We shouldn't believe in that fact based on what else we know about these entities and relationships. During training, we will provide the identity of these subjects, relations, and objects, and we will tell the algorithm whether that fact is true or false. By the way, when I say fact in this lecture, it basically means some kind of a claim. So a claim can be either true or false. During training, I will give triples like this and their ideal belief value, be it one or be it zero. and the algorithm will fit the representations like those points or vectors or displacements or rotations. After that, once the model is fitted, we can then infer unknown triples, which would be similar to predicting links in a social network. Some of you may be aware that in social networks, we might have a person, P1, who is already friends with some people, say P1, P2, P3, and P4, and now there may be another person, P20, and based on their behavior patterns and linkage patterns, we might ask, does the link P1 to P20 exist or should it exist? It's a bit similar to that, except that now our links are very richly endowed with features like links represent relation types and even the entities have rich features. But that's true <<PAGE_4>> even in social network analysis. Now, in principle, Neural networks are universal function approximators. So if we plug in a sufficiently strong or high capacity neural network into the box F, it should be able to learn a function which outputs the correct belief in every proposed fact or claim. In practice, though, the design of F is something of an art. And hundreds of papers through the last five or six years have invented and proposed mechanisms for the design of F and to fit it to observed data. Before we proceed further, let us set up some notation. We will represent entities as E in general. And when an entity takes the role of a subject, it will be written as S. When it has the role of an object, it will be written as O. Now, why do we distinguish between subject and object in relations? That's because not all relations are symmetric. If the relationship is sibling of, then obviously what is subject and what is object doesn't matter. But if the relation is asymmetric, like parent of or CEO of a company, then we cannot switch subject and object. And so E will have to be specialized into either S or O. When the learning system has fitted a representation for the entity E, we will write it as vector E or we'll write it as a boldface E. like this as a vector with an arrow on top or a boldface E. So subjects and objects will have corresponding embeddings bold S or bold O. And similarly, relations will have representations which are written as boldface R. Now, depending on the exact nature of the model, the exact shapes of S, O, and R might change, in some cases, there may be vectors. In other cases, there may be matrices, and so on and so forth. Now, this function f will apply to s, r and o given as inputs and output a real number. Generally speaking, this real number may be positive or maybe even not positive. It's a raw confidence score. It's not already a probability. <<PAGE_5>> To turn it into a probability, we may have to apply certain functions to it. If a particular SRO triple is known to be true in our knowledge graph, we'll call it a positive triple or a positive fact. Whereas if S prime, R prime, O prime is not in the knowledge graph, there are a couple of ways to think about it. Remember that our knowledge graph is incomplete. So if S prime, R prime, O prime does not appear in the knowledge graph, that doesn't necessarily prove that it's false. But there are ways to sample S prime, R prime, O prime, which makes it very unlikely that S prime, R prime, O prime is true. We shall visit one or two ways to do that in the upcoming slides. If S prime, R prime, O prime is known to be false, we'll call it a negative triple or a negative fact. Now, earlier I was saying that this F return value is not really a probability. If we wanted to turn it into a probability, we need some kind of a probability space. One example of this is to fix two of the three items in the triple, like the subject and the relation, and then say what's the probability that the object is O. For example, I might say probability of Hawaii given Barack Obama and born in, and that probability should be high. Whereas if I say probability of Tokyo, given Barack Obama and birthplace, that probability should be low. And it is defined in a straightforward softmax-like fashion. We take the raw F scores, which could be positive or negative reals, and we exponentiate them. <<PAGE_6>> In the numerator, we will keep e to the power fsro. And in the denominator, we will add that value over all possible values of o, which we call o prime. This normalizes the numerator and makes it add up to 1 over all candidates o. So think of a little edge r between s and an unknown o. We are trying to find out which particular o it might be. Similarly and conversely, given the relation and the object, we might want to find out what the subject is. Now, already you see the effect of asymmetry, and that will not be tackled for the first couple of models, but we'll eventually get to it. In the form of birthplace. So if I say, what's the probability of Barack Obama given birthplace and Hawaii? Because so many people were born in Hawaii, the probability of one Barack Obama will be quite small. And so the interesting thing to observe here is that Barack Obama is now competing with everyone born in Hawaii for the fixed chunk of probability one. And these are interesting implications for inference and this use in language models. Somehow in the literature, fixing subject and object and then trying to sample or find the probability of R has been relatively rare. I don't exactly know why. Now, if SRO is in the knowledge graph, we want the probability of O given SR to be large, and we want the probability of S given RO to be also large. How do we turn the probability to a loss function? That's fairly standard now in machine learning and deep learning. Given a knowledge graph with known triples i would like over all the triples in the knowledge graph the log probability of s given r o we add all of these up and then we also <<PAGE_7>> add the probability of o given s r take the log of that and then take the sum over that we want this entire sum to be maximized To turn that into a loss, we can negate that. So it's minus log probability of S given R O minus log probability O given S R. And then we sum over all triples in the kg and we want to minimize this negative log. So this only indirectly encourages small f of S prime R prime O prime for the negative triples. Remember that this probability has denominators. And to make the whole thing large, I need to make the denominator for all the wrong triples small. So that's how training would work. Observe also that the probabilities in the denominator involve summing over all elements in the knowledge graph. And so that could be quite expensive. To save computational cost, we might want to sample the denominator. So we take a positive fact SRO, and then we replace S with a randomly sampled entity S prime. So in our running example, we say Barack Obama born in Hawaii. And this is my positive triple. Now I cancel out Barack and replace by random samples like Gandhi. So this is assumed to be positive, to be negative. Why is that? Because there are so many people in the world that by random sampling, we would get another person born in Hawaii would be fairly small. This is called the local closed world assumption. It's not really valid given that knowledge graph is very incomplete. If we are sampling for people born in Hawaii versus elsewhere, this might succeed, but for classes which are much broader, we have to be really careful in not mistakenly sampling positive things and thinking they're negative. Even so, this helps replace the full sum in the denominator with more efficient to compute sampled estimates. We could do uniform sampling, but that would be too expensive. So we could sample K out of the E entities uniformly at random. Suppose there are total of E entities and we take K negative samples, then our expectation of that denominator has to be adjusted for that K, right? So we have to invert that by multiplying it by, E over K or K over E, depending on which way the sampling is going. And that restores the denominator to the expected sum over all entities in the knowledge graph. Now here, one problem is that if I'm doing <<PAGE_8>> the sampling of k things or entities uniformly at random, I might actually make the mistake of leaving out the gold or the true O. So we may have to introduce it back by force into this denominator sum, and this may further bias my estimates. So those are certain standard problems that we live with in defining these efficient sampled estimates. The other approach is to not bother with probabilities at all. So instead of trying to shoehorn an expression for a probability using a softmax, we might say that, look, the positive fact has a score F of SRO and any negative fact has a score of F of S prime, R prime, O prime. And the understanding is that at least one of them is wrong. and therefore the whole fact is wrong. So now we might say we want the good score, which is the score of the correct triple, to beat the bad score, which is the score of a triple which is wrong in at least one place, by an additive margin. So this is similar to support vector machines for those of you that are familiar with SVMs. This can be turned into what's called a hinge or a relu loss. So we say max of zero and then margin plus bad score minus good score. If our system, if our function f and the learned weights are already perfect or close to perfect, then the good score will exceed the bad score, not by itself, but even when assisted by the margin. And therefore, this quantity over here will go negative. Because it goes negative, my overall loss taken with a maximum of zero will turn out to be zero. And <<PAGE_9>> that's when we can stop the training because the system is already trained. Otherwise, if this loss turns out to be positive, we can differentiate this loss with regard to the shape of the function f and the embeddings of the entities and relations that participate in this expression. And that starts off back propagation into the model weights. The problem again with this formulation is very similar to the summing over a large number of entities. Because there are E entities and R relations, the number of possible facts in the universe are E squared R, each of which could be true or false. Only a small fraction is positive. So that means the number of constraints between good and bad is the number of good times the number of bad. So that's an astronomical and infeasible number. Now, generally the strategy that's followed is that I'm not going to take SRO and then perturb all three of them in all possible ways. I'm only going to perturb one thing at a time, and that limits the search space between the good bad pairs. So we would take SRO and only knock out the object, or we'll take SRO and only knock out the subject. So this leads to the formation of a batch for training a stochastic gradient based optimizer. For each positive fact SRO in the batch, we would sample K presumed negative facts by perturbing either the subject or the object, or maybe both. And then we would accumulate the losses over these K samples. in each we'll apply the hinge loss as mentioned before where we do margin plus bad minus good. So this is the good that is the bad and we want the good to beat the bad and this whole sum to come down to zero. So this is how many knowledge graph modeling and completion algorithms are trained. So far, we have assumed that a large score F makes the triple SRO more likely, and a small score makes it less likely. Some models may work with the opposite polarity. Some models may use a notion of distance, as we shall see. In that case, a large distance F will make SRO less likely, and a small distance will make it more likely. Usually, the loss and the optimization can be adjusted without much trouble to this flipped polarity. How are knowledge graph completion algorithms tested or evaluated? For this, people have defined a whole bunch of knowledge graph completion tasks or challenges. The <<PAGE_10>> knowledge graph is first sampled into three folds, which are usually disjoint, the training fold, the dev fold, and the test fold. The default is also called the validation for it sometimes. Given a test query, eventually when the system is deployed, which may be of the form SR question mark or question mark RO, the trained system must provide a ranked list for the blanks. For example, if the test query is SR question mark, the trained system should output candidates O1, O2, O3 and so on. And in the judgment of the system, O1 should be the best or most likely answer. On this list, we will define standard ranking measures like mean reciprocal rank or the number of hits at K. Let's take the second one first because that's easier to explain. So we go down to K elements and among the K elements, we see whether O1 is correct, O2 is correct and so on. And we count up the number of correct options and we divide it by K. So that's the hits at K. It's like you go to Google and do a search and the top 10 or top K results are given to you. How many of them make you happy? Now, this of course makes sense if, in general, if there are multiple objects for which the triple is true. And this can happen for relations that are not one is to one. A person can be only born in one place. So that's many to one, actually. Many people are born in a particular place. But there could be also the opposite, which is Barack Obama attended which academic institutions? And he might have attended two schools and three colleges. So it's one to many, one Barack Obama attending multiple educational institutions. So every time I get a hit, I will increment the counter and then eventually I'll divide it by K. Now mean reciprocal rank is a bit different. So think of going down this list again until I hit the first relevant entity, which is in this example at rank two. If I do that, my reciprocal rank will be half because two was the rank at which I hit my first success. So MRR is good for situations where there is exactly one correct answer, whereas HITSAT-K is useful if there are multiple correct answers. Now, think of what happens if I hit my correct object at rank 1. <<PAGE_11>> In that case, reciprocal rank will be equal to 1 over 1, which is 1, which is the maximum possible value. As the rank of the first correct answer goes down, reciprocal rank will dwindle to 0. Just like...hits at k ranges between 0 and 1, reciprocal rank will thus also range between 0 and 1. Now observe that the penalty for dropping something from rank 1 to rank 2 is to go down from 1 to half. Whereas the penalty for dropping something from 2 to infinity is to go down from half to 0. So that's sometimes found a bit extreme. For that reason, mean reciprocal rank is sometimes called the mean reciprocal rank. It's very mean. It penalizes you as much for dropping something from 1 to 2 as dropping from 2 to infinity, where you wouldn't even see it. There are other measures of knowledge graph completion efficacy, which we won't have time to get into, but you can read about it from the references. How about datasets? There are a few very prominent datasets that are used in this area of work. There is WordNet 18, which comes from the semantic or lexical network called WordNet. WordNet is a network where the nodes are words or synsets. So words which are synonyms of each other are collected into the same node and made into a sense set or synset, synonym set. And the edges represent Or toward relation types like hypernomy, a camel is a mammal is a living being, or synonymy that envy and jealousy are sometimes considered to be synonyms. The training and dev and test folds have sizes 141000, 5000 and 5000. And there are 18 relation types. <<PAGE_12>> Another popular data set in use is FB15K, which has 15,000 freebase entities connected with 1,345 relation types, and the training dev and test folds have about 480,000 and 50 to 60,000 fact triples in them. There have been refinements on these datasets. These datasets have been found to be somewhat simple in the sense that certain algorithms can cheat. And to prevent them from doing that, modified enhanced datasets, WN18RR and FB15K-237 have been evolved over time. There are other datasets like YAGO, yet another great ontology, and several other tasks. One important last note about evaluation is the element of filtering. So as I said, your query might be subject relation question mark, and now the ranking system has to provide a response list, O1, O2, et cetera, and so on. Suppose O6 and O8 are the designated gold correct answers. In that case, we might say that MRR is one over six, because that's the rank at which I found the first correct response. And mean average precision is the third measure might as well goal. Mean average precision says to get my first relevant response, I have to go to rank six. When I went to rank six, I found only one relevant response up to that time. For the moment, ignore the green color on O2. If I walk down two more steps to rank 8, I see 2, namely 6 and 8, correct responses. I will now average them up. 1 sixth plus 2 eighth divided by half So that's called mean average precision. And the mean reciprocal rank is 1 in 6, as we already saw. But now, suppose that in the training set, <<PAGE_13>> O2 was already known to be a correct answer. then you might say, look, it's unfair to penalize position 6 and 8 as 6 and 8 because training already revealed to us that 2 was relevant. So maybe we should knock O2 out of the list and regard the MRR as 1 out of 5 and the MAP as the average of 1 5th and 2 7th. This is a simple modification, but it's an important evaluation convention that is not followed sufficiently uniformly across papers. So when you compare the numbers reported in papers, you have to be careful about whether they did a filtered evaluation or an unfiltered evaluation. Next, we will start looking at what kind of functions can go into that box marked F and what form the input subject relation and object vectors or embeddings can take.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 53
  },
  "c52c8cdb-df08-465d-b1a0-0c3775c4817e": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "6c462e08-0d8e-4b04-8f85-c65387b029cc": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "341e590d-8da5-4230-9f9c-1e75102ebc72": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "74b23fa9-8801-4479-9125-444b69c4c659": {
    "filename": "2003.06112v4.pdf",
    "text": "<<PAGE_1>> 1202 ceD 42 ]GL.sc[ 4v21160.3002:viXra A Graph Convolutional Topic Model for Short and Noisy Text Streams NgoVan Linha,∗, TranXuan Bacha and KhoatThana aHanoiUniversityofScienceandTechnology,No.1,DaiCoVietroad,Hanoi,Vietnam ARTICLE INFO ABSTRACT Keywords: Learninghiddentopicsfromdatastreamshasbecomeabsolutelynecessarybutposedchallenging Topicmodels problemssuchasconceptdriftaswellasshortandnoisydata. Usingpriorknowledgetoenricha Graphconvolutionalnetworks topicmodelisoneofpotentialsolutionstocopewiththesechallenges. Priorknowledgethatisde- Knowledgegraph rivedfromhumanknowledge(e.g.Wordnet)orapre-trainedmodel(e.g.Word2vec)isveryvaluable Conceptdrift andusefultohelptopicmodelsworkbetter. However,inastreamingenvironmentwheredataar- Shorttexts rivescontinuallyandinfinitely,existingstudiesarelimitedtoexploitingtheseresourceseffectively. Especially,aknowledgegraph,thatcontainsmeaningfulwordrelations,isignored. Inthispaper,to aimatexploitingaknowledgegrapheffectively,weproposeanovelgraphconvolutionaltopicmodel (GCTM)whichintegrates graphconvolutional networks (GCN)intoatopicmodelandalearning methodwhichlearnsthenetworksandthetopicmodelsimultaneouslyfordatastreams.Ineachmini- batch,ourmethodnotonlycanexploitanexternalknowledgegraphbutalsocanbalancetheexternal andoldknowledgetoperformwellonnewdata. Weconductextensiveexperimentstoevaluateour methodwithbothahumanknowledgegraph(Wordnet)andagraphbuiltfrompre-trainedwordem- beddings(Word2vec). Theexperimentalresultsshowthatourmethodachievessignificantlybetter performances thanstate-of-the-art baselines intermsofprobabilistic predictive measureandtopic coherence. Inparticular,ourmethodcanworkwellwhendealingwithshorttextsaswellasconcept drift.TheimplementationofGCTMisavailableathttps://github.com/bachtranxuan/GCTM.git. 1. Introduction ingmethods[27,14,40]. Whilesparseorshortdatadoesnot provideaclearcontext,noisydatacanmisleadthemethods. Topicmodelingisa powerfulapproachtolearnhidden Asaresult,thegeneralizationoflearntmodelcanbelimited. topics/structuresinsidedata.LatentDirichletallocation(LDA) Exploitingaknowledgegraphisoneofthemostpoten- [7]isoneofthemostpopulartopicmodelsandhasbeenused tialsolutionstocopewiththesechallenges.Itisobviousthat widelyinavarietyofapplicationssuchastextmining[41], a knowledge graph that comes from global human knowl- recommendersystem [21], computer vision [13], bioinfor- edge(e.g. Wordnet)or a pre-trainedgraphis valuableand matics [37], etc. Recently, integrating external knowledge usefultoenrichatopicmodeltocopewithshortandnoisy into LDA emergesas an effective approachto improvethe textsinthestreamingenvironment. Moreover,aknowledge origin. Priorknowledge,whichisusedinpreviouswork,is graph(suchasWordnetoragraphtrainedonabigdataset) derivedfromhumanknowledge(suchasseedwords[26,22], containsmeaningfulwordrelationsthatseemtobestatical- Wordnet [2]) or pre-trained models like word embeddings thoughconceptdriftcan happen. Therefore,incorporating (Word2vec) [48, 24] learnt from big datasets. Therefore, thegraphintoatopicmodelshouldbetakenintoconsidera- priorknowledgecan enrich and improvethe performances tionfordatastreamstodealwithconceptdrift. oftopicmodels. Althoughexistingstudies[25,42,46,9]caneffectively Meanwhile,developinganeffectivelearningmethodfor exploita knowledgegraphin a static environment,theydo datastreamshasbecomeabsolutelynecessarybutposedchal- notconsiderfacingdatastreamsandthereforedonotwork lengingproblems[38]. Inthispaper,wewanttofocusontwo inthestreamingenvironmentwheredataarrivescontinually challenges. First,alearningmethodmustadaptwelltonew andinfinitely. Meanwhile,severalrecentmethods[8,28,32] datawithoutrevisitingpastdata. Inordertosolvethisissue cancopewithdatastreamswithoutrevisitingpastdata. But effectively,itmustdealwiththestability-plasticitydilemma theyarelimitedtoexploitingpriorknowledgeingeneraland [31,32,19,36,33]. Particularly,inthestreamingenviron- a knowledge graph in particular. An implicit idea behind ment, data is big, arrives continually, and concept drift in thesemethodsisthataposteriordistributionlearntinamini- whichthestatisticsofdatachangedramaticallycanhappen. batch is used as a prior for the following minibatch. As a Amethodshouldhaveamechanismtokeepacquiredknowl- result, in each minibatch, there are two priordistributions: edgefromlearningonpastdata. Thisknowledgeisusefulto Theoriginalpriordistributionwhichisinitializedinthefirst workonnewdatawhosecharacteristicsorpatternsaresim- minibatchandthenewpriorwhichisderivedfromthepos- ilar to those from the past data. Simultaneously, it should terior distribution learnt in the previous minibatch. Most bemoreplastic tolearna newconceptthatcanappearany of existing methods [8, 32, 19, 36] only use the former in time. Second,noisyandsparsedatathatisprevailinginthe thefirstminibatch,thenthelatterreplacestheformerinthe streamingenvironmentmakesalotofdifficultiesforlearn- nextminibatches. Afewmethods[28,3]exploitthemcon- Email:linhnv@soict.hust.edu.vn(N.V.Linh); currently. However,theydonotprovidea waytoexploita tranxuanbach1412@gmail.com(T.X.Bach);khoattq@soict.hust.edu.vn(K. knowledgegraph. Than) There are two main issues that we want to address for ORCID(s): Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page1of17 <<PAGE_2>> Graph Convolutional Topic Model an effectiveknowledgegraphexploitationin the streaming 2. RelatedWorkand Background environment.First,existingstreamingmethodsignoreprior In this section, we review streaming learning methods knowledge[8,19,32]orrequirepriorknowledgeofa vec- and graph convolutionalnetworks, then present how some tor form [3, 12]. In particular, they are unable to exploit streamingmethodsapplytoLDA. priorknowledgeofagraphformsuchasWordnetorapre- trained graph. For this problem, graph convolutional net- 2.1. RelatedWork works(GCN)[18]canprovideapotentialsolutiontoembed Recently,learningfromdatastreamshasbeenstudiedin- aknowledgegraphintopicspace.ThankstowhichGCNcan tensivelyandseveralmethodshavebeenproposedto solve encode high-order neighbourhoodrelationship/structure, it characteristic problemsin streaming environmentssuch as can learn good graph embeddings to enrich topic models. avoidingrevisitingallpastdata[15,8,29],adaptingtocon- Second,anautomaticmechanismwhichcontrolstheimpact ceptdrift[28],reducingcatastrophicforgetting[19,32],etc. of a knowledge graph in each minibatch plays an impor- Theyhaveachievedsomegoodresultsinbothpracticeand tantrole in balancingthe knowledgegraphandold knowl- theory[11]. edge learnt from the previous minibatch. A suitable bal- Withregardtolearningmanner,existingstudiescanbe ancingmechanismcanhelpexploiteffectivelybothkindsof divided into two major directions: Stochastic optimization knowledgeinpracticeandprovideapotentialsolutiontothe problem and recursive Bayesian learning. The first direc- stability-plasticitydilemma. tion [15, 29, 16] uses stochastic naturalGradientascent to Inthispaper,weproposeanovelmodel,namelyGraph maximizetheexpectationofthelikelihood. Stochasticvari- ConvolutionalTopicModel(GCTM),whichintegratesgraph ationalinference(SVI)[15]optimizesanempiricalexpecta- convolutionalnetworks (GCN) [18] into a topic model for tiononthewholedatasetandthereforerequirestheexistence data streams. We also develop a streaming method which ofafulldatasetwithafixednumberofdatainstances. This simultaneouslylearnsaprobabilistictopicmodelandGCN assumptionisunsuitableforstreamingenvironmentswhere inthestreamingenvironment. GCTMhassomebenefitsas the data can arriveinfinitely. PopulationvariationalBayes follows: (PVB) [29] alleviates this problemby anotherassumption. Itassumesthatthedataisgeneratedfromapopulationdis- • GCTMcaneffectivelyexploitaknowledgegraph,which tributionandwecansampleafixednumber(thesizeofthe comesfromhumanknowledgeorapre-trainedmodel population)𝑆 ofdatainstancesatatimeforcomputingand toenrichtopicmodelsfordatastreams,especiallyin optimizingtheexpectation.However,𝑆mustbetunedman- case of sparse or noisydata. We emphasizethatour ually to achieve good performance. In the other direction, workfirstprovidesawaytomodelpriorknowledgeof therecursiveBayesianapproach[8,28,3,19,32]baseson graphforminthestreamingenvironment. animplicitideathataposteriordistributionlearntinthepre- • Wealsoproposeanautomaticmechanismtobalance viousminibatchisusedto forma new priordistributionin theoriginalpriorknowledgeandoldknowledgelearnt the currentminibatch. Severalmethodssuch as Streaming inthepreviousminibatch. Thismechanismcanauto- variationalBayes(SVB)[8],Hierarchicalpowerprior(HPP) maticallycontroltheimpactofthepriorknowledgein [28],Variationalcontinuallearning(VCL)[32]usethefull each minibatch. When concept drifthappens, it can Bayesianapproachtoapproximatetheposteriordistribution, automaticallydecreasetheinfluenceoftheoldknowl- whileElasticweightconsolidation(EWC)[19]anditsvari- edgebutincreasetheinfluenceofthepriorknowledge ants [1, 36] base on the maximuma posterior(MAP) esti- tohelpourmethoddealwellwiththeconceptdrift. mate. Many methods [47, 19, 32, 36] in this direction are proposedtomakeneuralnetworksdealwiththechangesof Weconductexperiments1toevaluateGCTMwithboth tasksovertimeinstreamingenvironments.Inourwork,we ahumanknowledgegraph(Wordnet)andagraphbuiltfrom onlyconsidermethodsthatworkwellontopicmodelswith- pre-trainedWord2vec.Theextensiveexperimentsshowthat outchangingtask. ourmethodcanexploittheknowledgegraphwelltoachieve Meanwhile,tomitigatetheproblemsofnoisyandshort betterperformancesthanthestate-of-the-artbaselinesinterms texts, therearethreemainapproaches: Exploitingexternal ofprobabilisticpredictivemeasureandtopiccoherence. In knowledge,aggregatingshorttexts,anddevelopingnewsuit- particular, our method outperforms significantly baselines ablemodelsforshorttexts. Thefirstapproach[34,48,23] whendealingwithshorttextsaswellasconceptdrift. uses word embedding to enrich information and therefore Intherestofthepaper,therelatedworkandbackground achieves significant improvements in comparison with the arebrieflysummarizedinsection2. Section3presentsour originalmodels. However,existingstudiesinthisapproach proposed model and method along with some discussions havenotconsidereddevelopinga methodfor data streams. about them. We conduct experiments and analyse experi- They merely focuson a static environmentwithoutchang- mentalresultsinsection4. Theconclusionisdrawninsec- ingdata. Moreover,aknowledgegraphisalsoignored. In tion5. thesecondapproach,severalmethods[30, 35, 6, 27] mod- 1The implementation of GCTM is available at ifythedocumentinputofconventionaltopicmodelstoen- https://github.com/bachtranxuan/GCTM.git. hance word co-occurrence information. A strategy of ag- gregatingshorttextstoalongertextiswidelyusedinprac- Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page 2 of 17 <<PAGE_3>> Graph Convolutional Topic Model tice. The third approach [10, 44, 43, 45, 40] aims to pro- pose a new model which is more suitable to model word- 𝛼 𝜃 𝑧 𝑤 𝛽 𝜂 occurrence information for short texts instead of utilizing 𝑑 𝑑𝑖 𝑑𝑖 𝑘 conventionaltopic models. However, both the second and 𝑁 K 𝑑 thirdapproachesignoreexternalknowledgeinthestreaming D environment.Inourwork,wefocusondevelopinganeffec- Figure 1: Thegraphicalrepresentation ofLatentDirichlet Al- tivemethodtoexploitaknowledgegraphfordatastreams. location (LDA) We emphasizethatourmethodcanapplyto notonlyLDA butalsoawiderangeofexistingtopicmodels. Itmeansthat ourmethodcanimproveperformancesofexistingmodelsin isasfollows: boththesecondandthirdapproaches. 1. Drawtopics𝛽 ∼Dirichlet(𝜂)for𝑘∈[1,𝐾] In terms of exploiting prior knowledge in the stream- 𝑘 2. Foreachdocument𝑑: ingenvironment,KPS(KeepingpriorinstreamingBayesian (a) Drawtopicproportions𝜃 ∼Dirichlet(𝛼) learning) [3, 12] takes external knowledge into considera- 𝑑 (b) Foreachword𝑤 : tion, while the remaining methods neglect it. In the stan- 𝑑𝑛 i. Drawtopicassignment𝑧 ∼Multinomial(𝜃 ) dard view of Bayesian approach, a prior distribution does 𝑑𝑛 𝑑 ii. Drawword𝑤 ∼Multinomial(𝛽 ) 𝑑𝑛 𝑧 notplayanimportantrolewhendataisbigenough.Itseems 𝑑𝑛 to be the main reasonwhy almost existing methodsignore TrainingLDAisoftendividedintotwophases:Inferring priorknowledgeinstreamingenvironments. AlthoughKPS localvariables(𝑧 𝑑 and𝜃 𝑑)foreachdocument𝑑 andlearn- showsavitalroleofpriorknowledgefordatastreams,itre- ingglobalvariable(𝛽)sharedamongalldocuments.Almost mains two main drawbacks: The limit of prior knowledge streaminglearningmethodsforLDAarethesameinthefor- formandalackofbalancingmechanismbetweenpriorknowl- merbutaredifferentinthelatter. SVB [8], PVB[29], and edgeandoldknowledgelearntfrompreviousdata. Recently, HPP [29] approximate the posterior distribution of 𝛽 by a ourotherwork[4]aimstoexploitexternalknowledgeofdif- variationaldistribution𝑞(𝛽 𝜆)infullBayesianmanner.Note ferentforms(suchasvector,matrix)fordatastreams. How- thatVCLandSVBarethesame[39,32,11]whentheyare ever,itlacksaneffectivesolutiontocapturerelationbetween appliedtoaconjugatemode |llikeLDA.Moreover,VCL[32] nodesinaknowledgegraph. focuses on the problemof task changing, therefore, we do Recently,graphconvolutionalnetworks(GCN)[18]emergesnotconsiderinthispaper. Wewillbrieflypresentthelearn- asaneffectiveandefficientsolutiontolearngraphembed- ingalgorithmsofSVB,PVBandSVB-PP(asimpleversion dings.Inpractice,manypreviousstudiesshowthatGCNcan ofHPP)forLDA. workwellinawidevarietyofapplicationssuchasnodeclas- Suppose that in the streaming environment, the docu- sification [18], text classification [45], machine translation mentsarrivecontinuallyandarecollectedinsubsets(mini- [5],etc. Inarecentwork[49],GCNisusedinaninference batches) with 𝐷 documents. For each minibatch 𝑡, mean- network to learn a representationof a word co-occurrence field variational inference is used to approximate the true graphforinferringlocalvariables(thetopicproportionofa posterior distributions of variables by variational distribu- biterm subset) better in the biterm topic model. However, tions: thisworkdoesnotconsiderusingpriorknowledgetoenrich 𝐾 𝐷 𝑁 𝑑 atopicmodel. Ourworkaimsatadifferentgoal. Weexploit 𝑞(𝛽,𝜃 ,𝑧 )= 𝑞(𝛽 𝜆 ) 𝑞(𝜃 𝛾 ) 𝑞(𝑧 𝜙 ) 𝑑 𝑑 𝑘 𝑘 𝑑 𝑑 𝑑𝑛 𝑑𝑛 aknowledgegraphtoinferdirectlyglobalvariables(topics) 𝑘=1 𝑑=1( 𝑛=1 ) ∏ ∏ ∏ insteadoflocalvariables. Weemphasizethatourworkpro- (1) | | | videsageneralsolutionwithaknowledgegraphtoimprove where:𝑞(𝛽 𝜆 )=𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝜆 ),𝑞(𝜃 𝛾 )=𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛾 ) 𝑘 𝑘 𝑘 𝑑 𝑑 𝑑 existingmodels. and𝑞(𝑧 𝜙 ) = 𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝜙 )(𝜆,𝛾,and𝜙arevari- 𝑑𝑛 𝑑𝑛 𝑑𝑛 ational parameters). Let 𝑛 be the frequency of words 𝑣 2.2. OverviewofStreamingLearningMethodsfor | 𝑑𝑣 | indocument𝑑. ThelearningprocessofSVB,SVB-PP,and LDA | PVB are presented in Algorithms2, 3, and 4 respectively, Inthissubsection,webrieflypresentLDAandlearning where𝐸 [log𝜃 ]=𝜓(𝛾 )−𝜓( 𝐾 (𝛾 ))and𝐸 [log𝛽 ]= methodsthathelpLDAworkinthestreamingenvironment. 𝑞 𝑑𝑘 𝑑𝑘 𝑘=1 𝑑𝑘 𝑞 𝑘𝑣 𝜓(𝜆 ) − 𝜓( 𝑉 (𝜆 )) (𝜓 is a digamma function). The Supposethatadocument𝑑inadatasetcontains𝑁 𝑑words. 𝑘𝑣 𝑣=1 𝑘𝑣 ∑ three methods have the same algorithm (Algorithm 1) for Atopicisdefinedbyadistributionover𝑉 wordsofthevo- ∑ doinginferencelocalvariables. cabulary. LDA models𝐾 hiddentopics in the dataset and topicproportionofeachdocument. Let𝛽 ,...,𝛽 be𝐾 hid- 1 𝐾 dentopics,𝜃 betopicproportionofdocument𝑑,and𝑧 be 3. GraphConvolutional Topic Model 𝑑 𝑑𝑛 topicassignmentofword𝑛 indocument𝑑. LDAusestwo (GCTM)forData Streams Dirichletdistributionswithhyerparameters𝜂 and𝛼 togen- In this section, we first present a our proposed model, eratetopicsandtopicproportionsrespectively.Both𝛼and𝜂 thendevelopalearningmethodthatlearnsourmodelfrom areoftenselectedmanually. Thegraphicalrepresentationof thestreamingenvironment.Finally,wediscusssomeadvan- LDAisshowninFigure1. ThegenerativeprocessofLDA tagesofourmodel. Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page 3 of 17 <<PAGE_4>> Graph Convolutional Topic Model 𝐺,𝑋 Algorithm1LocalVB(d,𝜆) Initialize: 𝛾 𝑑 while(𝛾 ,𝜙 )notconvergeddo 𝑑 𝑑 ∀(𝑘,𝑣)set𝜙 𝑑𝑘𝑣 ∝ 𝑒𝑥𝑝(𝐸 𝑞[log𝜃 𝑑𝑘]+𝐸 𝑞[log𝛽 𝑘𝑣]) 𝑊̃ 𝑡−1 ℎ𝑡−1 𝑊̃ 𝑡 ℎ𝑡 (normalizedacrossk) ∀𝑘,𝛾 ←𝛼 + 𝑉 𝜙 𝑛 𝑑𝑘 𝑘 𝑣=1 𝑑𝑘𝑣 𝑑𝑣 endwhile return 𝛾 ,𝜙 ∑ 𝛽𝑡−1 𝛽̃𝑡−1 𝛽𝑡 𝛽̃𝑡 𝑑 𝑑 Algorithm2SVB Require: Hyper-parameter𝛼,𝜂 𝑧 𝑤 𝑧 𝑤 Ensure: Asequence𝜆(1),𝜆(2),… Initialize: ∀(𝑘,𝑣),𝜆( 𝑘0 𝑣) ←𝜂 𝑘𝑣 N N for𝑡=0,1,…do Collectnewdataminibatch𝐷 𝛼 𝜃 𝛼 𝜃 foreachdocument𝑑 in𝐷do (𝛾 ,𝜙 )←𝐿𝑜𝑐𝑎𝑙𝑉𝐵(𝑑,𝜆) M M 𝑑 𝑑 endfor Figure 2: The graphical representation of GCTM. Single lines ∀(𝑘,𝑣),𝜆𝑡 ←𝜆𝑡−1+ 𝜙 𝑛 demonstrate stochastic processes while double lines show de- 𝑘𝑣 𝑘𝑣 𝑑𝑖𝑛𝐶 𝑑𝑘𝑣 𝑑𝑣 endfor terministic processes ∑ Algorithm3SVB-PP nodeswhich are wordsin the vocabularyand 𝐸 is a set of Require: Hyper-parameter𝛼,𝜂,𝜌 𝑡 edgeswhichencodeparticularrelationshipsbetweenwords, Ensure: Asequence𝜆(1),𝜆(2),… weusegraphconvolutionalnetworkswith𝐿layerstolearn Initialize: ∀(𝑘,𝑣),𝜆(0) ←𝜂 𝑘𝑣 𝑘𝑣 therepresentationofnodes(words)inthegraph. Indetail, for𝑡=0,1,…do let 𝐴 (𝐴 ∈ ℝ𝑉×𝑉) be the adjacency matrix of 𝐺 and 𝑋 Collectnewdataminibatch𝐷 (𝑋 ∈ ℝ𝑉×𝑀) be a feature matrix in which each row 𝑋 𝑖 foreachdocument𝑑 in𝐷do (𝑖 ∈ {1,...,𝑉})isan𝑀-dimensionalfeaturevectorofeach (𝛾 𝑑,𝜙 𝑑)←𝐿𝑜𝑐𝑎𝑙𝑉𝐵(𝑑,𝜆) word𝑖. InGCN,eachlayercanencodeneighbourhoodrela- endfor tionshiptolearnarepresentationforallnodesinthegraph. Compute: 𝜆̃ =𝜌 𝑡𝜆𝑡 𝑘− 𝑣1+(1−𝜌 𝑡)𝜂 𝑘𝑣 Therepresentationℎ 𝑙 ofthenodesinlayer𝑙iscomputedas ∀(𝑘,𝑣),𝜆𝑡 𝑘𝑣 ←𝜆̃+ 𝑑𝑖𝑛𝐶𝜙 𝑑𝑘𝑣𝑛 𝑑𝑣 follows: endfor ∑ ℎ 𝑙 =𝑓 𝐷̃−1 2𝐴̃𝐷̃− 21 (ℎ 𝑙−1𝑊 𝑙+𝑏 𝑙) Algorithm4PVB ( ) where𝐴̃ =𝐴+𝐼 (𝐼 istheidentitymatrix),𝐷̃ = 𝐴̃ , Require: Hyper-parameter𝛼,𝜂,𝜌 𝑡,𝜏 0,𝜅,𝐵 𝑉 𝑉 𝑖𝑖 𝑗 𝑖𝑗 Ensure: Asequence𝜆(1),𝜆(2),… 𝑊̃ 𝑙 = {𝑊 𝑙,𝑏 𝑙} is the weight matrix of parameters ∑. ℎ 0 is Initialize: ∀(𝑘,𝑣),𝜆(0) ←𝜂 thefeaturematrix𝑋andtheactivationfunction𝑓 isusually 𝑘𝑣 𝑘𝑣 for𝑡=0,1,…do ReLUfunction. Intheoutputlayer,thedimensionofword Collectnewdataminibatch𝐷 representationissetby𝐾inordertofitthenumberoftopics foreachdocument𝑑 in𝐷do 𝐾 inLDA(ℎ 𝐿 isa𝑉 ×𝐾 matrixandeach𝐾-dimensional (𝛾 𝑑,𝜙 𝑑)←𝐿𝑜𝑐𝑎𝑙𝑉𝐵(𝑑,𝜆) vectorℎ 𝐿𝑣 istherepresentationofword𝑣). Then,weusea endfor transposeoperatoron ℎ 𝐿 to be able to integratewith topic Compute: 𝜌 =(𝜏 +𝑡)−𝜅 matrix𝛽 ofsize𝐾 ×𝑉. Thisdeterministicprocessissum- Compute: 𝜆̃𝑡 =𝜂 𝑘𝑣0 + 𝐵𝛼 𝑑𝑖𝑛𝐶𝜙 𝑑𝑘𝑣𝑛 𝑑𝑣 m ina pr ui tz ,e 𝑊d ̃co isnc ais we ely iga hs t: mℎ a= trix𝐺 o𝐶 f𝑁 G( Cℎ N0, ,𝐺 an;𝑊 d̃ ℎ) iw sh ae nre ouℎ t0 puis ta (ℎn ∀(𝑘,𝑣),𝜆𝑡 ←𝜌𝜆̃ +(1−𝜌)𝜆𝑡−1 𝑘𝑣 𝑡 𝑘𝑣 ∑ 𝑡 𝑘𝑣 isatransposematrixofℎ ). endfor 𝐿 Moreover, we need a mechanism to connect 𝛽 and ℎ. In general, this mechanism can be represented by a func- tion𝐹(𝛽,ℎ;𝜌)where𝛽 andℎareinputs,and𝜌isparameter. 3.1. ProposedModel For simplicity, we use a linear function to combine 𝛽 and In this subsection, we describe how to integrate GCN ℎ oneachtopic𝑘. Then,topicdistribution𝛽̃ is generated [18] into LDA to exploit a knowledge graph. Given prior 𝑘 by using the softmax function. In detail, for each topic 𝑘 knowledgeof graph form 𝐺 = (𝑉,𝐸) where 𝑉 is a set of Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page 4 of 17 <<PAGE_5>> Graph Convolutional Topic Model (𝑘∈{1,...,𝐾}), Because 𝑝(𝐷𝑡 𝛽̃𝑡,𝛼) is intractable to compute, we use variationalinferenceasin[7]todoinferencelocalvariables 𝛽̃ 𝑘 =𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝜌 𝑘𝛽 𝑘+(1−𝜌 𝑘)ℎ 𝑘) 𝑧and𝜃. Afterapp |lyingJenseninequality,wegetevidence lowerbound(ELBO): where 𝜌 is a scalar to balance 𝛽 and ℎ . In training, we 𝑘 𝑘 𝑘 mustlearn𝛽,𝑊̃ ,and𝜌. Fordatastreams,webaseontherecursiveBayesianap- 𝐿=− 1 𝛽𝑡−𝛽𝑡−1 2 − 1 𝑊̃ 𝑡−𝑊̃ 𝑡−1 2 proachtokeeptheimpactoflearntmodelfromtheprevious 2𝜎2 𝐹 2𝜎2 𝐹 𝛽 𝑤 minibatchtothecurrentone. Weassumethatthemodelsat twoconsecutiveminibatchesareconnectedbythefollowing +log || 𝑝(𝐷𝑡,𝜃|,|𝑧 𝛽̃𝑡,𝛼) 𝑞(|| 𝜃,𝑧)𝑑𝜃 || transition: ∫ 𝑧 𝑞(𝜃,𝑧) 1 ∑ | 1 𝑝(𝛽𝑡 𝛽𝑡−1,𝜎 𝛽)=(𝛽𝑡;𝛽𝑡−1,𝜎 𝛽2𝐼) ≥− 2𝜎2 𝛽𝑡−𝛽𝑡−1 2 𝐹 − 2𝜎2 𝑊̃ 𝑡−𝑊̃ 𝑡−1 2 𝐹 𝛽 𝑤 𝑝(𝑊̃ 𝑡 𝑊̃ 𝑡−1,𝜎 )=(𝑊̃ 𝑡;𝑊̃ 𝑡−1,𝜎2𝐼) | 𝑤 𝑤 + | 𝑞| (𝜃,𝑧)log|𝑝|(𝐷𝑡,𝜃,𝑧 𝛽|̃ |𝑡,𝛼) 𝑑𝜃 || where𝜎 and𝜎 areparametersthatrelatetothechangeof ∫ 𝑞(𝜃,𝑧) 𝛽 | 𝑤 𝑧 𝛽𝑡and𝑊̃ 𝑡around𝛽𝑡−1and𝑊̃ 𝑡−1respectively. ∑ 1 1 | ≥− 𝛽𝑡−𝛽𝑡−1 2 − 𝑊̃ 𝑡−𝑊̃ 𝑡−1 2 Thegenerativeprocess(Figure2)ofdocumentsinamini- 2𝜎2 𝐹 2𝜎2 𝐹 batch𝑡isdescribedexplicitlyasbelow: 𝛽 𝑤 1. Draw𝑊̃ 𝑡 ∼(𝑊̃ 𝑡;𝑊̃ 𝑡−1,𝜎2𝐼) +𝐸 𝑞(𝜃,𝑧)|[|log𝑝(𝐷𝑡,𝜃||,𝑧 𝛽̃𝑡,𝛼)]−||𝐸 𝑞(𝜃,𝑧)[log𝑞|(|𝜃,𝑧)] 2. Calculateℎ𝑡 =𝐺𝐶𝑁(ℎ ,𝐺;𝑊𝑤 ̃ 𝑡) =𝐸𝐿𝐵𝑂 0 | 3. Draw𝛽𝑡 ∼(𝛽𝑡;𝛽𝑡−1,𝜎2𝐼) 𝛽 where𝑞(𝜃,𝑧)isafactorizedvariationaldistribution: 4. Calculatetopicdistribution: 𝐷 𝑁 𝑑 𝛽̃𝑡 =𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝜌𝑡𝛽𝑡+(1−𝜌𝑡)ℎ𝑡)) (2) 𝑞(𝜃,𝑧)= 𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝜃 𝛾 ) 𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝑧 𝜙 ) 𝑑 𝑑 𝑑𝑛 𝑑𝑛 𝑑=1( 𝑛=1 ) 5. Foreachdocument𝑑: ∏ ∏ (4) | | (a) Drawtopicmixture:𝜃 ∼𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛼) 𝑑 (b) Forthe𝑛𝑡ℎ wordof𝑑: 𝛾 and𝜙arevariationalparameters. When𝛼,𝜎 and𝜎 are 𝛽 𝑤 i. Drawtopicindex: 𝑧 ∼𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝜃 ) fixed,wemaximizeELBOwithrespecttolocalparameters 𝑛 𝑑 ii. Drawword: 𝑤 ∼𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝛽̃𝑡 ) (𝛾and𝜙)andglobalparameters(𝜌𝑡,𝛽𝑡,and𝑊̃ 𝑡). According 𝑛 𝑧 𝑛 to[7],theupdateequationsoflocalparametersare: 3.2. LearningGCTM Ataminibatch𝑡,newdocumentsarriveandarecollected 𝑁 inasetof𝐷documents. Theposterior 𝛾 ←𝛼 + 𝑑 𝜙 for𝑘=1,...,𝐾 (5) 𝑑𝑘 𝑘 𝑑𝑛𝑘 𝑝(𝛽𝑡,𝑊̃ 𝑡 𝐷𝑡,𝛽𝑡−1,𝑊̃ 𝑡−1,𝐺,𝑋,𝜌𝑡,𝛼,𝜎 𝛽,𝜎 𝑤) ∑𝑛=1 𝑉 isexpressedasfollows: 𝜙 𝑑𝑛𝑘 ∝exp(𝐸 𝑞[log𝜃 𝑑𝑘]+ 𝐼[𝑤 𝑑𝑛 =𝑣]log𝛽̃ 𝑘𝑣) | 𝑣=1 ∑ (6) log𝑝(𝛽𝑡,𝑊̃ 𝑡 𝐷𝑡,𝛽𝑡−1,𝑊̃ 𝑡−1,𝐺,𝑋,𝜌𝑡,𝛼,𝜎 ,𝜎 ) 𝛽 𝑤 where𝐼[⋅]isanindicatorfunctionand𝐸 [log𝜃 ]=𝜓(𝛾 )− ∝log𝑝(𝛽𝑡,𝑊̃ 𝑡,𝐷𝑡 𝛽𝑡−1,𝑊̃ 𝑡−1,𝐺,𝑋,𝜌𝑡,𝛼,𝜎 𝛽,𝜎 𝑤) 𝜓( 𝐾 (𝛾 ))(𝜓 isadigammafunction𝑞 ). 𝑑𝑘 𝑑𝑘 ∝log𝑝(𝑊̃ 𝑡| 𝑊̃ 𝑡−1,𝜎 )+log𝑝(𝛽𝑡 𝛽𝑡−1,𝜎 ) R𝑘 e= g1 ard𝑑 i𝑘 ngglobalparameters,weextractthepartofELBO 𝑤 𝛽 +log𝑝(𝐷𝑡 𝛽𝑡,𝑊̃ 𝑡,| 𝐺,𝑋,𝜌𝑡,𝛼)=𝐿 (3) w.r∑ .t𝜌𝑡,𝛽𝑡,and𝑊̃ 𝑡: | | We learn GCTM based on maximizing 𝐿 (Eq 3). We 𝐸𝐿𝐵𝑂(𝜌𝑡,𝛽𝑡,𝑊̃ 𝑡) apply𝛽̃𝑡 =𝑠𝑜𝑓𝑡𝑚| 𝑎𝑥(𝜌𝑡𝛽𝑡−1+(1−𝜌𝑡)𝐺𝐶𝑁(𝑋,𝐺;𝑊̃ 𝑡))into 1 1 =− 𝛽𝑡−𝛽𝑡−1 2 − 𝑊̃ 𝑡−𝑊̃ 𝑡−1 2 Eq3: 2𝜎2 𝐹 2𝜎2 𝐹 𝛽 𝑤 𝑀 𝑁 𝑑||𝑉 𝐾 || || || 𝐿=log𝑝(𝑊̃ 𝑡 𝑊̃ 𝑡−1,𝜎 𝑤)+log𝑝(𝛽𝑡 𝛽𝑡−1,𝜎 𝛽) + 𝐼(𝑤 𝑑𝑛 =𝑣)𝜙 𝑑𝑛𝑘log(𝛽̃ 𝑘𝑡 𝑣) (7) +log𝑝(𝐷𝑡 𝛽̃𝑡,𝛼) 𝑑 ∑=1 ∑𝑛=1 ∑𝑣=1 ∑𝑘=1 1 | 1 | where𝛽̃𝑡 =𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝜌𝑡𝛽𝑡−1+(1−𝜌𝑡)𝐺𝐶𝑁(𝑋,𝐺;𝑊̃ 𝑡)). =− 𝛽𝑡−𝛽𝑡−1 2 − 𝑊̃ 𝑡−𝑊̃ 𝑡−1 2 2𝜎2 | 𝐹 2𝜎2 𝐹 WeuseAdam[17]tomaximize𝐸𝐿𝐵𝑂(𝜌𝑡,𝛽𝑡,𝑊̃ 𝑡). 𝛽 𝑤 ThewholelearningprocessofGCTMispresentedinAl- +log𝑝(𝐷||𝑡 𝛽̃𝑡,𝛼) || || || gorithm5. Linh Ngo Van et al.:| PreprintsubmittedtoElsevier Page 5 of 17 <<PAGE_6>> Graph Convolutional Topic Model Algorithm5LearningGCTM Table 1 Require: Graph G, hyper-parameter 𝛼, data sequence Some statistics about thedatasets. {𝐷1,𝐷2,...} Dataset Vocab Training Evaluation words/doc Ensure: 𝑊̃ ,𝛽,𝜌 Agnews 32,483 110,000 10,000 24.9 Initialize𝑊̃ 0,𝛽0randomly TMN 11,599 31,604 1,000 24.3 forminibatch𝑡withdata𝐷𝑡do NYT-title 46,854 1,664,127 10,000 5.0 Compute𝛽̃byEquation(2) Yahoo-title 21,439 517,770 10,000 4.6 foreachdocument𝑑 in𝐷𝑡do Agnews-title 15,936 108,400 10,000 4.9 TMN-title 2,823 26,251 1,000 4.6 Infer 𝛾 and 𝜙 by iteratively updating (5) and (6) 𝑑 𝑑 Irishtimes 28,816 1,364,669 10,000 5.0 untilconvergence Twitter 35072 1247321 10000 6.2 endfor Update𝑊̃ 𝑡,𝛽𝑡,𝜌𝑡byusingAdam[17]tomaximize(7) endfor Finally, our method learns both GCN and LDA simul- taneouslyinthestreamingenvironment. Moregenerally,it canbeextendedtotrainahybridmodelofaneuralnetwork 3.3. Discussion andaprobabilisticmodelfordatastreams. Inthissubsection,wediscusstheadvantagesofGCTM andcompareitwithothermethods. GCTMcanwellexploit 4. Evaluation anexternalknowledgegraphfordatastreams. Therefore,we discusssomeaspectsofthistopic. Inthissection,weconductintensiveexperimentstoeval- First,GCN,whichisaneffectivemodeltoencoderela- uatetheperformanceofourmethodintermsoflogpredic- tionshipsbetweenedgesinagraph,canlearngraphembed- tiveprobabilityandtopiccoherenceonseveraldatasets(both dingtofittheformoftopicmatrixinLDA.Therefore,our shortandregulartextdatasets)inthestreamingenvironment. methodcanutilizethegraphembeddingtoenrichinforma- Wealsoexaminehowourmethoddealswithconceptdrift. tionforlearningtopicsbetter. Tothebestofourknowledge, Finally,weinvestigatethesensitivityofourmethodw.r.thy- this is the first work which can exploit a prior knowledge perparameters. graphfor LDA in the streaming environment. Meanwhile, almostexistingstreamingmethodsignorepriorknowledge; 4.1. DatasetsandBaselines and KPS aims to use but is limited to prior knowledge of Weconductexperimentson6shorttextdatasets(NYT- vectorform. title2,Yahoo-title3,TagMyNews-title(TMN-title),Irishtimes4), Second,ineachminibatch,ourmethodprovidesamech- Agnews-title,Twitter5and2regulartextdatasets(Agnews6, anism to automatically balance old knowledge (that is ob- TagMyNews(TMN)7).TheYahoo-titleandTwitterdatasets tainedfromthepreviousminibatch)andapriorknowledge [27,40]arecrawledfroma forumandasocialnetworkre- graph. Meanwhile,KPS[3]mustmanuallycontroltheim- spectively,thereforetheyoftencontainnoisytexts. Thedatasets pactofpriorknowledgeineachminibatch,anditisdifficult are preprocessed with some steps such as: tokenizing, re- totunethisimpactinstreamingenvironments. movingstopwordsandlow-frequencywords(whichappear Third,ourmethodcandealwithconceptdriftwellwhen inlessthan3documents)tobuildthecorrespondingvocab- dataarrivescontinually. Usingexternalknowledgethatcov- ularies,andremovingextremelyshortdocuments(lessthan ersorrelatestonewconceptsisaneffectivesolutiontohan- 3 words). The statistics of these datasets are described in dle conceptdrift. However, it is difficult to guaranteethat Table1. Experimentingontheshorttextcorpora,inwhich priorknowledgecontainsinformationaboutnew concepts. eachdocumentcontainsabout5words,helpsustoexamine Fortunately, this is possible with a knowledge graph such the role of a knowledge graph in case of short and sparse asWordnetoragraphtrainedona bigdataset. Especially, data. when new topics occur, a set of new wordscan be used to describethem. However, the words and their relationsare Knowledgegraphs: Intheseexperiments,weexploitex- alsoincludedintheknowledgegraph.Asaresult,exploiting ternalknowledgewhichisderivedfrombothhumanknowl- thegraphhelpsourmethodtolearnnewtopicsinnewarriv- edge (Wordnet8) and a pre-trained model (Word2vec9) on ingdocuments.Ontheotherhand,manystreamingmethods a big dataset. Wordnet and Word2vec are used to create sufferfromconceptdriftbecause theyonlyuse oldknowl- 2 knowledge graphs respectively. In terms of building the edgelearntfromthepreviousminibatchaspriorinthecur- Wordnetgraph,foreachwordinthevocabularyofeachdataset, rent minibatch. It means that emphasizing the old knowl- 2http://archive.ics.uci.edu/ml/datasets/Bag+of+Words edge prevents the model from adapting to new data. HPP 3https://answers.yahoo.com/ [28] also has a mechanism to combine old knowledge and 4https://www.kaggle.com/therohk/ireland-historical-news/ initialprior. Itdealswellwithconceptdriftincasesthatthe 5http://twitter. com/ priorisgoodenoughandthemechanismhelpstoforgetthe 6https://course.fast.ai/datasets 7http://acube.di.unipi.it/tmn-dataset/ oldknowledge. Inour work, we also use a similar mecha- 8https://Wordnet.princeton.edu/ nism,butexploitbetterexternalknowledge. 9http://nlp.stanford.edu/projects/glove/ Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page 6 of 17 <<PAGE_7>> Graph Convolutional Topic Model wegetallwordsthathaveeithersynonymorantonymrela- therangeofeachhyperparameterissetasfollows: themul- tionships with it from Wordnet to create a set of its word tiple powerprior𝜌 ∈ {0.6,0.7,0.8,0.9,0.99}forSVB-PP, neighbors. However, in order to avoid a big graph, we re- thepopulationsize𝑆 in{103,104,105,106}andtheforget- move neighborsthat are outof vocabulary. Then, an edge ting factor 𝜅 in {0.7,0.8,0.9,0.99}for PVB, and variance is built based on neighbor relation and the weight of each 𝜎 = 𝜎 = 𝜎 ∈ {0.1,1,10}, the number of GCN layers 𝛽 𝑤 edgeistheWu-Palmersimilarityofthecorrespondingpair 𝐿 = 2 forGCTM. We list the besthyperparametersof the ofwords. Weemphasizethatwetakeneighborswithalldif- methodsfromgridsearchinappendixC. ferentmeaningsforeachword. Therefore,althoughconcept drifthappensorawordisusedinadifferentmeaningfrom Performancemeasure: Weuse2measurestoevaluatethe previouslyappearingmeanings,theWordnetgraphincludes methods: Logpredictiveprobability(LPP)[15]whichcon- thismeaningto enricha topicmodel. For theothergraph, sidersthegeneralizationofamodelandNormalizedpoint- webaseonWord2vectocomputecosinesimilaritybetween wisemutualinformation(NPMI)[20]whichexamstheco- apairofwordsinthevocabulary. Then,foreachword,we herenceoftopics. WemeasuretheLPPsofthemethodsafter selectthetop200wordswithhighestsimilarscoretobuilda every minibatch. However, due to computingon all docu- graph.The2graphsareusedaspriorknowledgeforGCTM. mentsofeachdataset,NPMIisonlymeasuredafterfinish- First,weignorenodefeaturestofocusonevaluatingthe ingthewholetrainingprocess. Wedescribethesemeasures impactofaknowledgegraphinstreamingenvironments. It inappendicesAandB. meansthat𝑋 issettobetheidentitymatrix𝐼 . Then,we 𝑉 4.2. Experimentsondatasetswithfixedbatchsize investigatethecombinationofbothaknowledgegraphfrom Due to the lack of time informationon almost datasets WordnetandnodefeaturesfromWord2vectoenrichatopic (exceptIrishtimesdataset),wesimulatethestreamingenvi- model. ronmentby followingexperimentaldesignsin [8, 29]. We Baselines: Weuse3state-of-the-artbaselinestolearnLDA conductexperimentswiththescenarioson6datasets(TMN, fromdatastreamsincomparisonwithourmethod.Webriefly TMN-title, Agnews, Agnews-title, Yahoo-title, and NYT- describethesemethodsasfollows: title). In each dataset, we randomly select a holdout test set (which contains documentswith more than or equal to • PopulationvariationalBayes(PVB)[29]usesstochas- 5words)andthenshuffletheremainingdocumentsanddi- tic naturalGradientascent to maximize the expecta- videthemintominibatcheswithfixedbatchsizefortraining. tionofthelikelihoodofdata. Basedonthesizeofeachdataset,wesetbatchsizeto500for TMN,TMN-title,1000forAgnews,Agnews-title,and5000 • StreamingvariationalBayes(SVB)[8]basesonrecur- forYahoo-title,NYT-title. Theinformationoftrainingand sive Bayesian approach. SVB can only use external testsetsisdescribedinTable1. knowledgeencodedintheprioratthefirstminibatch, In terms of LPP, Figure 3 shows the experimental re- thenignoresitinthenextminibatches. sults. Wehavesomenoticeableobservationsfromthesere- • Powerprior(SVB-PP)10[28]isanextensionofSVB. sults. First,bothGCTM-WNandGCTM-W2Vsignificantly Itcanexploittheoriginalpriordistributionthroughall outperformthebaselines. Providinganexternalknowledge minibatchesandprovidesamechanismtocontrolthe graph from Wordnet or Word2vec is the main reason why impactofthepriorineachminibatch. theGCTM-basedmethodsachievebetterperformancesthan the baselines which do not exploit prior knowledge. Sec- • GCTM-WN:GCTMusesaknowledgegraphfromWord- ond, bothGCTM-WN and GCTM-W2Vare inferiorto the net. baselinesinafewbeginningminibatchesonNYT-titleand Yahoo-title datasets, while they need more minibatches to • GCTM-W2V:GCTMexploitsaknowledgegraphfrom catchupwiththebaselinesontheremainingdatasets. Due Word2vec. tohavingtolearnalotofparametersingraphconvolutional Thesamehyperparametersinallmethodsaresetthesame. networks,theGCTM-basedmethodsneedmoredatatolearn Indetail,wesetthehyperparameterofDirichletdistribution the model. Moreover, the differences of batchsize among 𝛼 =0.01fortopicproportionofeachdocument,thenumber datasets lead GCTM-WN and GCTM-W2V to require the of topics𝐾 = 50 for Agnews, Agnews-title, TMN, TMN- differentnumbersofminibatchestoovercomethebaselines. title and 𝐾 = 100 for Yahoo-title, NYT-title, Irishtimes. Third,theperformancesofthebaselinesonlyincreaseina Wenotethatthebaselinescannotexploitapriorknowledge fewbeginningminibatches,thengraduallydecreaseonshort graph,theyonlyuseaDirichletpriorwithahyperparameter text datasets. It means that the baselines deal badly with 𝜂 = 0.01foreachtopicasintheoriginalpapers. Forother shorttextseventhoughthedataisbig.Incontrast,theGCTM- hyperparameters, we use grid search to determine the best based methods with external knowledge can work well on hyperparameterforeachmethodoneachdataset. Indetail, short texts. Finally, in comparison with the baselines, the improvementsoftheGCTM-basedmethodsontheshorttext 10Duetorequiringnon-trivialefforts,SVB-HPPisnotincludedinthis datasets(Agnews-titleandTMN-title)aremoreremarkable paper. However, theoriginal work[28]showedthatifSVB-PPistuned thanthoseontheregulartextdatasets(AgnewsandTMNre- well,itisoftencomparabletoSVB-HPP. spectively).Thisprovidesconvincingevidenceofexploiting Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page 7 of 17 <<PAGE_8>> Graph Convolutional Topic Model −9.0 −10.5 0 150 300 PPL NYT-title Agnews-title TMN-title −8.5 −7.6 −9.0 −8.0 −9.5 −8.4 0 40 80 0 20 40 −8.8 −9.6 0 40 80 Minibatch PPL Yahoo-title Agnews TMN −8.8 −8.0 −9.6 −8.8 −10.4 0 40 80 0 25 50 Minibatch Minibatch SVB SVB-PP PVB GCTM-WN GCTM-W2V Figure 3: Performance of themethods in terms of generalization as learning from more data. Higher is better. Table 2 Performance of themethods in terms of topic coherence. Higher is better. Dataset GCTM-WN GCTM-W2V SVB SVB-PP PVB Agnews 0.287 0.263 0.005 0.005 0.018 Agnews-title -0.026 -0.067 -0.114 -0.111 -0.107 TMN 0.0791 0.073 -0.015 -0.054 -0.019 TMN-title 0.032 0.021 -0.103 -0.105 -0.090 NYT-title 0.266 0.246 -0.069 -0.065 -0.069 Yahoo-title 0.171 0.173 -0.087 -0.088 -0.076 externalknowledgefordatastreams. 4.3. Experimentsondatasetwithtimestamp Regarding NPMI, Table 2 shows the experimental re- SinceonlytheIrishtimesdatasethasinformationabout sults. Both GCTM-WN and GCTM-W2V also outperform time, weonlyconductexperimentswithtimestamponthis thebaselinesbynoticeablemargins. BecauseWordnetand dataset. Wegetthedocumentsoverperiodofeachmonthto Word2vec,whichencodetheinformationofwordsemantic createaminibatch.GCTMistrainedonaminibatchandthe andlocalcontexts,helpLDAtolearncoherenttopics. The nextminibatchisusedtomeasureLPP.Weusethisscenario regulartextdatasets(AgnewsandTMN)containmoreinfor- toevaluatethemethodsinarealstreamingenvironment.We mationofwordco-occurrencethantheshortones,therefore, alsoconductextraexperimentswiththepreviousscenarioon themethodsworkbetterontheregulardatasets. Moreover, thisdatasettoinvestigatethedifferencesbetweenthescenar- the GCTM-based methodsalso performmore significantly ios. Fortheextraexperiments,wefixbatchsizeto5000and ontheshorttextdatasets. thesizeoftestsetto10000. Inbothscenarios,weevaluate ThedifferentgraphsfromWordnetandWord2vechave NPMIonalldocumentsinthedataset. differentimpactsin termsofLPPandNPMI.Itseemsthat TheLPPresultsarereportedinFigure4. WhileFigure the word-embeddings-based graph improves LDA slightly 4(a)showstheresultsonthedatasetwithtimestamp,Figure betterthantheWordnet-basedgraphintermsofLPPonall 4(b)illustratestheresultsonthedatasetwithfixedbatchsize. the datasets (Figure 3). However, GCTM-W2V performs It is obviousthat the behavioursof lines in both scenarios worsethanGCTM-WN intermsof topiccoherence(Table aresimilar. Inthetimestampscenario,theperformancesof 2). the GCTM-based methods are significantly better than the baselinesintermsofLPP.However,thelinesinFigure4(a) Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page 8 of 17 <<PAGE_9>> Graph Convolutional Topic Model −8.8 −9.6 0 100 200 Minibatch PPL (a) −9.0 −9.6 −10.2 0 100 200 Minibatch PPL (b) SVB SVB-PP PVB GCTM-WN GCTM-W2V Figure 4: Performance of the methods on the Irishtimes dataset. While Figure (a) shows the results on the dataset with timestamp,Figure (b) reports theresults on thedataset with fixed batchsize. Table 3 PerformanceofthemethodsintermsoftopiccoherenceontheIrishtimeswithbothtimestampandfixedbatchsize. Dataset GCTM-WN GCTM-W2V SVB SVB-PP PVB Timestamp 0.127 0.124 -0.068 -0.083 -0.082 Fixed batchsize 0.002 0.002 -0.068 -0.072 -0.065 are more curved than the ones in Figures 4(b). Since test minibatch. Moreover,NPMIsofthebaselinesdonotobtain setineachminibatchisthenextoneintheexperimentswith positiveresults. Byusingexternalknowledgegraphs,both timestamp,theresultsarenotassmoothasthoseintheother GCTM-WNandGCTN-W2Vachievebetterresultsthanthe experimentswith fixed holdouttest set. Meanwhile, Table baselines on both measures. These results provide experi- 4showsthattheGCTM-basedmethodsalsoachievebetter mentalevidencewhyexploitingexternalknowledgeingen- NPMIresultsthanthebaselinesinboththescenarios. eralandknowledgegraphinparticularisaneffectivesolu- tiontodealwithnoisyandshortdata. 4.4. Experimentsonnoisydata In this subsection, we consider how the methods deal 4.5. Experimentsondatasetwithconceptdrift with noisy texts. We conduct experiments on Yahoo-title andcatastrophicforgetting andTwitterdatasets. WhiletheTwitterdatasetisacollection Concept drift: We design a scenario to evaluate the oftweetsfromasocialnetwork11,theYahoo-titledatasetis methodswhendealingwithconceptdrift. Wesimulatecon- crawled from a question and answer forum12 where users cept drift dataset on the Irishtimes dataset in which docu- freely post questions and others help to answer. Because mentsarecategorizedin6classeswhoselabelsare\"News\", texts fromboth the forum and social network are informal \"Opinion\",\"Sport\",\"Lifestyle\",\"Business\", \"Culture\". We andcontainnoises,wecanusethemtoevaluateperformance divide the dataset into minibatcheswith constraints as fol- ofthemethodswhendealingwithnoisydata. lows: Documentsinthesameminibatchhavethesameclass Figure5andTable??showtheperformancesofthemeth- labelandtheminibatchesofthesameclassareusedconsec- odsin termsof generalizationand topic coherencerespec- utivelytotrainthemodel. Duetodataimbalanceinclasses, tively. Itisstraightforwardtoseethatshortandnoisytexts batchsizeisonlysetto2000. Aftertrainingthemodelina notonlyrarelyprovidethebaselineswithenoughword-occurrencmeinibatch,weusethenextonetomeasureLPP.Inthissce- information but also mislead them. As a result, the LPPs nario,conceptdriftariseswhendatachangesfromaparticu- ofthebaselinesdecreasewhenmoretextsarriveaftereach larclasstoanewone. Itrequiresthemodeltoadaptquickly todataofanewclass. Weconductexperimentswith2sce- 11http://twitter.com/ 12https://answers.yahoo.com/ narioswhicharedifferentintheorderoflabels. Thenumber Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page 9 of 17 <<PAGE_10>> Graph Convolutional Topic Model −8.8 −9.6 0 40 80 Minibatch PPL Yahoo-title −7.5 −9.0 −10.5 0 100 200 Minibatch PPL Twitter SVB SVB-PP PVB GCTM-WN GCTM-W2V Figure 5: Performance of themethods in terms of generalization when dealing with noisy texts. Higher is better. Table 4 Performance of themethods in terms of topic coherence when dealing with noisy texts. Higher is better. Dataset GCTM-WN GCTM-W2V SVB SVB-PP PVB Yahoo-title 0.171 0.173 -0.087 -0.088 -0.076 Twitter -0.009 -0.010 -0.062 -0.060 -0.047 oftextsin\"News\"issignificantlybiggerthanthoseofother forgettingphenomenoninwhichthemethodsforgetthelearnt labels. Wewillchangetheorderofthislabel. Indetail,the knowledgewhentrainingonnewdata. Wefollowthemea- first scenario uses the order of labels: \"News\", \"Opinion\", sureofcontinuallearningstudies[32,19,36]toconsiderthe \"Sport\",\"Lifestyle\",\"Business\",\"Culture\"whileintheother forgettingproblem. In detail, we again use 2 experimental scenario,labelsareutilizedsequentiallyinthefollowingor- scenariosinconceptdrift,however,wecreateahold-outtest der: \"Sport\", \"Opinion\", \"News\", \"Lifestyle\", \"Business\", setforeachclass. Eachhold-outtestsetofeachclasscon- \"Culture\". sistsof2000texts. Afterfinishingtrainingalltextsofaclass, Figures6and7illustratetheperformancesofthemeth- wecalculatetheaverageLPPonthehold-outtestsetsofthe odsinthefirstandsecondscenariosrespectively. Eachfig- currentandpreviousclasses. ThehighertheaverageLPPof ureincludes5subfigures:Themainfigureand4smallextra amethodis,thebetterthismethoddealswiththeforgetting figures(whichareextractedfromthemainfiguretozoomin problem. whenconceptdrifthappens). ThemainfiguresinbothFig- Figure8andFigure9showtheaverageLPPsofthemeth- ures6and7showthatGCTM-WNandSVB-PPachievebet- odsaftereachclassinboththescenarios. Itisobviousthat terresultsthanPVBandSVB.Thankstoabalancingmech- GCTM-WNandGCTM-W2Vstillachievebetterresultsthan anism,bothGCTM-WNandSVB-PPreducetheimpactof thebaselinesatalmostevaluationtimes. Theyareonlyinfe- oldknowledgelearntfromdataofpreviousclassestowork riortothebaselinesafewtimessuchasattheclass\"Sport\" well on new data of the current class when concept drift inFigure8andtheclasses\"News\"and\"Lifestyle\"inFigure happens. It is obviousthatusing a knowledgegraphhelps 9. Therefore,inboththescenarios,GCTMnotonlyadapts GCTM-WNoutperformSVB-PP.Furthermore,theextrafig- morequicklytoconceptdriftbutalsoreducesmorenotice- uresillustratethattheperformancesofthemethodsdropdra- ablythecatastrophicforgettingphenomenonincomparison maticallywhenconceptdriftarises. However,GCTM-WN withthebaselines. ItmeansthatGCTMcandealbetterwith increases significantly in a few minibatches, then remains theplasticity-stabilitydilemmathanthebaselines. However, stable. TheseresultsdemonstratethatGCTM-WNcanadapt itseemsthatGCTMdealswithconceptdriftbetterthanfor- quicklytoconceptdrift. gettingproblem.TheLPPsofGCTMinFigures6and7are Catastrophicforgetting: Weexaminethecatastrophic significantlyhigherthanthoseinFigures8and9. Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page10 of 17 <<PAGE_11>> Graph Convolutional Topic Model −9.0 −10.5 90 100 110 120 130 PPL −8 −9 −10 150 160 170 180 −8.8 −9.6 510 520 530 540 PPL −8 −9 −10 540 550 560 570 580 −7.5 −9.0 −10.5 0 100 200 300 400 500 600 700 Minibatch PPL SVB SVB-PP GCTM-WN PVB Figure 6: Performance of themethods when dealing with concept drift 4.6. Ablationstudies 4.6.2. SensitivityofGCTMw.r.t. hyperparameters Inthissubsection,weinvestigatetheeffectivenessofen- Inthissubsection,weexaminethesensitivityofGCTM richingWordnetgraphwithnodefeaturesfromWord2vecas w.r.t. 𝜎 and the numberof topics 𝐾. We use the scenario wellasanalyzethesensitivityofGCTMw.r.thyperparame- withfixedbatchsize(1000)toconductexperimentsontwo ters. datasets: AgnewsandAgnews-title. WemeasuretheLPPof GCTM-WN when one of these parameters is changed and 4.6.1. EnrichingWordnetgraphwithnodefeatures theotherisfixed. fromWord2vecinGCTM The sensitivity of GCTM-WN w.r.t. 𝜎: Figure 11 illus- We exploit both the Wordnetgraph and the featuresof trates the experimentalresults when 𝐾 is fixed to 100 and nodesfromWord2vectocreateGCTM-WN-W2V.Wecon- 𝜎 isvaried. Itisobviousthatthedifferentvaluesof𝜎 only ductexperimentstocomparethiscombinationwithGCTM- makeGCTM-WNvaryintermsofLPP.Moreover,theeffect WNandGCTM-W2Vwhichignorenodefeatures.Figure10 of𝜎 isdifferentbetweentheshortandregulartextdatasets. showsthatGCTM-WN-W2VoutperformsbothGCTM-WN GCTM-WNontheshorttextsismoresensitivethanitselfon andGCTM-W2Vwithsignificantmagnitudesin3datasets: theregulartexts. However,𝜎 = 0.1(𝜎2 = 0.01)makesthe Yahoo-title,Agnews, and TMN-title. Itachievescompara- performancesofGCTM-WNonbothAgnewsandAgnews- bleresultswiththeothersintheTMNandNYT-titledatasets. titletheworst. 𝜎 providesawaytoadjusttheimpactofthe ItismerelyinferiortoGCTM-W2V,butissuperiortoGCTM- globalvariables(𝛽and𝑊̃ )fromaminibatchtothenextone. WN on the Agnews-title. In particular, it is obvious that Thesmaller𝜎 is, thestrictertheconstraintofthevariables GCTM-WN-W2Vis betterthanGCTM-WN. Itmeansthat betweentwoconsecutiveminibatchesbecomes. Therefore, exploitinggoodfeaturesofnodesinaknowledgegraphcan a small value of 𝜎 (𝜎 = 0.1) causes GCTM-WN to badly improvetheeffectivenessofGCTM. learnnewknowledgefromthecurrentminibatch. Thesensitivity ofGCTM-WN w.r.t. 𝐾: Figure12illus- tratestheexperimentalresultswhenthenumberoftopics𝐾 is variedand𝜎 is fixedto 1. TheLPPs ofGCTM-WN are Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page11 of 17 <<PAGE_12>> Graph Convolutional Topic Model −8.0 −8.8 −9.6 340 350 360 370 380 PPL −8 −9 −10 400 410 420 430 −9 −10 510 520 530 540 PPL −8 −9 −10 540 550 560 570 580 −7.5 −9.0 −10.5 0 100 200 300 400 500 600 700 Minibatch PPL SVB SVB-PP GCTM-WN PVB Figure 7: Performance of themethods when dealing with concept drift stable on the Agnews dataset when 𝐾 is changed. On the References Agnews-titledataset, only𝐾 = 50makestheperformance [1] Aljundi, R.,Babiloni, F.,Elhoseiny, M.,Rohrbach, M.,Tuytelaars, ofGCTM-WNdecrease. Themoreinformationofwordco- T.,2018. Memoryawaresynapses: Learningwhat(not)toforget, occurrence leads LDA to reduce ambiguous topics, there- in: ComputerVision-ECCV2018-15thEuropeanConference,pp. fore, GCTM-WN on the regular texts is less sensitive than 144–161. itself on the shorttexts. Moreover, when the short dataset [2] Alkhodair,S.A.,Fung,B.C.,Rahman,O.,Hung,P.C.,2018.Improv- inginterpretations oftopicmodelinginmicroblogs. Journalofthe is big, the number of topics 𝐾 should be large enough to AssociationforInformationScienceandTechnology69,528–540. achievebetterperformances. [3] Anh,N.D.,Linh,N.V.,Anh,N.K.,Than,K.,2017.Keepingpriorsin streamingbayesianlearning,in: AdvancesinKnowledgeDiscovery andDataMining: 21stPacific-AsiaConference, PAKDD2017,pp. 5. Conclusion 247–258. Inconclusion,thispaperproposesanovelmodelwhich [4] Bach,T.X.,Anh,N.D.,Van,L.N.,Than,K.,2020. Dynamictrans- formationofpriorknowledgeintobayesianmodelsfordatastreams. integratesgraphconvolutionalnetworksintoa topicmodel arXivpreprintarXiv:2003.06123. toexploitaknowledgegraphwell. Moreover,anovellearn- [5] Bastings,J.,Titov,I.,Aziz,W.,Marcheggiani,D.,Sima’an,K.,2017. ingmethodispresentedtosimultaneouslytrainboththenet- Graphconvolutionalencodersforsyntax-awareneuralmachinetrans- worksandthetopicmodelinstreamingenvironments. Itis lation. EMNLP,1957–1967. worth noting that our method can be extended for a wide [6] Bicalho,P.,Pita,M.,Pedrosa,G.,Lacerda,A.,Pappa,G.L.,2017. A generalframeworktoexpandshorttextfortopicmodeling. Informa- class of probabilistic models. The extensive experiments tionSciences393,66–81. showthatourmethodcanworkwellwhendealingwithshort [7] Blei,D.M.,Ng,A.Y.,Jordan,M.I.,2003. Latentdirichletallocation. texts and concept drift. Our method significantly outper- JournalofMachineLearningResearch3,993–1022. forms the state-of-the-artbaselines in terms of generaliza- [8] Broderick, T.,Boyd,N.,Wibisono,A.,Wilson,A.C.,Jordan,M.I., tionabilityandtopiccoherence. 2013. Streamingvariationalbayes,in: AdvancesinNeuralInforma- tionProcessingSystems,pp.1727–1735. [9] Chen,Z.,Mukherjee,A.,Liu,B.,Hsu,M.,Castellanos,M.,Ghosh, R.,2013.Leveragingmulti-domainpriorknowledgeintopicmodels, in: Twenty-ThirdInternationalJointConferenceonArtificialIntelli- gence. Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page12 of 17 <<PAGE_13>> Graph Convolutional Topic Model −9.0 −9.3 Sport Opinion News Lifestyle Business Culture Class PPL Irishtimes SVB SVB-PP PVB GCTM-WN GCTM-W2V Figure 8: Catastrophic forgetting phenomenon after finishing training each class. LPP is averagely measured on hold-out test sets of thecurrent and previous classes. Higher is better. −8.75 −9.00 −9.25 Sport Opinion News Lifestyle Business Culture Class PPL Irishtimes SVB SVB-PP PVB GCTM-WN GCTM-W2V Figure 9: Catastrophic forgetting phenomenon after finishing training each class. LPP is averagely measured on hold-out test sets of thecurrent and previous classes. Higher is better. [10] Cheng,X.,Yan,X.,Lan,Y.,Guo,J.,2014.Btm:Topicmodelingover 104. shorttexts. IEEETransactionsonKnowledgeandDataEngineering [15] Hoffman,M.D.,Blei,D.M.,Wang,C.,Paisley,J.W.,2013. Stochas- 26,2928–2941. ticvariationalinference. JournalofMachineLearningResearch14, [11] Chérief-Abdellatif, B.E.,Alquier,P.,Khan,M.E.,2019. Ageneral- 1303–1347. izationboundforonlinevariationalinference,in: AsianConference [16] Khan, M.E., Nielsen, D., 2018. Fast yet simple natural-gradient onMachineLearning. descent for variational inference in complex models, in: 2018 In- [12] Duc, A.N.,Linh, N.V.,Anh,N.K.,Nguyen, C.H.,Than,K.,2021. ternationalSymposiumonInformationTheoryandItsApplications Boostingpriorknowledgeinstreamingvariationalbayes.Neurocom- (ISITA),IEEE.pp.31–35. puting424,143–159. [17] Kingma,D.P.,Ba,J.,2014.Adam:Amethodforstochasticoptimiza- [13] Fei-Fei, L., Perona, P., 2005. A bayesian hierarchical model for tion,in: TheInternationalConferenceonLearningRepresentations learningnaturalscenecategories, in: 2005IEEEComputerSociety (ICLR). ConferenceonComputerVisionandPatternRecognition(CVPR’05), [18] Kipf,T.N.,Welling, M.,2017. Semi-supervisedclassificationwith IEEE.pp.524–531. graphconvolutional networks, in: TheInternational Conference on [14] Ha,C.,Tran,V.D.,Van,L.N.,Than,K.,2019. Eliminatingoverfit- LearningRepresentations(ICLR). tingofprobabilistictopicmodelsonshortandnoisytext:Theroleof [19] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- dropout. International JournalofApproximateReasoning112,85– jardins,G.,Rusu,A.A.,Milan,K.,Quan,J.,Ramalho,T.,Grabska- Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page13 of 17 <<PAGE_14>> Graph Convolutional Topic Model −8 −9 −10 0 150 300 PPL NYT-title Agnews-title TMN-title −8.6 −7.50 −7.75 −8.8 40 80 20 40 −8.0 −8.8 40 80 Minibatch PPL Yahoo-title Agnews TMN −7.8 −8.4 −8.4 −8.7 −9.0 40 80 20 40 60 Minibatch Minibatch GCTM-WN GCTM-W2V GCTM-WN-W2V Figure10: PerformanceofGCTM-WN-W2VwithboththeWordnetgraphandnodefeaturesfromWord2vecincomparisonwith GCTM-WNand GCTM-W2V. Higher is better. −8.8 −9.6 −10.4 0 40 80 Minibatch PPL Agnews −8.8 −9.2 −9.6 0 40 80 Minibatch PPL Agnews-title σ2=0.01 σ2=0.1 σ2=1 σ2=10 σ2=100 Figure 11: Sensitivity of GCTM-WN w.r.t 𝜎 Barwinska, A.,etal.,2017. Overcomingcatastrophic forgetting in teroftheAssociationforComputationalLinguistics,pp.530–539. neuralnetworks. ProceedingsoftheNationalAcademyofSciences [21] Le,H.M.,Cong,S.T.,The,Q.P.,VanLinh,N.,Than,K.,2018. Col- 114,3521–3526. laborativetopicmodelforpoissondistributedratings. International [20] Lau, J.H., Newman, D., Baldwin, T., 2014. Machine reading tea JournalofApproximateReasoning95,62–76. leaves: Automatically evaluating topic coherence and topic model [22] Le,V., Phung,C., Vu,C.,Linh,N.V.,Than, K.,2016. Streaming quality,in:Proceedingsofthe14thConferenceoftheEuropeanChap- sentiment-aspectanalysis,in:RIVF,pp.181–186. Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page14 of 17 <<PAGE_15>> Graph Convolutional Topic Model −8.8 −9.6 −10.4 0 30 60 90 Minibatch PPL Agnews −8.5 −9.0 −9.5 0 30 60 90 Minibatch PPL Agnews-title K=50 K=100 K=150 K=200 Figure 12: Sensitivity of GCTM-WN w.r.t thenumberof topics [23] Li,C.,Duan,Y.,Wang,H.,Zhang,Z.,Sun,A.,Ma,Z.,2017.Enhanc- [34] Qiang,J.,Chen,P.,Wang,T.,Wu,X.,2017. Topicmodelingover ingtopicmodelingforshorttextswithauxiliary wordembeddings. shorttextsbyincorporatingwordembeddings,in:Pacific-AsiaCon- ACMTransactionsonInformationSystems(TOIS)36,1–30. ferenceonKnowledgeDiscoveryandDataMining,Springer.pp.363– [24] Li,C.,Wang,H.,Zhang,Z.,Sun,A.,Ma,Z.,2016. Topicmodel- 374. ingforshorttextswithauxiliarywordembeddings,in: Proceedings [35] Quan,X.,Kit,C.,Ge,Y.,Pan,S.J.,2015.Shortandsparsetexttopic ofthe39thInternational ACMSIGIRconference onResearch and modelingviaself-aggregation, in: Twenty-fourthinternational joint DevelopmentinInformationRetrieval,pp.165–174. conferenceonartificialintelligence,Citeseer. [25] Li, D., Zamani, S., Zhang, J., Li, P., 2019. Integration ofknowl- [36] Ritter,H.,Botev,A.,Barber,D.,2018. Onlinestructuredlaplaceap- edgegraphembeddingintotopicmodelingwithhierarchicaldirichlet proximationsforovercomingcatastrophicforgetting,in:Advancesin process,in:Proceedingsofthe2019ConferenceoftheNorthAmer- NeuralInformationProcessingSystems,pp.3738–3748. icanChapteroftheAssociationforComputationalLinguistics: Hu- [37] Rogers,S.,Girolami,M.,Campbell,C.,Breitling,R.,2005. Thela- manLanguageTechnologies,Volume1(LongandShortPapers),pp. tentprocessdecompositionofcdnamicroarraydatasets.IEEE/ACM 940–950. TransactionsonComputationalBiologyandBioinformatics2,143– [26] Lin,C.,He,Y.,Pedrinaci, C.,Domingue, J.,2012. Featurelda: a 156. supervisedtopicmodelforautomaticdetectionofwebapidocumen- [38] Than,K.,Bui,X.,Nguyen-Trong,T.,Truong,K.,Nguyen,S.,Tran, tationsfromtheweb,in:InternationalSemanticWebConference,pp. B.,Van,L.N.,Nguyen-Duc,A.,2019. Howtomakeamachinelearn 328–343. continuously: atutorialofthebayesianapproach,in:ArtificialIntel- [27] Mai,K.,Mai,S.,Nguyen,A.,VanLinh,N.,Than,K.,2016.Enabling ligenceandMachineLearningforMulti-DomainOperationsAppli- hierarchicaldirichletprocessestoworkbetterforshorttextsatlarge cations,InternationalSocietyforOpticsandPhotonics.p.110060I. scale,in:Pacific-AsiaConferenceonKnowledgeDiscoveryandData [39] Theis,L.,Hoffman,M.D.,2015. Atrust-regionmethodforstochas- Mining,Springer.pp.431–442. ticvariationalinferencewithapplicationstostreamingdata,in:Pro- [28] Masegosa, A., Nielsen, T.D., Langseth, H., Ramos-López, D., ceedingsofthe32ndInternationalConferenceonMachineLearning, Salmerón,A.,Madsen,A.L.,2017. Bayesianmodelsofdatastreams ICML2015,pp.2503–2511. withhierarchicalpowerpriors,in:Precup,D.,Teh,Y.W.(Eds.),Pro- [40] Tuan,A.P.,Tran,B.,Nguyen,T.H.,Van,L.N.,Than,K.,2020. Bag ceedingsofthe34thInternationalConferenceonMachineLearning, ofbitermsmodelingforshorttexts.KnowledgeandInformationSys- PMLR.pp.2334–2343. tems62,4055–4090. [29] McInerney,J.,Ranganath,R.,Blei,D.M.,2015.Thepopulationpos- [41] VanLinh,N.,Anh,N.K.,Than,K.,Dang,C.N.,2017. Aneffective teriorandbayesianmodelingonstreams,in:AdvancesinNeuralIn- andinterpretablemethodfordocumentclassification.Knowledgeand formationProcessingSystems28,pp.1153–1161. InformationSystems50,763–793. [30] Mehrotra,R.,Sanner,S.,Buntine,W.,Xie,L.,2013. Improvinglda [42] Wang,X.,Zhang,Y.,Wang,X.,Chen,J.,2019. Aknowledgegraph topicmodelsformicroblogsviatweetpoolingandautomaticlabeling, enhancedtopicmodelingapproachforherbrecommendation,in:In- in:Proceedingsofthe36thinternationalACMSIGIRconferenceon ternationalConferenceonDatabaseSystemsforAdvancedApplica- Researchanddevelopmentininformationretrieval,pp.889–892. tions,Springer.pp.709–724. [31] Mermillod,M.,Bugaiska,A.,Bonin,P.,2013.Thestability-plasticity [43] Xu,Y.,Xu,H.,Zhu,L.,Hao,H.,Deng,J.,Sun,X.,Bai,X.,2018. dilemma:Investigatingthecontinuumfromcatastrophicforgettingto Topicdiscoveryforstreamingshorttextswithctm,in:2018Interna- age-limitedlearningeffects. Frontiersinpsychology4,504. tionalJointConferenceonNeuralNetworks(IJCNN),IEEE.pp.1–7. [32] Nguyen,C.V.,Li,Y.,Bui,T.D.,Turner,R.E.,2018. Variationalcon- [44] Yang,Y.,Wang,F.,Zhang,J.,Xu,J.,Philip,S.Y.,2018.Atopicmodel tinuallearning,in:TheInternationalConferenceonLearningRepre- forco-occurringnormaldocumentsandshorttexts.WorldWideWeb sentations(ICLR). 21,487–513. [33] Nguyen, V.S., Nguyen, D.T., Van, L.N., Than, K., 2019. Infinite [45] Yao,L.,Mao,C.,Luo,Y.,2019. Graphconvolutionalnetworksfor dropout for training bayesian models from data streams, in: 2019 textclassification,in:ProceedingsoftheAAAIConferenceonArti- IEEEInternational Conference onBigData (BigData), IEEE.pp. ficialIntelligence,pp.7370–7377. 125–134. [46] Yao,L.,Zhang,Y.,Wei,B.,Jin,Z.,Zhang,R.,Zhang,Y.,Chen,Q., Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page15 of 17 <<PAGE_16>> Graph Convolutional Topic Model 2017. Incorporatingknowledgegraphembeddingsintotopicmodel- 2 𝑡 𝑖−1 2log𝐷−log𝐷(𝑤𝑘 𝑖)−log𝐷(𝑤𝑘 𝑗) ing,in:Thirty-FirstAAAIConferenceonArtificialIntelligence. = −1+ 𝑡(𝑡−1) log𝐷−log(𝐷(𝑤𝑘,𝑤𝑘)+10−2) [47] Zenke,F.,Poole,B.,Ganguli,S.,2017. Continuallearningthrough 𝑖=2𝑗=1 𝑖 𝑗 ∑∑ synapticintelligence,in: Proceedingsofthe34thInternationalCon- ferenceonMachineLearning,ICML2017,pp.3987–3995. where 𝐷 is the total number of documents, 𝐷(𝑤𝑘) is the 𝑖 [48] Zhao,H.,Du,L.,Buntine,W.,2017. Awordembeddingsinformed numberofdocumentsthatcontain𝑤𝑘,𝐷(𝑤𝑘,𝑤𝑘)isthenum- focusedtopicmodel,in:AsianConferenceonMachineLearning,pp. 𝑖 𝑖 𝑗 423–438. ber of documents that contain both 𝑤𝑘 and 𝑤𝑘). Finally, 𝑖 𝑗 [49] Zhu,Q.,Feng,Z.,Li,X.,2018. Graphbtm:Graphenhancedautoen- NPMIisaveragedonall𝐾 topics. codedvariationalinferenceforbitermtopicmodel,in: Proceedings ofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage Processing,pp.4663–4672. C. The effectivesettings ofthemethods In this section, we list the best hyperparameter for the methodsfromgridsearch. A. Log PredictiveProbability Wecalculatelogpredictiveprobabilityonatestsetasin C.1. Experimentsondatasetsintermsoffixed [15]. Let 𝐷 and 𝐷 be training and test sets respec- batchsize,timestamp,andnoisydata 𝑡𝑟𝑎𝑖𝑛 𝑡𝑒𝑠𝑡 tively. The modelparameter 𝛽 of LDA is learnton 𝐷 . PVB: 𝑡𝑟𝑎𝑖𝑛 Eachdocumentinthetestset𝐷 isdividedrandomlyinto Yahoo-title: 𝜅 =0.9,𝑆 =106 𝑡𝑒𝑠𝑡 twodisjointparts𝐰 𝐨𝐛𝐬 and𝐰 𝐡𝐨 witharatioof80:20. LPP NYtimes-title: 𝜅 =0.9,𝑆 =105 examinehow a modelpredictsthe words𝐰 𝐡𝐨 whengiving Agnews: 𝜅 =0.9,𝑆 =104 thewords𝐰 𝐨𝐛𝐬 foreverydocumentinthetestset. Thepre- Agnews-title: 𝜅 =0.9,𝑆 =106 dictiveprobabilityiscalculatedasbelow: TMN:𝜅 =0.9,𝑆 =103 TMN-title: 𝜅 =0.9,𝑆 =103 𝑝(𝐰 𝐡𝐨 ∣𝐰 𝐨𝐛𝐬,𝛽)= 𝑝(𝑤∣𝐰 𝐨𝐛𝐬,𝛽) Irishtimes(withtimestamp): 𝜅 =0.5,𝑆 =105 𝑤∈𝐰 𝐡𝐨 Irishtimes(withfixedbatchsize): 𝜅 =0.9,𝑆 =105 ∏ ≈ 𝑝(𝑤∣𝜃𝑜𝑏𝑠,𝛽) Twitter: 𝜅 =0.9,𝑆 =106 𝑤∈𝐰 𝐡𝐨 SVB-PP: ∏ Yahoo-title: 𝜌=0.99 𝐾 = 𝑝(𝑤∣𝑧=𝑘,𝛽)𝑝(𝑧=𝑘∣𝜃𝑜𝑏𝑠) NYtimes-title: 𝜌=0.99 𝑤∈𝐰 𝐡𝐨𝑘=1 Agnews: 𝜌=0.99 ∏ ∑ Agnews-title: 𝜌=0.99 𝐾 = 𝜃𝑜𝑏𝑠𝛽 TMN:𝜌=0.99 𝑤∈𝐰 𝐡𝐨𝑘=1 𝑘 𝑘𝑤 TMN-title: 𝜌=0.99 ∏ ∑ Irishtimes(withtimestamp): 𝜌=0.5 where 𝜃𝑜𝑏𝑠 is inferred from 𝐰 𝐨𝐛𝐬 and the learnt model 𝛽. Irishtimes(withfixedbatchsize): 𝜌=0.9 ThenLPPofeachdocument𝑑iscomputed: Twitter: 𝜌=0.99 GCTM-WN: log𝑝(𝐰 𝐡𝐨 ∣𝐰 𝐨𝐛𝐬,𝛽) Yahoo-title: 𝜎 =0.01 𝐿𝑃𝑃 = (8) 𝑑 𝐰 𝐡𝐨 NYtimes-title: 𝜎 =100.0 Agnews: 𝜎 =1.0 where 𝐰 𝐡𝐨 is the leng |th o |f 𝑑 in 𝐰 𝐡𝐨). Then, the LPP of Agnews-title: 𝜎 =1.0 𝐷 is averagedon all documentsin the test set. We also 𝑡𝑒𝑠𝑡 TMN:𝜎 =1.0 run5timeswith5randomsplitstoaverage. | | TMN-title: 𝜎 =1.0 Irishtimes(withfixedbatchsize): 𝜎 =0.01 B. NormalizedPointwiseMutual Information Irishtimes(withfixedbatchsize): 𝜎 =0.01 Twitter: 𝜎 =1 Thismetricwascomputedasin[20]. AftertrainingLDA, GCTM-W2V: we pick top𝑡 = 20wordswith the highestprobabilitiesin Yahoo-title: 𝜎 =100.0 topicdistribution(𝐰𝐤 = {𝑤𝑘,𝑤𝑘,...,𝑤𝑘})foreachtopic𝑘. 1 2 𝑡 NYtimes-title: 𝜎 =1.0 WecalculateNPMIofatopic𝑘asfollows: Agnews: 𝜎 =1.0 𝑝(𝑤𝑘,𝑤𝑘) Agnews-title: 𝜎 =1.0 NPMI(𝑘,𝐰𝐤 )= 2 𝑡 𝑖−1 log 𝑝(𝑤𝑘 𝑖𝑖 )𝑝(𝑤𝑗 𝑘 𝑗) T TM MN N: -t𝜎 itl= e:1 𝜎0 =0.0 100.0 𝑡(𝑡−1) 𝑖=2𝑗=1 −log𝑝(𝑤𝑘 𝑖,𝑤𝑘 𝑗) Irishtimes(withfixedbatchsize): 𝜎 =0.01 ∑∑ 𝐷(𝑤𝑘,𝑤𝑘)+10−2 𝐷(𝑤𝑘)𝐷(𝑤𝑘) Irishtimes(withfixedbatchsize): 𝜎 =0.01 𝑡 𝑖−1 log 𝑖 𝑗 −log 𝑖 𝑗 2 𝐷 𝐷2 Twitter: 𝜎 =1 ≈ 𝑡(𝑡−1) 𝐷(𝑤𝑘,𝑤𝑘)+10−2 𝑖=2𝑗=1 −log 𝑖 𝑗 ∑∑ 𝐷 Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page16 of 17 <<PAGE_17>> Graph Convolutional Topic Model C.2. Experimentsondatasetsintermsofconcept driftandcatastrophicforgetting PVB:𝜅 =0.9,𝑆 =106 SVB-PP:𝜌=0.9 GCTM-WN:𝜎 =100 GCTM-W2V:𝜎 =0.01 Linh Ngo Van et al.: PreprintsubmittedtoElsevier Page17 of 17",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 146
  },
  "1fdfd43a-3340-49d7-ac89-33184174d744": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "b0ac0e5c-53cf-444c-b6e4-8dc9a2560b9a": {
    "filename": "Lec27.pdf",
    "text": "<<PAGE_1>> Introduction to Large Language Models (LLMs) Prof. Tanmoy Chakraborty, Prof. Soumen Chakraborti Department of Computer Science & Engineering Indian Institute of Technology, Delhi Lecture 27 Knowledge and Retrieval: Knowledge Graph Completion and Evaluation Hello, today we will continue with knowledge graph completion and its evaluation. As we mentioned recently, knowledge graphs are very incomplete and we need to complete them either as hard decisions or as soft probabilistic judgments. In this setting, an incomplete knowledge graph is provided and a modeling or machine learning system first fits embedding representations of the entities and relations in the knowledge graph. Within this paradigm, there are two possibilities. One is that these representations are based on the topology or the shape of the knowledge graph alone. <<PAGE_2>> And then there is the more realistic and beneficial setting where we supplement the knowledge of the graph topology with the text aliases of the entities and relations that we briefly viewed in the last part. After these embedding representations are fitted, these will be applied back to the knowledge graph to infer missing fact triples in the knowledge graph with a certain level of confidence. So in short, knowledge graph completion consists of proposing a function f. The inputs to this function are the subject, the relation, and the object. While training, we would like to provide all the three inputs, but while testing one of these inputs may be withheld or unknown. The output of F is the belief in the fact, which connects the subject relation and object. Now, most knowledge graph representation and completion algorithms will represent each entity, which may be a subject or an object, and each relation as some kind of continuous geometric artifacts. Such an artifact may be a point in space, some kind of high dimensional space where the KG artifacts are living, or it could be a vector or a displacement. It can be a projection or a hyperplane onto which these points are being projected. It can be a rotation. <<PAGE_3>> It can be all kinds of real or complex artifacts. Without getting into the detail of that yet, the purpose of the scoring function f is to take these representations as input and output some measure of belief in subject relation object as a triple. For example, if I say Barack Obama, president of US, as the three inputs to f, f should output a high level of belief. If instead I say something like Barack Obama, discovered theory of relativity, then the output of F should be very low. We shouldn't believe in that fact based on what else we know about these entities and relationships. During training, we will provide the identity of these subjects, relations, and objects, and we will tell the algorithm whether that fact is true or false. By the way, when I say fact in this lecture, it basically means some kind of a claim. So a claim can be either true or false. During training, I will give triples like this and their ideal belief value, be it one or be it zero. and the algorithm will fit the representations like those points or vectors or displacements or rotations. After that, once the model is fitted, we can then infer unknown triples, which would be similar to predicting links in a social network. Some of you may be aware that in social networks, we might have a person, P1, who is already friends with some people, say P1, P2, P3, and P4, and now there may be another person, P20, and based on their behavior patterns and linkage patterns, we might ask, does the link P1 to P20 exist or should it exist? It's a bit similar to that, except that now our links are very richly endowed with features like links represent relation types and even the entities have rich features. But that's true <<PAGE_4>> even in social network analysis. Now, in principle, Neural networks are universal function approximators. So if we plug in a sufficiently strong or high capacity neural network into the box F, it should be able to learn a function which outputs the correct belief in every proposed fact or claim. In practice, though, the design of F is something of an art. And hundreds of papers through the last five or six years have invented and proposed mechanisms for the design of F and to fit it to observed data. Before we proceed further, let us set up some notation. We will represent entities as E in general. And when an entity takes the role of a subject, it will be written as S. When it has the role of an object, it will be written as O. Now, why do we distinguish between subject and object in relations? That's because not all relations are symmetric. If the relationship is sibling of, then obviously what is subject and what is object doesn't matter. But if the relation is asymmetric, like parent of or CEO of a company, then we cannot switch subject and object. And so E will have to be specialized into either S or O. When the learning system has fitted a representation for the entity E, we will write it as vector E or we'll write it as a boldface E. like this as a vector with an arrow on top or a boldface E. So subjects and objects will have corresponding embeddings bold S or bold O. And similarly, relations will have representations which are written as boldface R. Now, depending on the exact nature of the model, the exact shapes of S, O, and R might change, in some cases, there may be vectors. In other cases, there may be matrices, and so on and so forth. Now, this function f will apply to s, r and o given as inputs and output a real number. Generally speaking, this real number may be positive or maybe even not positive. It's a raw confidence score. It's not already a probability. <<PAGE_5>> To turn it into a probability, we may have to apply certain functions to it. If a particular SRO triple is known to be true in our knowledge graph, we'll call it a positive triple or a positive fact. Whereas if S prime, R prime, O prime is not in the knowledge graph, there are a couple of ways to think about it. Remember that our knowledge graph is incomplete. So if S prime, R prime, O prime does not appear in the knowledge graph, that doesn't necessarily prove that it's false. But there are ways to sample S prime, R prime, O prime, which makes it very unlikely that S prime, R prime, O prime is true. We shall visit one or two ways to do that in the upcoming slides. If S prime, R prime, O prime is known to be false, we'll call it a negative triple or a negative fact. Now, earlier I was saying that this F return value is not really a probability. If we wanted to turn it into a probability, we need some kind of a probability space. One example of this is to fix two of the three items in the triple, like the subject and the relation, and then say what's the probability that the object is O. For example, I might say probability of Hawaii given Barack Obama and born in, and that probability should be high. Whereas if I say probability of Tokyo, given Barack Obama and birthplace, that probability should be low. And it is defined in a straightforward softmax-like fashion. We take the raw F scores, which could be positive or negative reals, and we exponentiate them. <<PAGE_6>> In the numerator, we will keep e to the power fsro. And in the denominator, we will add that value over all possible values of o, which we call o prime. This normalizes the numerator and makes it add up to 1 over all candidates o. So think of a little edge r between s and an unknown o. We are trying to find out which particular o it might be. Similarly and conversely, given the relation and the object, we might want to find out what the subject is. Now, already you see the effect of asymmetry, and that will not be tackled for the first couple of models, but we'll eventually get to it. In the form of birthplace. So if I say, what's the probability of Barack Obama given birthplace and Hawaii? Because so many people were born in Hawaii, the probability of one Barack Obama will be quite small. And so the interesting thing to observe here is that Barack Obama is now competing with everyone born in Hawaii for the fixed chunk of probability one. And these are interesting implications for inference and this use in language models. Somehow in the literature, fixing subject and object and then trying to sample or find the probability of R has been relatively rare. I don't exactly know why. Now, if SRO is in the knowledge graph, we want the probability of O given SR to be large, and we want the probability of S given RO to be also large. How do we turn the probability to a loss function? That's fairly standard now in machine learning and deep learning. Given a knowledge graph with known triples i would like over all the triples in the knowledge graph the log probability of s given r o we add all of these up and then we also <<PAGE_7>> add the probability of o given s r take the log of that and then take the sum over that we want this entire sum to be maximized To turn that into a loss, we can negate that. So it's minus log probability of S given R O minus log probability O given S R. And then we sum over all triples in the kg and we want to minimize this negative log. So this only indirectly encourages small f of S prime R prime O prime for the negative triples. Remember that this probability has denominators. And to make the whole thing large, I need to make the denominator for all the wrong triples small. So that's how training would work. Observe also that the probabilities in the denominator involve summing over all elements in the knowledge graph. And so that could be quite expensive. To save computational cost, we might want to sample the denominator. So we take a positive fact SRO, and then we replace S with a randomly sampled entity S prime. So in our running example, we say Barack Obama born in Hawaii. And this is my positive triple. Now I cancel out Barack and replace by random samples like Gandhi. So this is assumed to be positive, to be negative. Why is that? Because there are so many people in the world that by random sampling, we would get another person born in Hawaii would be fairly small. This is called the local closed world assumption. It's not really valid given that knowledge graph is very incomplete. If we are sampling for people born in Hawaii versus elsewhere, this might succeed, but for classes which are much broader, we have to be really careful in not mistakenly sampling positive things and thinking they're negative. Even so, this helps replace the full sum in the denominator with more efficient to compute sampled estimates. We could do uniform sampling, but that would be too expensive. So we could sample K out of the E entities uniformly at random. Suppose there are total of E entities and we take K negative samples, then our expectation of that denominator has to be adjusted for that K, right? So we have to invert that by multiplying it by, E over K or K over E, depending on which way the sampling is going. And that restores the denominator to the expected sum over all entities in the knowledge graph. Now here, one problem is that if I'm doing <<PAGE_8>> the sampling of k things or entities uniformly at random, I might actually make the mistake of leaving out the gold or the true O. So we may have to introduce it back by force into this denominator sum, and this may further bias my estimates. So those are certain standard problems that we live with in defining these efficient sampled estimates. The other approach is to not bother with probabilities at all. So instead of trying to shoehorn an expression for a probability using a softmax, we might say that, look, the positive fact has a score F of SRO and any negative fact has a score of F of S prime, R prime, O prime. And the understanding is that at least one of them is wrong. and therefore the whole fact is wrong. So now we might say we want the good score, which is the score of the correct triple, to beat the bad score, which is the score of a triple which is wrong in at least one place, by an additive margin. So this is similar to support vector machines for those of you that are familiar with SVMs. This can be turned into what's called a hinge or a relu loss. So we say max of zero and then margin plus bad score minus good score. If our system, if our function f and the learned weights are already perfect or close to perfect, then the good score will exceed the bad score, not by itself, but even when assisted by the margin. And therefore, this quantity over here will go negative. Because it goes negative, my overall loss taken with a maximum of zero will turn out to be zero. And <<PAGE_9>> that's when we can stop the training because the system is already trained. Otherwise, if this loss turns out to be positive, we can differentiate this loss with regard to the shape of the function f and the embeddings of the entities and relations that participate in this expression. And that starts off back propagation into the model weights. The problem again with this formulation is very similar to the summing over a large number of entities. Because there are E entities and R relations, the number of possible facts in the universe are E squared R, each of which could be true or false. Only a small fraction is positive. So that means the number of constraints between good and bad is the number of good times the number of bad. So that's an astronomical and infeasible number. Now, generally the strategy that's followed is that I'm not going to take SRO and then perturb all three of them in all possible ways. I'm only going to perturb one thing at a time, and that limits the search space between the good bad pairs. So we would take SRO and only knock out the object, or we'll take SRO and only knock out the subject. So this leads to the formation of a batch for training a stochastic gradient based optimizer. For each positive fact SRO in the batch, we would sample K presumed negative facts by perturbing either the subject or the object, or maybe both. And then we would accumulate the losses over these K samples. in each we'll apply the hinge loss as mentioned before where we do margin plus bad minus good. So this is the good that is the bad and we want the good to beat the bad and this whole sum to come down to zero. So this is how many knowledge graph modeling and completion algorithms are trained. So far, we have assumed that a large score F makes the triple SRO more likely, and a small score makes it less likely. Some models may work with the opposite polarity. Some models may use a notion of distance, as we shall see. In that case, a large distance F will make SRO less likely, and a small distance will make it more likely. Usually, the loss and the optimization can be adjusted without much trouble to this flipped polarity. How are knowledge graph completion algorithms tested or evaluated? For this, people have defined a whole bunch of knowledge graph completion tasks or challenges. The <<PAGE_10>> knowledge graph is first sampled into three folds, which are usually disjoint, the training fold, the dev fold, and the test fold. The default is also called the validation for it sometimes. Given a test query, eventually when the system is deployed, which may be of the form SR question mark or question mark RO, the trained system must provide a ranked list for the blanks. For example, if the test query is SR question mark, the trained system should output candidates O1, O2, O3 and so on. And in the judgment of the system, O1 should be the best or most likely answer. On this list, we will define standard ranking measures like mean reciprocal rank or the number of hits at K. Let's take the second one first because that's easier to explain. So we go down to K elements and among the K elements, we see whether O1 is correct, O2 is correct and so on. And we count up the number of correct options and we divide it by K. So that's the hits at K. It's like you go to Google and do a search and the top 10 or top K results are given to you. How many of them make you happy? Now, this of course makes sense if, in general, if there are multiple objects for which the triple is true. And this can happen for relations that are not one is to one. A person can be only born in one place. So that's many to one, actually. Many people are born in a particular place. But there could be also the opposite, which is Barack Obama attended which academic institutions? And he might have attended two schools and three colleges. So it's one to many, one Barack Obama attending multiple educational institutions. So every time I get a hit, I will increment the counter and then eventually I'll divide it by K. Now mean reciprocal rank is a bit different. So think of going down this list again until I hit the first relevant entity, which is in this example at rank two. If I do that, my reciprocal rank will be half because two was the rank at which I hit my first success. So MRR is good for situations where there is exactly one correct answer, whereas HITSAT-K is useful if there are multiple correct answers. Now, think of what happens if I hit my correct object at rank 1. <<PAGE_11>> In that case, reciprocal rank will be equal to 1 over 1, which is 1, which is the maximum possible value. As the rank of the first correct answer goes down, reciprocal rank will dwindle to 0. Just like...hits at k ranges between 0 and 1, reciprocal rank will thus also range between 0 and 1. Now observe that the penalty for dropping something from rank 1 to rank 2 is to go down from 1 to half. Whereas the penalty for dropping something from 2 to infinity is to go down from half to 0. So that's sometimes found a bit extreme. For that reason, mean reciprocal rank is sometimes called the mean reciprocal rank. It's very mean. It penalizes you as much for dropping something from 1 to 2 as dropping from 2 to infinity, where you wouldn't even see it. There are other measures of knowledge graph completion efficacy, which we won't have time to get into, but you can read about it from the references. How about datasets? There are a few very prominent datasets that are used in this area of work. There is WordNet 18, which comes from the semantic or lexical network called WordNet. WordNet is a network where the nodes are words or synsets. So words which are synonyms of each other are collected into the same node and made into a sense set or synset, synonym set. And the edges represent Or toward relation types like hypernomy, a camel is a mammal is a living being, or synonymy that envy and jealousy are sometimes considered to be synonyms. The training and dev and test folds have sizes 141000, 5000 and 5000. And there are 18 relation types. <<PAGE_12>> Another popular data set in use is FB15K, which has 15,000 freebase entities connected with 1,345 relation types, and the training dev and test folds have about 480,000 and 50 to 60,000 fact triples in them. There have been refinements on these datasets. These datasets have been found to be somewhat simple in the sense that certain algorithms can cheat. And to prevent them from doing that, modified enhanced datasets, WN18RR and FB15K-237 have been evolved over time. There are other datasets like YAGO, yet another great ontology, and several other tasks. One important last note about evaluation is the element of filtering. So as I said, your query might be subject relation question mark, and now the ranking system has to provide a response list, O1, O2, et cetera, and so on. Suppose O6 and O8 are the designated gold correct answers. In that case, we might say that MRR is one over six, because that's the rank at which I found the first correct response. And mean average precision is the third measure might as well goal. Mean average precision says to get my first relevant response, I have to go to rank six. When I went to rank six, I found only one relevant response up to that time. For the moment, ignore the green color on O2. If I walk down two more steps to rank 8, I see 2, namely 6 and 8, correct responses. I will now average them up. 1 sixth plus 2 eighth divided by half So that's called mean average precision. And the mean reciprocal rank is 1 in 6, as we already saw. But now, suppose that in the training set, <<PAGE_13>> O2 was already known to be a correct answer. then you might say, look, it's unfair to penalize position 6 and 8 as 6 and 8 because training already revealed to us that 2 was relevant. So maybe we should knock O2 out of the list and regard the MRR as 1 out of 5 and the MAP as the average of 1 5th and 2 7th. This is a simple modification, but it's an important evaluation convention that is not followed sufficiently uniformly across papers. So when you compare the numbers reported in papers, you have to be careful about whether they did a filtered evaluation or an unfiltered evaluation. Next, we will start looking at what kind of functions can go into that box marked F and what form the input subject relation and object vectors or embeddings can take.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 53
  },
  "5e23eb45-213a-4d89-9663-144a120838b8": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "59c71605-b710-4c89-b4f2-4170168bfb70": {
    "filename": "Lecture 12.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 12 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Unsupervised Learning <<PAGE_3>> Introduction • Regression and Classification problems belong to a class of problems called Supervised Learning Problems. • In both regression and classification, the historical or training data provided to build a predictive model includes data on the variable to be predicted, i.e., the outcome or target variables. • When the data is provided on the variable to be predicted then the problem is a Supervised Learning problem. • Unsupervised learning problems are those where the dataset does not have any target or outcome variables. <<PAGE_4>> If there is no data on what needs to be predicted, then what can such algorithms do? • Suppose a customer has selected a few products to purchase. • We would like to provide suggestions about other products that he should or could be buying. • The data we have is historical data of what customers have bought. • In the historical data, if we find repeat patterns of milk being bought with bread, and • If customer who is currently shopping has bought milk, then our algorithm can suggest buying bread. • We can not say that if milk is the value of a feature , then bread is the value to be predicted because vice versa, i.e., if the customer has bought bread, he could buy milk. <<PAGE_5>> Contd… • So, the role of feature and predictor quickly gets changed. • Rather, in this case, we have patterns in our historical data, and we define rules saying that if certain patterns occur very often, then they can be used for making certain recommendations. • This problem formulation goes by the name of Associate Rule Mining and is an Unsupervised Learning problem. • We do not discuss associate rule mining in this course • Rules in Associate rule mining are automatically inferred from the data (i.e machine learned) and are not created manually by human beings. <<PAGE_6>> Contd.. • In unsupervised learning , the models still perform predictions. • But there is no prior data on the outcomes. • We need to define the outcome or prediction we desire and accordingly come up with an algorithm for doing so. <<PAGE_7>> Clustering • Clustering is an unsupervised machine learning technique that groups similar data points together into clusters based on their characteristics, without using any labeled data. • The objective is to ensure that data points within the same cluster are more similar to each other than to those in different clusters, enabling the discovery of natural groupings and hidden patterns in complex datasets. <<PAGE_8>> • Goal: Discover the natural grouping or structure in unlabeled data without predefined categories. • How: Data points are assigned to clusters based on similarity or distance measures. • Similarity Measures: Can include Euclidean distance, cosine similarity or other metrics depending on data type and clustering method. • Output: Each group is assigned a cluster ID, representing shared characteristics within the cluster. <<PAGE_9>> • For example, if we have customer purchase data, clustering can group customers with similar shopping habits. • These clusters can then be used for targeted marketing, personalized recommendations or customer segmentation. <<PAGE_10>> Types of Clustering • Hard Clustering: In hard clustering, each data point strictly belongs to exactly one cluster, no overlap is allowed. This approach assigns a clear membership, making it easier to interpret and use for definitive segmentation tasks. • Example: If clustering customer data into 2 segments, each customer belongs fully to either Cluster 1 or Cluster 2 without partial memberships. • Use cases: Market segmentation, customer grouping, document clustering. • Limitations: Cannot represent ambiguity or overlap between groups; boundaries are crisp. <<PAGE_11>> • Soft Clustering: Soft clustering assigns each data point a probability or degree of membership to multiple clusters simultaneously, allowing data points to partially belong to several groups. • Example: A data point may have a 70% membership in Cluster 1 and 30% in Cluster 2, reflecting uncertainty or overlap in group characteristics. • Use cases: Situations with overlapping class boundaries, fuzzy categories like customer personas or medical diagnosis. • Benefits: Captures ambiguity in data, models gradual transitions between clusters. <<PAGE_12>> Types of Clustering Methods • Clustering methods can be classified on the basis of how they for clusters, 1. Centroid-based Clustering (Partitioning Methods) • Centroid-based clustering organizes data points around central prototypes called centroids, where each cluster is represented by the mean (or medoid) of its members. • The number of clusters is specified in advance and the algorithm allocates points to the nearest centroid, making this technique efficient for spherical and similarly sized clusters but sensitive to outliers and initialization. <<PAGE_13>> Algorithms • K-means: Iteratively assigns points to nearest centroid and recalculates centroids to minimize intra-cluster variance. • K-medoids : Similar to K-means but uses actual data points (medoids) as centers, robust to outliers. Pros: • Fast and scalable for large datasets. • Simple to implement and interpret. Cons: • Requires pre-knowledge of k. • Sensitive to initialization and outliers. • Not suitable for non-spherical clusters. <<PAGE_14>> Density-based Clustering (Model-based Methods) • Density-based clustering defines clusters as contiguous regions of high data density separated by areas of lower density. • This approach can identify clusters of arbitrary shapes, handles noise well and does not require predefining the number of clusters, though its effectiveness depends on chosen density parameters. Algorithms • DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points with sufficient neighbors; labels sparse points as noise. • OPTICS (Ordering Points To Identify Clustering Structure): Extends DBSCAN to handle varying densities. <<PAGE_15>> Pros/Cons Pros: • Handles clusters of varying shapes and sizes. • Does not require cluster count upfront. • Effective in noisy datasets. Cons: • Difficult to choose parameters like epsilon and min points. • Less effective for varying density clusters (except OPTICS). <<PAGE_16>> Connectivity-based Clustering (Hierarchical Clustering) • Connectivity-based (or hierarchical) clustering builds nested groupings of data by evaluating how data points are connected to their neighbors. • It creates a dendrogram—a tree-like structure—that reflects relationships at various granularity levels and does not require specifying cluster numbers in advance, but can be computationally intensive. Approaches: • Agglomerative (Bottom-up): Start with each point as a cluster; iteratively merge closest clusters. • Divisive (Top-down): Start with one cluster; iteratively split into smaller clusters. <<PAGE_17>> Pros • Provides a full hierarchy, easy to visualize. • No need to specify number of clusters upfront. Cons • Computationally intensive for large datasets. • Merging/splitting decisions are irreversible. <<PAGE_18>> Distribution-based Clustering • Distribution-based clustering assumes data is generated from a mixture of probability distributions, such as Gaussian distributions and assigns points to clusters based on statistical likelihood. • This method supports clusters with flexible shapes and overlaps, but usually requires specifying the number of distributions. <<PAGE_19>> Algorithm: • Gaussian Mixture Model (GMM): Fits data as a weighted mixture of Gaussian distributions; assigns data points based on likelihood. Pros: • Flexible cluster shapes. • Provides probabilistic memberships. • Suitable for overlapping clusters. Cons: • Requires specifying number of components. • Computationally more expensive. • Sensitive to initialization. <<PAGE_20>> Fuzzy Clustering • Fuzzy clustering extends traditional methods by allowing each data point to belong to multiple clusters with varying degrees of membership. • This approach captures ambiguity and soft boundaries in data and is particularly useful when the clusters overlap or boundaries are not clear-cut. <<PAGE_21>> Algorithm: • Fuzzy C-Means: Similar to K-means but with fuzzy memberships updated iteratively. Pros: • Models data ambiguity explicitly. • Useful for complex or imprecise data. Cons: • Choosing fuzziness parameter can be tricky. • Computational overhead compared to hard clustering. <<PAGE_22>> Use Cases • Customer Segmentation: Grouping customers based on behavior or demographics for targeted marketing and personalized services. • Anomaly Detection: Identifying outliers or fraudulent activities in finance, network security and sensor data. • Image Segmentation: Dividing images into meaningful parts for object detection, medical diagnostics or computer vision tasks. • Recommendation Systems: Clustering user preferences to recommend movies, products or content tailored to different groups. • Market Basket Analysis: Discovering products frequently bought together to optimize store layouts and promotions. <<PAGE_23>> K means Clustering • K-Means Clustering is an unsupervised machine learning algorithm that helps group data points into clusters based on their inherent similarity. • Unlike supervised learning, where we train models using labeled data, K-Means is used when we have data that is not labeled and the goal is to uncover hidden patterns or structures. • For example, an online store can use K-Means to segment customers into groups like \"Budget Shoppers,\" \"Frequent Buyers,\" and \"Big Spenders\" based on their purchase history <<PAGE_24>> Working of K-Means Clustering • Suppose we are given a data set of items with certain features and values for these features like a vector. • The task is to categorize those items into groups. • To achieve this, we will use the K-means algorithm. 𝑘 • \" \" represents the number of groups or clusters we want to classify our items into. <<PAGE_25>> <<PAGE_26>> <<PAGE_27>> <<PAGE_28>> <<PAGE_29>> K-Means Clustering Algorithm • The algorithm will categorize the items into \"𝑘 \" groups or clusters of similarity. • To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows: 1. Initialization: We begin by randomly selecting k cluster centroids. 2. Assignment Step: Each data point is assigned to the nearest centroid, forming clusters. 3. Update Step: After the assignment, we recalculate the centroid of each cluster by averaging the points within it. 4. Repeat: This process repeats until the centroids no longer change or the maximum number of iterations is reached. • The goal is to partition the dataset into 𝑘 clusters such that data points within each cluster are more similar to each other than to those in other clusters. <<PAGE_30>> Euclidean distance <<PAGE_31>> Elbow Method for optimal value of k in KMeans • Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means. • However, deciding the ideal k is not straightforward. • The Elbow Method helps by plotting the Within-Cluster Sum of Squares (WCSS) against increasing k values and looking for a point where the improvement slows down, this point is called the \"elbow.\" <<PAGE_32>> Working of Elbow Point The Elbow Method works in the following steps: 1) We iterate over a range of k values, typically from 1 to n (where n is a hyperparameter you choose). 2) For each k, we calculate a distance measure called WCSS (Within-Cluster Sum of Squares). • This tells us how spread out the data points are within each cluster. • WCSS measures how well the data points are clustered around their respective centroids. 3) We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS. 4) We plot a graph with k on the X-axis and WCSS on the Y-axis. 5) As we increase k, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph. <<PAGE_33>> Within-Cluster Sum of Squares(WCSS) 𝑘 𝑘 • Where (𝑥 , 𝑦 ) is the centroid of the points in the cluster 𝐶 , and | 𝐶 | is 𝑘 𝑘 the number of points in the cluster. • WCSS is essentially the sum of the square of the distances between the points in the cluster and its centroid. • It should be obvious that if the points are tightly clustered ,then WCSS will be a small number and vice versa. <<PAGE_34>> • Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability. • After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting. • The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. • This \"elbow\" point suggests the optimal number of clusters. <<PAGE_35>> Inertia • Inertia simply as the sum of the WCSs values for a group of clusters. • If there are K clusters ,then inertia of the clusters is given by, • Given K ,the number of clusters and n points , there are many ways to cluster the n points into K clusters. • The K-means algorithm creates K-Clusters such that the sum of the WCSS values for the K-clusters is minimal, i.e., it tries to create K clusters such that the inertia of the K clusters created is less than the inertia of clustering of the n points into some other K clusters. • It should be mentioned at the onset that K-means is based on heuristics and does not guarantee a clustering with minimum inertia. • It merely finds a clustering with low inertia • Lower Inertia suggests better clustering <<PAGE_36>> Why Use K-Means Clustering? K-Means is popular in a wide variety of applications due to its simplicity, efficiency and effectiveness. Data Segmentation: One of the most common uses of K-Means is segmenting data into distinct groups. For example, businesses use K-Means to group customers based on behavior, such as purchasing patterns or website interaction. Image Compression: K-Means can be used to reduce the complexity of images by grouping similar pixels into clusters, effectively compressing the image. This is useful for image storage and processing. Anomaly Detection: K-Means can be applied to detect anomalies or outliers by identifying data points that do not belong to any of the clusters. Document Clustering: In natural language processing (NLP), K-Means is used to group similar documents or articles together. It’s often used in applications like recommendation systems or news categorization. Organizing Large Datasets: When dealing with large datasets, K-Means can help in organizing the data into smaller, more manageable chunks based on similarities, improving the efficiency of data analysis. <<PAGE_37>> Challenges with K-Means Clustering • Choosing the Right Number of Clusters (𝑘 ): One of the biggest challenges is deciding how many clusters to use. • Sensitive to Initial Centroids: The final clusters can vary depending on the initial random placement of centroids. • Non-Spherical Clusters: K-Means assumes that the clusters are spherical and equally sized. This can be a problem when the actual clusters in the data are of different shapes or densities. • Outliers: K-Means is sensitive to outliers, which can distort the centroid and, ultimately, the clusters.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 40
  },
  "c5e5fafb-972a-4744-8826-4a35ed561904": {
    "filename": "Lecture 10.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 10 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Ensemble Learning Models <<PAGE_3>> Introduction • Let us say we have three different predictors, P1 , P2 and P3 for the same task. • All three, given information about the situation , predict the class to be 0 or 1. • We can then create one optimal predictive model “P” which combines best of three predictors P1,P2 and P3. • Optimal predictor “P” predicts the class to be 1 if two or more of the base predictors predict the class to be 1. • Similarly , P predicts the class to be “0” if two or more of the base predictors predict the class to be 0. • This is shown in figure below. <<PAGE_4>> Three Independent Predictors and a Predictor based on an ensemble of the three <<PAGE_5>> • Now let us assume that each of the predictors have prediction accuracy of more than 50%. • We will assume that the accuracy of all of them is 70%, though this assumption is not necessary for the discussion to follow. • We can then show that the predictor “P” is more accurate than either of the three individual predictors. • To understand why that is ,refer to figure below <<PAGE_6>> Illustration – Ensemble gives Higher quality predictions than the base models <<PAGE_7>> • Suppose there are 100 examples , • P1 predicts 70 of them correctly and 30 of them incorrectly as shown in figure. • Of the 70 that P1 predicts correctly, P2 will predict 70% of them correctly, • That is of these 70 , P2 will predict 49 ( 0.7 * 70) correctly. • These 49 will be thus predicted correctly by P also, regardless of the prediction made by P3 on these 49 cases. <<PAGE_8>> • Of the 70 predicted correctly by P1 , P2 will predict 30% of them , or 21 of them incorrectly. • This is again shown in figure. • But of these 21, P3 will predict 70% of them, or 14.7 ( 0.7 * 21) correctly. • Since P1 and P3 agree on these 14.7 examples and they are correct. P will be correct on these 14.7. <<PAGE_9>> • Next consider the 30 examples predicted incorrectly by P1 , • P2 will predict 70% of them or 21 ( 0.7*30) correctly. • Out of these 21, P3 will further predict a 70% of them or 14.7 ( 0.7 * 21) correctly. • Since on these 14.7 both P2 and P3 agree and are correct ,P will predict these correctly. <<PAGE_10>> • P will thus predict 78.4 ( = 49+14.7+14.7) of the cases correctly. • The accuracy of P is thus greater than 78%, which is higher than the accuracy of the individual predictors which is 70%. • The above argument shows how a set of predictors with accuracy greater than 50% can be used as an ensemble to build a predictor with higher accuracy than each individual predictor. <<PAGE_11>> • Another argument favoring using an ensemble is as follows: • It can be shown that if there are “n” independent distributions 2, from the sample population and each has a variance of σ • Then the variance of the mean of the distributions is given by σ2 . 𝑛 • Hence averaging of an ensemble can be used to reduce the variance of a model which essentially generates a model with less variance (i.e which is less likely to overfit). <<PAGE_12>> • However ,there is a caveat that the models used in the ensemble should not be correlated or if they are ,correlation should be low. • As an example , consider the extreme case of correlation, where we use three base classifiers and each of them predict with 70% of accuracy, but when any of them predicts an example correctly, the remaining two also predict correctly. • Similarly, when one predicts incorrectly , the remaining two also predict incorrectly. • This is the worst scenario, all three predictions are perfectly correlated. • In this case ,it should be obvious that the ensemble will have the same accuracy ,70% of either of the base classifiers. <<PAGE_13>> • The base models from which an ensemble is built need not all be of the same type. • For Example , we can build an Ensemble from a Logistic Regression Classifier ,a KNN Classifier and a Decision Tree Classifier. • We will discuss an Ensemble model built entirely from decision trees called a Random Forest Classifier. <<PAGE_14>> Ensemble Learning • Ensemble learning is a method where we use many small models instead of just one. • Each of these models may not be very strong on its own, but when we put their results together, we get a better and more accurate answer. • It's like asking a group of people for advice instead of just one person—each one might be a little wrong, but together, they usually give a better answer. <<PAGE_15>> Ensemble Learning • Ensemble Learning is a powerful machine learning technique where multiple models (often called base learners or weak learners) are combined to create a stronger, more robust model — typically called an ensemble model. • Instead of relying on a single model, combine several models to improve accuracy, reduce variance, and increase generalization. • Each model in an ensemble makes its own predictions, and the ensemble aggregates these predictions (by voting, averaging, or stacking) to produce the final output. <<PAGE_16>> <<PAGE_17>> <<PAGE_18>> Types of Ensemble Learning Methods • Bagging (Bootstrap Aggregating) : Models are trained independently on different random subsets of the training data. Their results are then combined—usually by averaging (for regression) or voting (for classification). This helps reduce variance and prevents overfitting. • Boosting : Models are trained one after another. Each new model focuses on fixing the errors made by the previous ones. The final prediction is a weighted combination of all models, which helps reduce bias and improve accuracy. • Stacking (Stacked Generalization) : Multiple different models (often of different types) are trained and their predictions are used as inputs to a final model, called a meta-model. The meta-model learns how to best combine the predictions of the base models, aiming for better performance than any individual model. <<PAGE_19>> <<PAGE_20>> <<PAGE_21>> <<PAGE_22>> Bagging (Bootstrap Aggregating) Goal: Reduce variance and prevent overfitting. How it works: • Train several models on different random subsets of the training data (sampled with replacement). • Combine results (e.g., by majority vote for classification, or averaging for regression). Popular Algorithms: • Random Forest (ensemble of decision trees) • Bagged Decision Trees <<PAGE_23>> Boosting Goal: Reduce bias and build a strong model by focusing on errors made by previous models. How it works: • Models are trained sequentially. • Each new model tries to correct the mistakes of the previous ones. Popular Algorithms: • AdaBoost (Adaptive Boosting) • Gradient Boosting • XGBoost, LightGBM, CatBoost <<PAGE_24>> Stacking (Stacked Generalization) Goal: Combine predictions from multiple models using a meta- model (or blender). How it works: • Train several base models. • Their predictions are used as input features for a meta-model, which makes the final prediction. Popular Meta-Models: Logistic Regression, Random Forest, etc. <<PAGE_25>> Bagging Algorithm • Bagging classifier can be used for both regression and classification tasks. • Here is an overview of Bagging classifier algorithm:  Bootstrap Sampling : Divides the original training data into ‘N’ subsets and randomly selects a subset with replacement in some rows from other subsets. This step ensures that the base models are trained on diverse subsets of the data and there is no class imbalance.  Base Model Training : For each bootstrapped sample we train a base model independently on that subset of data. These weak models are trained in parallel to increase computational efficiency and reduce time consumption. We can use different base learners i.e. different ML models as base learners to bring variety and robustness.  Prediction Aggregation : To make a prediction on testing data combine the predictions of all base models. For classification tasks it can include majority voting or weighted majority while for regression it involves averaging the predictions.  Out-of-Bag (OOB) Evaluation : Some samples are excluded from the training subset of particular base models during the bootstrapping method. These “out-of-bag” samples can be used to estimate the model’s performance without the need for cross-validation.  Final Prediction : After aggregating the predictions from all the base models, Bagging produces a final prediction for each instance. <<PAGE_26>> Boosting Algorithm • Boosting is an ensemble technique that combines multiple weak learners to create a strong learner. • Weak models are trained in series such that each next model tries to correct errors of the previous model until the entire training dataset is predicted correctly. • One of the most well-known boosting algorithms is AdaBoost (Adaptive Boosting). • Here is an overview of Boosting algorithm: <<PAGE_27>> Initialize Model Weights: Begin with a single weak learner and assign equal weights to all training examples. Train Weak Learner: Train weak learners on these dataset. Sequential Learning: Boosting works by training models sequentially where each model focuses on correcting the errors of its predecessor. Boosting typically uses a single type of weak learner like decision trees. Weight Adjustment: Boosting assigns weights to training datapoints. Misclassified examples receive higher weights in the next iteration so that next models pay more attention to them. <<PAGE_28>> Benefits of Ensemble Learning in Machine Learning • Reduction in Overfitting: By aggregating predictions of multiple model's ensembles can reduce overfitting that individual complex models might exhibit. • Improved Generalization: It generalizes better to unseen data by minimizing variance and bias. • Increased Accuracy: Combining multiple models gives higher predictive accuracy. • Robustness to Noise: It mitigates the effect of noisy or incorrect data points by averaging out predictions from diverse models. • Flexibility: It can work with diverse models including decision trees, neural networks and support vector machines making them highly adaptable. • Bias-Variance Tradeoff: Techniques like bagging reduce variance, while boosting reduces bias leading to better overall performance. <<PAGE_29>> Technique Category Description Random forest constructs multiple decision trees on bootstrapped subsets of the data and aggregates their Bagging predictions for final output, reducing overfitting and variance. Random Forest Trains models on random subsets of input features to Bagging enhance diversity and improve generalization while reducing overfitting. Random Subspace Method Gradient Boosting Machines sequentially builds decision trees, with each tree correcting errors of the Boosting previous ones, enhancing predictive accuracy iteratively. Gradient Boosting Machines (GBM) XGBoost do optimizations like tree pruning, Boosting regularization and parallel processing for robust and Extreme Gradient Boosting efficient predictive models. (XGBoost) AdaBoost focuses on challenging examples by Boosting assigning weights to data points. Combines weak classifiers with weighted voting for final predictions. AdaBoost (Adaptive Boosting) CatBoost specialize in handling categorical features Boosting natively without extensive preprocessing with high predictive accuracy and automatic overfitting handling. CatBoost <<PAGE_30>> Stacking <<PAGE_31>> What is Stacking? • While bagging and boosting use homogenous weak learners for ensemble, Stacking often considers heterogeneous weak learners, learns them in parallel, and combines them by training a meta-learner to output a prediction based on the different weak learners’ predictions. • A meta-learner inputs the predictions as the features and the target as the ground truth values in data D as shown in figure. <<PAGE_32>> • It attempts to learn how to combine the input predictions best to make a better output prediction. • In averaging ensemble, e.g. Random Forest, the model combines the predictions from multiple trained models. • A limitation of this approach is that each model contributes the same amount to the ensemble prediction, irrespective of how well the model performed. • An alternate approach is a weighted average ensemble, which weighs the contribution of each ensemble member by the trust in their contribution in giving the best predictions. • The weighted average ensemble provides an improvement over the model average ensemble. <<PAGE_33>> Stacking • A further generalization of this approach is replacing the linear weighted sum with Linear Regression (regression problem) or Logistic Regression (classification problem) to combine the predictions of the sub-models with any learning algorithm. • This approach is called Stacking. • In stacking, an algorithm takes sub-model outputs as input and attempts to learn how to combine the input predictions best to make a better output prediction. <<PAGE_34>> Stacking for ML • The stacked model with meta learner = Logistic Regression and the weak learners = Decision Tree, Random Forest, K Neighbors Classifier and XGBoost <<PAGE_35>> Random Forest Classifier • A Random Forest Classifier is an ensemble of many decision trees(DT). • Number of DT’ s to have in a random forest is a Hyperparameter which needs to be tuned for the dataset. • Idea is that an ensemble of DT’s will have lower variance and higher accuracy than a single DT. • To build a Random Forest ,we need DT’s for the same over all dataset ,but the trees should not be correlated or should have low correlation. • In practice ,it is difficult to ensure that the base trees have zero correlation. • Heuristics discussed here are used to minimize the correlation between decision trees used in a Random Forest Ensemble Model. <<PAGE_36>> • Let us say that , we have decided to build a random forest comprising of “N” decision trees where “N” is usually a large number and can be set to 100 or more for large datasets. • To build each of the “N” decision trees , training dataset is sampled using random sampling techniques ‘N’ times. • If the training dataset has 10000 examples, each of the 100 decision trees might be built from say 6000 examples which are sampled from the set of 10000. • This ensures that the dataset used for building each tree is different ( though there will be some overlap) ,thus hopefully leading to ‘N’ decision trees which are not significantly correlated. <<PAGE_37>> • To further reduce correlation, random forests limit the number of features that can be used for each decision made to be a subset of the features. • If there are 10 features , may be 4 features are used for each decision node in the first level of the tree. • A different set of 4 features may be used for the second level and so on. • In the second tree of the ensemble, the same strategy is used , but the features used for the first level could be different from the features used in the first level in the first tree. • Thus , different random subsets of the features are used at different levels of the same tree as well as different trees. • This has a strong effect on de-correlating the trees. • Along with minimum lead size and tree depth ,which reduces the overfitting , random forests have proved to be powerful predictors both for classification and regression. <<PAGE_38>> Build the Model using Random Forest (Default Hyperparameters) Input: from sklearn.ensemble import RandomForestClassifier Classifier = RandomForestClassifier(random_state=0,min_samples_leaf=10) Classifier.fit(X_train, y_train) Output: RandomForestClassifier(bootstrap=True , class_weight=None,criterion=‘gini’,max_depth=None,max_featu res=‘auto’,max_leaf_nodes=None,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=10,min_samples_s plit=2,min_weight_fraction_leaf=0.0,n_estimators=10.n_jobs=N one,oob_score=False,random_state=0,verbose=0,warm_start=Fal se) <<PAGE_39>> Build New Random Forest Model to avoid Overfitting • clf=RandomForestClassifier(random_state=0, • max_depth=6, • min_samples_leaf=2, • min_samples_split =2, • n_estimators=8)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 41
  },
  "7f2c7a8c-0f7f-4dd8-89a7-da7efcecfa55": {
    "filename": "MR assignment1.pdf",
    "text": "<<PAGE_1>> MR problem creation GROUP 10 Team Members: Gayathri P K (M.Sc. MI, 241109) Ajil K Dev (MBA, 242003) Nanda Krishnan V (MSc CS, 241036) Afnan K (MSc CS, 241008) Alina N A, (MSc CSDA, 243006) Joseph (MBA) <<PAGE_2>> Report on Identifying Reasons for Client Account Closure and Non-Opening of New Accounts Introduction Customer acquisition and retention are two crucial success factors in the contemporary banking sector. Customers are increasingly refusing to open new accounts or closing their current ones, despite banks spending a lot of money on marketing campaigns and digital platforms to draw in new customers. In the long run, this problem undermines customer trust in addition to having an impact on profitability. The goal of the current study is to look into the root causes of this issue and offer practical solutions to raise customer retention and satisfaction. Objectives The following are the research’s goal: • to determine the causes of prospective customers' reluctance to open new accounts • to ascertain what causes current customers to terminate their accounts. • to offer data-based suggestions for enhancing client acquisition and retention. Review of Literature Prior research in banking and financial services indicates that a number of factors, including: Service Quality: According to research, one of the main reasons why accounts are closed is inadequate customer service. <<PAGE_3>> Minimum Balance Requirements: Exorbitant maintenance costs or balance requirements put a strain on finances and deter people from opening new accounts. Service Accessibility: Convenience and accessibility are diminished by a lack of branches in rural or semi-urban areas. Transaction Delays: Prolonged processing times for simple banking services can irritate customers and drive them to rival banks. By concentrating on the Indian banking environment and clients with moderate to high income levels, this study adds to the body of existing literature. Theories The following theories are put forth in light of initial observations:  H1: Customers are deterred from opening or keeping accounts by high minimum balance requirements  H2: Account closures are a result of subpar customer service.  H3: Fewer people open accounts when there aren't enough bank branches nearby.  H4: Client discontent and closures are caused by lengthy transaction and processing times. Data Needs The following information will be gathered in order to test the aforementioned theories: 1) User demographics include location, age, and income level. 2) User input: Detailed justifications for closing accounts or declining to open new ones. <<PAGE_4>> Methods of Research The purpose of the causal research design is to ascertain the causal relationships between the factors that have been identified and the decisions made by customers. People who earn at least ₹50,000 per month are the target population. Sampling Technique: SMS-based surveys and email marketing. 100–200 people make up the sample size. Method of Data Collection: Digital distribution of structured questionnaires via email and SMS. Plan for Data Analysis The following methods will be used to analyse the gathered data: a. Descriptive analysis: a summary of feedback patterns and demographic distribution. b. Testing hypotheses: Regression analysis or the chi-square test are used to determine the connections between customer choices and service-related problems. c. Comparative Analysis: Finding the main distinctions between respondents who closed their accounts and those who did not open new ones is known as comparative analysis. Expected Outcomes It is anticipated that the study will identify the key elements causing customer discontent, including: • high minimum requirements for balance. • Poor customer service quality and responsiveness. • In semi-urban and rural areas, there are not enough branches. <<PAGE_5>> • Transaction processing time is inefficient. These results will give banks useful information to rethink their regulations and raise the caliber of their services. Conclusion Banks will be better able to comprehend the causes of customer attrition and account opening hesitancy thanks to this research. Banks can improve their competitiveness, draw in new customers, and fortify enduring bonds with current ones by tackling the issues that have been identified.",
    "chunk_size": 600,
    "overlap": 120,
    "chunks": 9
  },
  "b64c2892-f3df-42e7-844e-58aac1ce6922": {
    "filename": "MR assignment1.pdf",
    "text": "<<PAGE_1>> MR problem creation GROUP 10 Team Members: Gayathri P K (M.Sc. MI, 241109) Ajil K Dev (MBA, 242003) Nanda Krishnan V (MSc CS, 241036) Afnan K (MSc CS, 241008) Alina N A, (MSc CSDA, 243006) Joseph (MBA) <<PAGE_2>> Report on Identifying Reasons for Client Account Closure and Non-Opening of New Accounts Introduction Customer acquisition and retention are two crucial success factors in the contemporary banking sector. Customers are increasingly refusing to open new accounts or closing their current ones, despite banks spending a lot of money on marketing campaigns and digital platforms to draw in new customers. In the long run, this problem undermines customer trust in addition to having an impact on profitability. The goal of the current study is to look into the root causes of this issue and offer practical solutions to raise customer retention and satisfaction. Objectives The following are the research’s goal: • to determine the causes of prospective customers' reluctance to open new accounts • to ascertain what causes current customers to terminate their accounts. • to offer data-based suggestions for enhancing client acquisition and retention. Review of Literature Prior research in banking and financial services indicates that a number of factors, including: Service Quality: According to research, one of the main reasons why accounts are closed is inadequate customer service. <<PAGE_3>> Minimum Balance Requirements: Exorbitant maintenance costs or balance requirements put a strain on finances and deter people from opening new accounts. Service Accessibility: Convenience and accessibility are diminished by a lack of branches in rural or semi-urban areas. Transaction Delays: Prolonged processing times for simple banking services can irritate customers and drive them to rival banks. By concentrating on the Indian banking environment and clients with moderate to high income levels, this study adds to the body of existing literature. Theories The following theories are put forth in light of initial observations:  H1: Customers are deterred from opening or keeping accounts by high minimum balance requirements  H2: Account closures are a result of subpar customer service.  H3: Fewer people open accounts when there aren't enough bank branches nearby.  H4: Client discontent and closures are caused by lengthy transaction and processing times. Data Needs The following information will be gathered in order to test the aforementioned theories: 1) User demographics include location, age, and income level. 2) User input: Detailed justifications for closing accounts or declining to open new ones. <<PAGE_4>> Methods of Research The purpose of the causal research design is to ascertain the causal relationships between the factors that have been identified and the decisions made by customers. People who earn at least ₹50,000 per month are the target population. Sampling Technique: SMS-based surveys and email marketing. 100–200 people make up the sample size. Method of Data Collection: Digital distribution of structured questionnaires via email and SMS. Plan for Data Analysis The following methods will be used to analyse the gathered data: a. Descriptive analysis: a summary of feedback patterns and demographic distribution. b. Testing hypotheses: Regression analysis or the chi-square test are used to determine the connections between customer choices and service-related problems. c. Comparative Analysis: Finding the main distinctions between respondents who closed their accounts and those who did not open new ones is known as comparative analysis. Expected Outcomes It is anticipated that the study will identify the key elements causing customer discontent, including: • high minimum requirements for balance. • Poor customer service quality and responsiveness. • In semi-urban and rural areas, there are not enough branches. <<PAGE_5>> • Transaction processing time is inefficient. These results will give banks useful information to rethink their regulations and raise the caliber of their services. Conclusion Banks will be better able to comprehend the causes of customer attrition and account opening hesitancy thanks to this research. Banks can improve their competitiveness, draw in new customers, and fortify enduring bonds with current ones by tackling the issues that have been identified.",
    "chunk_size": 600,
    "overlap": 120,
    "chunks": 9
  },
  "6d8b723f-26b0-4441-8e28-ce24341535c2": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "b73f450c-18b7-49e9-9bca-a29661313530": {
    "filename": "Lecture 10.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 10 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Ensemble Learning Models <<PAGE_3>> Introduction • Let us say we have three different predictors, P1 , P2 and P3 for the same task. • All three, given information about the situation , predict the class to be 0 or 1. • We can then create one optimal predictive model “P” which combines best of three predictors P1,P2 and P3. • Optimal predictor “P” predicts the class to be 1 if two or more of the base predictors predict the class to be 1. • Similarly , P predicts the class to be “0” if two or more of the base predictors predict the class to be 0. • This is shown in figure below. <<PAGE_4>> Three Independent Predictors and a Predictor based on an ensemble of the three <<PAGE_5>> • Now let us assume that each of the predictors have prediction accuracy of more than 50%. • We will assume that the accuracy of all of them is 70%, though this assumption is not necessary for the discussion to follow. • We can then show that the predictor “P” is more accurate than either of the three individual predictors. • To understand why that is ,refer to figure below <<PAGE_6>> Illustration – Ensemble gives Higher quality predictions than the base models <<PAGE_7>> • Suppose there are 100 examples , • P1 predicts 70 of them correctly and 30 of them incorrectly as shown in figure. • Of the 70 that P1 predicts correctly, P2 will predict 70% of them correctly, • That is of these 70 , P2 will predict 49 ( 0.7 * 70) correctly. • These 49 will be thus predicted correctly by P also, regardless of the prediction made by P3 on these 49 cases. <<PAGE_8>> • Of the 70 predicted correctly by P1 , P2 will predict 30% of them , or 21 of them incorrectly. • This is again shown in figure. • But of these 21, P3 will predict 70% of them, or 14.7 ( 0.7 * 21) correctly. • Since P1 and P3 agree on these 14.7 examples and they are correct. P will be correct on these 14.7. <<PAGE_9>> • Next consider the 30 examples predicted incorrectly by P1 , • P2 will predict 70% of them or 21 ( 0.7*30) correctly. • Out of these 21, P3 will further predict a 70% of them or 14.7 ( 0.7 * 21) correctly. • Since on these 14.7 both P2 and P3 agree and are correct ,P will predict these correctly. <<PAGE_10>> • P will thus predict 78.4 ( = 49+14.7+14.7) of the cases correctly. • The accuracy of P is thus greater than 78%, which is higher than the accuracy of the individual predictors which is 70%. • The above argument shows how a set of predictors with accuracy greater than 50% can be used as an ensemble to build a predictor with higher accuracy than each individual predictor. <<PAGE_11>> • Another argument favoring using an ensemble is as follows: • It can be shown that if there are “n” independent distributions 2, from the sample population and each has a variance of σ • Then the variance of the mean of the distributions is given by σ2 . 𝑛 • Hence averaging of an ensemble can be used to reduce the variance of a model which essentially generates a model with less variance (i.e which is less likely to overfit). <<PAGE_12>> • However ,there is a caveat that the models used in the ensemble should not be correlated or if they are ,correlation should be low. • As an example , consider the extreme case of correlation, where we use three base classifiers and each of them predict with 70% of accuracy, but when any of them predicts an example correctly, the remaining two also predict correctly. • Similarly, when one predicts incorrectly , the remaining two also predict incorrectly. • This is the worst scenario, all three predictions are perfectly correlated. • In this case ,it should be obvious that the ensemble will have the same accuracy ,70% of either of the base classifiers. <<PAGE_13>> • The base models from which an ensemble is built need not all be of the same type. • For Example , we can build an Ensemble from a Logistic Regression Classifier ,a KNN Classifier and a Decision Tree Classifier. • We will discuss an Ensemble model built entirely from decision trees called a Random Forest Classifier. <<PAGE_14>> Ensemble Learning • Ensemble learning is a method where we use many small models instead of just one. • Each of these models may not be very strong on its own, but when we put their results together, we get a better and more accurate answer. • It's like asking a group of people for advice instead of just one person—each one might be a little wrong, but together, they usually give a better answer. <<PAGE_15>> Ensemble Learning • Ensemble Learning is a powerful machine learning technique where multiple models (often called base learners or weak learners) are combined to create a stronger, more robust model — typically called an ensemble model. • Instead of relying on a single model, combine several models to improve accuracy, reduce variance, and increase generalization. • Each model in an ensemble makes its own predictions, and the ensemble aggregates these predictions (by voting, averaging, or stacking) to produce the final output. <<PAGE_16>> <<PAGE_17>> <<PAGE_18>> Types of Ensemble Learning Methods • Bagging (Bootstrap Aggregating) : Models are trained independently on different random subsets of the training data. Their results are then combined—usually by averaging (for regression) or voting (for classification). This helps reduce variance and prevents overfitting. • Boosting : Models are trained one after another. Each new model focuses on fixing the errors made by the previous ones. The final prediction is a weighted combination of all models, which helps reduce bias and improve accuracy. • Stacking (Stacked Generalization) : Multiple different models (often of different types) are trained and their predictions are used as inputs to a final model, called a meta-model. The meta-model learns how to best combine the predictions of the base models, aiming for better performance than any individual model. <<PAGE_19>> <<PAGE_20>> <<PAGE_21>> <<PAGE_22>> Bagging (Bootstrap Aggregating) Goal: Reduce variance and prevent overfitting. How it works: • Train several models on different random subsets of the training data (sampled with replacement). • Combine results (e.g., by majority vote for classification, or averaging for regression). Popular Algorithms: • Random Forest (ensemble of decision trees) • Bagged Decision Trees <<PAGE_23>> Boosting Goal: Reduce bias and build a strong model by focusing on errors made by previous models. How it works: • Models are trained sequentially. • Each new model tries to correct the mistakes of the previous ones. Popular Algorithms: • AdaBoost (Adaptive Boosting) • Gradient Boosting • XGBoost, LightGBM, CatBoost <<PAGE_24>> Stacking (Stacked Generalization) Goal: Combine predictions from multiple models using a meta- model (or blender). How it works: • Train several base models. • Their predictions are used as input features for a meta-model, which makes the final prediction. Popular Meta-Models: Logistic Regression, Random Forest, etc. <<PAGE_25>> Bagging Algorithm • Bagging classifier can be used for both regression and classification tasks. • Here is an overview of Bagging classifier algorithm:  Bootstrap Sampling : Divides the original training data into ‘N’ subsets and randomly selects a subset with replacement in some rows from other subsets. This step ensures that the base models are trained on diverse subsets of the data and there is no class imbalance.  Base Model Training : For each bootstrapped sample we train a base model independently on that subset of data. These weak models are trained in parallel to increase computational efficiency and reduce time consumption. We can use different base learners i.e. different ML models as base learners to bring variety and robustness.  Prediction Aggregation : To make a prediction on testing data combine the predictions of all base models. For classification tasks it can include majority voting or weighted majority while for regression it involves averaging the predictions.  Out-of-Bag (OOB) Evaluation : Some samples are excluded from the training subset of particular base models during the bootstrapping method. These “out-of-bag” samples can be used to estimate the model’s performance without the need for cross-validation.  Final Prediction : After aggregating the predictions from all the base models, Bagging produces a final prediction for each instance. <<PAGE_26>> Boosting Algorithm • Boosting is an ensemble technique that combines multiple weak learners to create a strong learner. • Weak models are trained in series such that each next model tries to correct errors of the previous model until the entire training dataset is predicted correctly. • One of the most well-known boosting algorithms is AdaBoost (Adaptive Boosting). • Here is an overview of Boosting algorithm: <<PAGE_27>> Initialize Model Weights: Begin with a single weak learner and assign equal weights to all training examples. Train Weak Learner: Train weak learners on these dataset. Sequential Learning: Boosting works by training models sequentially where each model focuses on correcting the errors of its predecessor. Boosting typically uses a single type of weak learner like decision trees. Weight Adjustment: Boosting assigns weights to training datapoints. Misclassified examples receive higher weights in the next iteration so that next models pay more attention to them. <<PAGE_28>> Benefits of Ensemble Learning in Machine Learning • Reduction in Overfitting: By aggregating predictions of multiple model's ensembles can reduce overfitting that individual complex models might exhibit. • Improved Generalization: It generalizes better to unseen data by minimizing variance and bias. • Increased Accuracy: Combining multiple models gives higher predictive accuracy. • Robustness to Noise: It mitigates the effect of noisy or incorrect data points by averaging out predictions from diverse models. • Flexibility: It can work with diverse models including decision trees, neural networks and support vector machines making them highly adaptable. • Bias-Variance Tradeoff: Techniques like bagging reduce variance, while boosting reduces bias leading to better overall performance. <<PAGE_29>> Technique Category Description Random forest constructs multiple decision trees on bootstrapped subsets of the data and aggregates their Bagging predictions for final output, reducing overfitting and variance. Random Forest Trains models on random subsets of input features to Bagging enhance diversity and improve generalization while reducing overfitting. Random Subspace Method Gradient Boosting Machines sequentially builds decision trees, with each tree correcting errors of the Boosting previous ones, enhancing predictive accuracy iteratively. Gradient Boosting Machines (GBM) XGBoost do optimizations like tree pruning, Boosting regularization and parallel processing for robust and Extreme Gradient Boosting efficient predictive models. (XGBoost) AdaBoost focuses on challenging examples by Boosting assigning weights to data points. Combines weak classifiers with weighted voting for final predictions. AdaBoost (Adaptive Boosting) CatBoost specialize in handling categorical features Boosting natively without extensive preprocessing with high predictive accuracy and automatic overfitting handling. CatBoost <<PAGE_30>> Stacking <<PAGE_31>> What is Stacking? • While bagging and boosting use homogenous weak learners for ensemble, Stacking often considers heterogeneous weak learners, learns them in parallel, and combines them by training a meta-learner to output a prediction based on the different weak learners’ predictions. • A meta-learner inputs the predictions as the features and the target as the ground truth values in data D as shown in figure. <<PAGE_32>> • It attempts to learn how to combine the input predictions best to make a better output prediction. • In averaging ensemble, e.g. Random Forest, the model combines the predictions from multiple trained models. • A limitation of this approach is that each model contributes the same amount to the ensemble prediction, irrespective of how well the model performed. • An alternate approach is a weighted average ensemble, which weighs the contribution of each ensemble member by the trust in their contribution in giving the best predictions. • The weighted average ensemble provides an improvement over the model average ensemble. <<PAGE_33>> Stacking • A further generalization of this approach is replacing the linear weighted sum with Linear Regression (regression problem) or Logistic Regression (classification problem) to combine the predictions of the sub-models with any learning algorithm. • This approach is called Stacking. • In stacking, an algorithm takes sub-model outputs as input and attempts to learn how to combine the input predictions best to make a better output prediction. <<PAGE_34>> Stacking for ML • The stacked model with meta learner = Logistic Regression and the weak learners = Decision Tree, Random Forest, K Neighbors Classifier and XGBoost <<PAGE_35>> Random Forest Classifier • A Random Forest Classifier is an ensemble of many decision trees(DT). • Number of DT’ s to have in a random forest is a Hyperparameter which needs to be tuned for the dataset. • Idea is that an ensemble of DT’s will have lower variance and higher accuracy than a single DT. • To build a Random Forest ,we need DT’s for the same over all dataset ,but the trees should not be correlated or should have low correlation. • In practice ,it is difficult to ensure that the base trees have zero correlation. • Heuristics discussed here are used to minimize the correlation between decision trees used in a Random Forest Ensemble Model. <<PAGE_36>> • Let us say that , we have decided to build a random forest comprising of “N” decision trees where “N” is usually a large number and can be set to 100 or more for large datasets. • To build each of the “N” decision trees , training dataset is sampled using random sampling techniques ‘N’ times. • If the training dataset has 10000 examples, each of the 100 decision trees might be built from say 6000 examples which are sampled from the set of 10000. • This ensures that the dataset used for building each tree is different ( though there will be some overlap) ,thus hopefully leading to ‘N’ decision trees which are not significantly correlated. <<PAGE_37>> • To further reduce correlation, random forests limit the number of features that can be used for each decision made to be a subset of the features. • If there are 10 features , may be 4 features are used for each decision node in the first level of the tree. • A different set of 4 features may be used for the second level and so on. • In the second tree of the ensemble, the same strategy is used , but the features used for the first level could be different from the features used in the first level in the first tree. • Thus , different random subsets of the features are used at different levels of the same tree as well as different trees. • This has a strong effect on de-correlating the trees. • Along with minimum lead size and tree depth ,which reduces the overfitting , random forests have proved to be powerful predictors both for classification and regression. <<PAGE_38>> Build the Model using Random Forest (Default Hyperparameters) Input: from sklearn.ensemble import RandomForestClassifier Classifier = RandomForestClassifier(random_state=0,min_samples_leaf=10) Classifier.fit(X_train, y_train) Output: RandomForestClassifier(bootstrap=True , class_weight=None,criterion=‘gini’,max_depth=None,max_featu res=‘auto’,max_leaf_nodes=None,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=10,min_samples_s plit=2,min_weight_fraction_leaf=0.0,n_estimators=10.n_jobs=N one,oob_score=False,random_state=0,verbose=0,warm_start=Fal se) <<PAGE_39>> Build New Random Forest Model to avoid Overfitting • clf=RandomForestClassifier(random_state=0, • max_depth=6, • min_samples_leaf=2, • min_samples_split =2, • n_estimators=8)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 41
  },
  "69e8fead-53f9-433f-8d31-5e45f1e476e3": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "8f7b40e3-c449-42b8-a97d-a5613f5b5ba6": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "005bcc77-6e1a-4482-8c5a-fbeb20a1c544": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "3f78a995-f1a4-4109-b656-ae559fe1ef8a": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "11cabc99-8507-4780-b6d0-93d55d6025c0": {
    "filename": "sample doc.pdf",
    "text": "<<PAGE_1>> 1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. <<PAGE_2>> • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "c401cdd9-d6d9-43bd-9bd2-23f97ef8601b": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "51566d01-04a4-43ec-a85f-fb504af5b6c8": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "89d2ecec-932e-48bd-8698-f71157143e1a": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  },
  "084381bc-85d5-463d-906f-eca7e02d35cd": {
    "filename": "H_33426864.pdf",
    "text": "<<PAGE_1>> INVOICE Invoice # HSG-6291848 Hostinger PTE 16 Raffles Quay, #33-02, Hong Leong Building Invoice Issued # Nov 25, 2025 Singapore 48581 Invoice Amount # ₹8,988.00 (INR) Singapore GST Reg #: 9919SGP29004OSJ Next Billing Date # Nov 25, 2026 Order Nr. # hb_41187973 PAID BILLED TO NASSCOM FOUNDATION NASSCOM FOUNDATION IN07AAATN4866D1ZK NASSCOM FOUNDATION 3RD FLOOR, A1/125, SAFDARJUNG ENCLAVE, SAFDARJUNG ENCLAVE South Delhi 110029 Delhi India adabizz025@gmail.com 917736585213 DESCRIPTION PRICE DISCOUNT TOTAL EXCL. GST GST AMOUNT (INR) KVM 4 (billed every year) ₹8,988.00 x 1 - ₹8,988.00 ₹0.00 ₹8,988.00 Nov 25, 2025 to Nov 25, 2026 Total excl. GST ₹8,988.00 (SGD 131.27) Total ₹8,988.00(SGD 131.27) Payments(₹8,988.00) Amount Due ₹0.00 (INR)",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 2
  }
}