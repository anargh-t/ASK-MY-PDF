{
  "62bd5715-6e2e-48a9-90f2-8ec97700257f": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "7220e5c0-2333-4632-88b3-1cb5b518d47c": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "121e909d-5014-43d5-91d4-86212417c19f": {
    "filename": "compliance-report-job_20251116_162840-2025-11-16.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_162840 Search Term: almond milk Generated On: 16/11/2025, 4:30:30 pm Analysis Date: 16/11/2025, 4:30:10 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 7 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: So Good Plant-Based Almond Beverage Vanilla NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: So Good Plant-Based Almond Beverage Unsweetened NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 3 of 5 Generated by Product Compliance Checker Product 3: So Good Plant-Based Almond Beverage Chocolate NON-COMPLIANT Score: 71% Product Images: Violations (2): 1. MRP value does not contain a numeric price. 2. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 So Good Plant-Based Almond Beverage Vani... Non-Compliant 71% 2 2 So Good Plant-Based Almond Beverage Unsw... Non-Compliant 57% 3 3 So Good Plant-Based Almond Beverage Choc... Non-Compliant 71% 2 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "05b0b6a4-40c5-4122-b244-6a576cf56c5d": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "e49aed1e-22dc-4673-913e-8ecf9d4d2dc3": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "0a23b352-8d1b-442a-bafa-813e66456fbe": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "304f1742-839a-424f-98ac-678e019ab83f": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 3
  },
  "9c8158ec-6b5e-4586-9f45-6cdfbabe9113": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "fa1296ac-73e5-405a-8b16-80939340cf83": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "af1c896c-d0b5-4007-bcc2-5a2da427768f": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "a8152c1b-0a8a-4f8d-9c87-847e1493bbb1": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "da27028a-17ea-4390-87f5-d22f092a5449": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "6d0683d7-fb10-412a-97d5-f26cc89b1629": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "02fe0b67-844c-43de-a521-82d9346e2ccd": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "d05fe906-8e0c-4b50-99e0-64c929c81be9": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "94843540-ea96-444c-a198-20032c7b9b30": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "d8a148c9-7a08-4d61-9cd9-2daf0ddce6f0": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "cbb02ffd-c1b3-4479-b0e2-23dd978beb57": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "27ff32aa-0488-4996-80ba-67b974137918": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "4fb084bd-edf8-4c38-b53e-f887e4c68154": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "b147d7ef-1520-435c-b506-6b8b69efde27": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "4f7f63c8-b8f2-42ff-9a87-5315f4b4e20d": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "2beb6d48-08a0-465b-a365-c6b7c547d6ae": {
    "filename": "compliance-report-job_20251116_120736-2025-11-16 1.pdf",
    "text": "Product Compliance Report AI-Powered Compliance Analysis Job ID: job_20251116_120736 Search Term: maggie Generated On: 16/11/2025, 2:40:07 pm Analysis Date: 16/11/2025, 12:09:24 pm Executive Summary Metric Value Total Products Analyzed 3 Compliant Products 0 Non-Compliant Products 3 Total Violations Found 5 Compliance Rate 0.0% Page 1 of 5 Generated by Product Compliance Checker Detailed Product Analysis Product 1: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 57% Product Images: Violations (3): 1. MRP value is missing. 2. Missing date of manufacture or import. 3. Country of Origin is missing or invalid. Page 2 of 5 Generated by Product Compliance Checker Product 2: Slurrp Farm Instant Millet Noodles Magic Masala NON-COMPLIANT Score: 86% Product Images: Violations (1): 1. Country of Origin is missing or invalid. Product 3: Slurrp Farm Millet Noodles NON-COMPLIANT Score: 86% Product Images: Page 3 of 5 Generated by Product Compliance Checker Violations (1): 1. Country of Origin is missing or invalid. Page 4 of 5 Generated by Product Compliance Checker Compliance Summary Table # Product Name Status Score Violations 1 Slurrp Farm Millet Noodles Non-Compliant 57% 3 2 Slurrp Farm Instant Millet Noodles Magic... Non-Compliant 86% 1 3 Slurrp Farm Millet Noodles Non-Compliant 86% 1 Page 5 of 5 Generated by Product Compliance Checker",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 4
  },
  "c0f73802-a53a-453c-a49f-0ea649af6814": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "fbd189fc-b475-469e-8bac-e7f160646dce": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "1ecc6cf7-1781-42d9-8cc8-5a887b21b1fe": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "befbb262-ec7f-44c3-a3ce-cf02427fc80c": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "4f2b4e39-3266-4fbf-9c0b-fff21b2d1ad7": {
    "filename": "sample_pdf.pdf",
    "text": "1. The Era of Generative AI: Beyond the Hype This is the dominant trend, largely driven by the public release of models like ChatGPT, DALL-E, and Midjourney. The focus is now shifting from pure novelty to practical application and refinement. • Multimodality is King: The newest models don't just handle text or images in isolation. They can seamlessly understand and generate combinations of text, images, audio, and video. For example, you can show a model a picture and ask it a question about it, or generate a video from a text description. GPT-4V and Google's Gemini are prime examples. • Smaller, Specialized Models: While giants like GPT-4 are powerful, they are expensive and unwieldy for many specific tasks. The trend is now towards creating smaller, more efficient \"fine-tuned\" models that excel at a particular job (e.g., legal document review, medical diagnosis assistance) at a fraction of the cost. This is often called \"Small Language Models (SLMs)\". • AI Agents: The next step is moving from a conversational chatbot to an autonomous agent. These are AI systems that can be given a high-level goal (e.g., \"Plan a 5-day vacation to Tokyo\"), and then independently break it down into steps: researching flights, booking hotels, creating an itinerary, and making restaurant reservations by using tools and APIs. 2. The Rise of \"Responsible AI\" and Governance As AI becomes more powerful, the backlash and concern about its ethical implications have grown exponentially. This is no longer an afterthought but a core business requirement. • Explainable AI (XAI): There's a massive push to make the \"black box\" of complex models more transparent. Why did the model make that decision? This is critical for high-stakes fields like healthcare, finance, and criminal justice. • Bias, Fairness, and Safety: Actively developing techniques to identify and mitigate biases in training data and model outputs. This includes \"red teaming\" models to find harmful behaviors before release and implementing robust guardrails. • AI Regulation: Governments worldwide are scrambling to create rules. The EU's AI Act is leading the charge, creating a risk-based regulatory framework. Companies are now building internal governance teams to ensure compliance. 3. The Hardware and Infrastructure Revolution You can't run a trillion-parameter model on a standard laptop. The demand for more powerful and efficient computing is fueling its own trends. • The NPU (Neural Processing Unit): Chip manufacturers (like Intel, AMD, Apple, and Qualcomm) are integrating dedicated AI accelerators (NPUs) directly into CPUs and consumer devices. This is what powers the \"AI PC\" trend, allowing you to run models locally on your laptop or phone for better privacy and speed. • Custom AI Chips: Beyond consumer hardware, tech giants like Google (TPU), Amazon (Inferentia/Trainium), and NVIDIA (H100/B100) are in an arms race to build the most powerful data center chips specifically designed for training and running massive AI models. 4. Cutting-Edge Model Architectures Research continues to push the boundaries of what's possible. • Transformers & Attention Mechanisms: While the Transformer architecture (from the 2017 \"Attention is All You Need\" paper) still dominates, new variants and improvements are constantly emerging to make them more efficient and capable of handling longer contexts (e.g., processing entire books at once). • Diffusion Models: This is the architecture that powers most of the current state-of-the-art image generators (like Stable Diffusion and Midjourney). It works by progressively adding noise to data and then learning to reverse the process, creating highly detailed and coherent images from chaos. • Multimodal Architectures: Research is focused on the best ways to fuse different data types (text, vision, audio) into a single, cohesive model that has a deep, unified understanding of the world. 5. Practical Applications and Democratization The \"how\" of using AI is becoming as important as the \"what.\" • Retrieval-Augmented Generation (RAG): This is a pivotal technique for making LLMs useful for businesses. Instead of relying solely on a model's internal knowledge (which can be outdated or generic), RAG allows the model to pull information from a custom, up-to-date database (like a company's internal documents) to provide accurate, context-specific answers. This is the foundation of most modern corporate chatbots. • Low-Code/No-Code AI Platforms: Tools are emerging that allow non-experts to build and deploy AI solutions using drag-and-drop interfaces. This is democratizing AI, enabling domain experts in marketing, finance, or HR to create solutions without needing a PhD in data science. • AI in Science and Engineering: AI is accelerating discovery in fields like drug discovery (predicting protein structures with AlphaFold), material science (designing new alloys or batteries), and climate modeling.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "0d7e5960-0377-433f-ab59-7db357a97367": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "c1b39f51-c7f9-45f8-9339-08577c2d7827": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "d87c8f74-4b38-4d5c-beac-e916941a1043": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "3a6bac03-43eb-48c6-929e-8bae9a810bff": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "514ab5b9-e896-45fe-aa0f-3c1f83f37c01": {
    "filename": "MobSF_Assignment 2.pdf",
    "text": "Mobile Security Framework (MobSF) To perform of a mobile application (APK/IPA) using the and interpret the results. I clone Mobile Security Framework (MobSF): https://github.com/MobSF/Mobile- Security-Framework-MobSF and install and configure it with docker Login Screen of MobSF Default Username and password – mobsf:mobsf Next we upload an android apk file for analyze for that I already download a apk from the external source : SuperVPN Pro_1.9.2_APKPure.apk APP SCORES • Security Score: 52/100 • Trackers Detected: 3/32 • Button: MobiSF Scorecard FILE INFORMATION • File Name: SuperVPN_Pro_1.9.2_APKPure.apk • Size: 20.4 MB • MD5: bc2678292e0befdd1daa221558bf339 • SHA1: 760498738dfeb6d6a2064eaa68fb76fd65d614c9 • SHA256: 3965132062c85d5640fb9d02705dc85b813ca2a927892c16c7644c1b553edae APP INFORMATION • App Name: SuperVPN Pro • Package Name: com.jrzheng.supervnpayment • Main Activity: com.jrzheng.supervnpayment.activity.MainActivity • Target SDK: 34 • Min SDK: 23 • Max SDK: (blank / not listed) • Android Version Name: 1.9.2 • Android Version Code: 52 PLAYSTORE INFORMATION • Title: SuperVPN Pro • Score: 4.6 (113K reviews) • Installs: 10,000,000+ • Price: Android Version Support: Category Productivity • Play Store URL: com.jrzheng.supervnpayment • Developer: SuperSoftTech • Developer 2: SuperSoftTech • Developer Address: None • Developer Website: https://www.supervpnpro.app • Developer Email: goanalyticsgpp@gmail.com • Release Date: Apr 11, 2014 • Privacy Policy: Privacy link Description (as in screenshot) SuperVPN Pro, total free VPN service without any limitation. Easy to use, one click to connect to VPN server. Unlimited bandwidth and unlimited free time. Fast servers across the world. Features: • Protect your privacy, keep you safe from 3rd party tracking • Unblock geographically restricted websites • No registration required, no settings required • No speed limitation, no bandwidth limitation • One-click to connect to VPN • No root access needed • Encrypt your internet traffic • Top server speed & reliability • Using most secure VPN solution The app is free forever. There is no need to purchase if the free version is good enough for you. Purchasing VIP will remove ads, give you faster speed and better experience. You can cancel VIP at any time and get a full refund. EXPORTED COMPONENTS • Exported Activities: 0 / 20 • Exported Services: 2 / 14 • Exported Receivers: 2 / 11 • Exported Providers: 0 / 4 Application Permissions These are the permissions the app requests from Android. Most are normal network-related permissions, but a couple stand out: – It can query all installed apps. – It can post notifications. Overall, it leans heavily into advertising-related permissions. Android API Usage This shows which Android APIs the app actively uses. It uses crypto, Base64 encoding/decoding, certificate handling, dynamic class loading, OS command execution, and advertising ID access — a mix often seen in VPNs but also potentially in shady apps. Certificate Analysis The APK is signed using outdated or weak signature schemes. Two warnings appear: – Vulnerable to the Janus signing flaw. – Uses SHA-1 in parts of the signing chain, which is collision-prone. This doesn’t prove malicious intent, but it weakens trust in the app’s integrity. App Issues These are weaknesses found in the app’s configuration. It shows outdated Android support, clear-text traffic enabled, backup enabled, and several components that expose permissions incorrectly — all signs of sloppy security hygiene. Code Analysis These findings come from scanning the app’s source code. Issues include weak cryptography, insecure random numbers, logging sensitive data, hardcoded secrets, SQL injection risk, and MD5 usage. Only a couple of items are marked secure, like SSL pinning. Behaviour Analysis This lists what the app actually does during execution. It reads files, parses JSON, opens local files, connects to remote URLs, queries installed apps, uses reflection, triggers implicit intents, and retrieves SIM country data. The behaviors match a networked app with analytics and tracking capabilities. Conclusion The whole picture points to an app that works, but cuts corners everywhere that matters for security. The permissions are broad, the APIs used lean toward tracking and dynamic loading, the certificates are outdated, and the code base mixes weak crypto, insecure randomness, clear-text traffic, and sloppy logging. On top of that, several components expose permissions incorrectly, making them accessible to other apps. The behavior analysis shows it probes the device, reaches out to remote hosts, inspects installed packages, and pulls SIM data. None of these items alone guarantee malicious intent, but together they form a pattern: the app is built with minimal security discipline and maximum data access. It’s the kind of VPN product that may function on the surface but introduces more risk than it removes. A secure VPN should reduce your exposure; this one expands it. For someone studying or analyzing app security, it’s a solid example of how technical debt, weak crypto, and unnecessary permissions converge into a high-risk footprint.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "d3199ffd-8bd5-4280-a26b-81909529aa50": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "042b06e2-0ec1-42d7-8fbb-8eb014d0b2f3": {
    "filename": "MobSF_Assignment 3.pdf",
    "text": "Mobile Security Framework (MobSF) To perform of a mobile application (APK/IPA) using the and interpret the results. I clone Mobile Security Framework (MobSF): https://github.com/MobSF/Mobile- Security-Framework-MobSF and install and configure it with docker Login Screen of MobSF Default Username and password – mobsf:mobsf Next we upload an android apk file for analyze for that I already download a apk from the external source : SuperVPN Pro_1.9.2_APKPure.apk APP SCORES • Security Score: 52/100 • Trackers Detected: 3/32 • Button: MobiSF Scorecard FILE INFORMATION • File Name: SuperVPN_Pro_1.9.2_APKPure.apk • Size: 20.4 MB • MD5: bc2678292e0befdd1daa221558bf339 • SHA1: 760498738dfeb6d6a2064eaa68fb76fd65d614c9 • SHA256: 3965132062c85d5640fb9d02705dc85b813ca2a927892c16c7644c1b553edae APP INFORMATION • App Name: SuperVPN Pro • Package Name: com.jrzheng.supervnpayment • Main Activity: com.jrzheng.supervnpayment.activity.MainActivity • Target SDK: 34 • Min SDK: 23 • Max SDK: (blank / not listed) • Android Version Name: 1.9.2 • Android Version Code: 52 PLAYSTORE INFORMATION • Title: SuperVPN Pro • Score: 4.6 (113K reviews) • Installs: 10,000,000+ • Price: Android Version Support: Category Productivity • Play Store URL: com.jrzheng.supervnpayment • Developer: SuperSoftTech • Developer 2: SuperSoftTech • Developer Address: None • Developer Website: https://www.supervpnpro.app • Developer Email: goanalyticsgpp@gmail.com • Release Date: Apr 11, 2014 • Privacy Policy: Privacy link Description (as in screenshot) SuperVPN Pro, total free VPN service without any limitation. Easy to use, one click to connect to VPN server. Unlimited bandwidth and unlimited free time. Fast servers across the world. Features: • Protect your privacy, keep you safe from 3rd party tracking • Unblock geographically restricted websites • No registration required, no settings required • No speed limitation, no bandwidth limitation • One-click to connect to VPN • No root access needed • Encrypt your internet traffic • Top server speed & reliability • Using most secure VPN solution The app is free forever. There is no need to purchase if the free version is good enough for you. Purchasing VIP will remove ads, give you faster speed and better experience. You can cancel VIP at any time and get a full refund. EXPORTED COMPONENTS • Exported Activities: 0 / 20 • Exported Services: 2 / 14 • Exported Receivers: 2 / 11 • Exported Providers: 0 / 4 Application Permissions These are the permissions the app requests from Android. Most are normal network-related permissions, but a couple stand out: – It can query all installed apps. – It can post notifications. Overall, it leans heavily into advertising-related permissions. Android API Usage This shows which Android APIs the app actively uses. It uses crypto, Base64 encoding/decoding, certificate handling, dynamic class loading, OS command execution, and advertising ID access — a mix often seen in VPNs but also potentially in shady apps. Certificate Analysis The APK is signed using outdated or weak signature schemes. Two warnings appear: – Vulnerable to the Janus signing flaw. – Uses SHA-1 in parts of the signing chain, which is collision-prone. This doesn’t prove malicious intent, but it weakens trust in the app’s integrity. App Issues These are weaknesses found in the app’s configuration. It shows outdated Android support, clear-text traffic enabled, backup enabled, and several components that expose permissions incorrectly — all signs of sloppy security hygiene. Code Analysis These findings come from scanning the app’s source code. Issues include weak cryptography, insecure random numbers, logging sensitive data, hardcoded secrets, SQL injection risk, and MD5 usage. Only a couple of items are marked secure, like SSL pinning. Behaviour Analysis This lists what the app actually does during execution. It reads files, parses JSON, opens local files, connects to remote URLs, queries installed apps, uses reflection, triggers implicit intents, and retrieves SIM country data. The behaviors match a networked app with analytics and tracking capabilities. Conclusion The whole picture points to an app that works, but cuts corners everywhere that matters for security. The permissions are broad, the APIs used lean toward tracking and dynamic loading, the certificates are outdated, and the code base mixes weak crypto, insecure randomness, clear-text traffic, and sloppy logging. On top of that, several components expose permissions incorrectly, making them accessible to other apps. The behavior analysis shows it probes the device, reaches out to remote hosts, inspects installed packages, and pulls SIM data. None of these items alone guarantee malicious intent, but together they form a pattern: the app is built with minimal security discipline and maximum data access. It’s the kind of VPN product that may function on the surface but introduces more risk than it removes. A secure VPN should reduce your exposure; this one expands it. For someone studying or analyzing app security, it’s a solid example of how technical debt, weak crypto, and unnecessary permissions converge into a high-risk footprint.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "80315d2f-d7a0-4e2d-9801-5ab408a01bc9": {
    "filename": "MobSF_Assignment 3.pdf",
    "text": "Mobile Security Framework (MobSF) To perform of a mobile application (APK/IPA) using the and interpret the results. I clone Mobile Security Framework (MobSF): https://github.com/MobSF/Mobile- Security-Framework-MobSF and install and configure it with docker Login Screen of MobSF Default Username and password – mobsf:mobsf Next we upload an android apk file for analyze for that I already download a apk from the external source : SuperVPN Pro_1.9.2_APKPure.apk APP SCORES • Security Score: 52/100 • Trackers Detected: 3/32 • Button: MobiSF Scorecard FILE INFORMATION • File Name: SuperVPN_Pro_1.9.2_APKPure.apk • Size: 20.4 MB • MD5: bc2678292e0befdd1daa221558bf339 • SHA1: 760498738dfeb6d6a2064eaa68fb76fd65d614c9 • SHA256: 3965132062c85d5640fb9d02705dc85b813ca2a927892c16c7644c1b553edae APP INFORMATION • App Name: SuperVPN Pro • Package Name: com.jrzheng.supervnpayment • Main Activity: com.jrzheng.supervnpayment.activity.MainActivity • Target SDK: 34 • Min SDK: 23 • Max SDK: (blank / not listed) • Android Version Name: 1.9.2 • Android Version Code: 52 PLAYSTORE INFORMATION • Title: SuperVPN Pro • Score: 4.6 (113K reviews) • Installs: 10,000,000+ • Price: Android Version Support: Category Productivity • Play Store URL: com.jrzheng.supervnpayment • Developer: SuperSoftTech • Developer 2: SuperSoftTech • Developer Address: None • Developer Website: https://www.supervpnpro.app • Developer Email: goanalyticsgpp@gmail.com • Release Date: Apr 11, 2014 • Privacy Policy: Privacy link Description (as in screenshot) SuperVPN Pro, total free VPN service without any limitation. Easy to use, one click to connect to VPN server. Unlimited bandwidth and unlimited free time. Fast servers across the world. Features: • Protect your privacy, keep you safe from 3rd party tracking • Unblock geographically restricted websites • No registration required, no settings required • No speed limitation, no bandwidth limitation • One-click to connect to VPN • No root access needed • Encrypt your internet traffic • Top server speed & reliability • Using most secure VPN solution The app is free forever. There is no need to purchase if the free version is good enough for you. Purchasing VIP will remove ads, give you faster speed and better experience. You can cancel VIP at any time and get a full refund. EXPORTED COMPONENTS • Exported Activities: 0 / 20 • Exported Services: 2 / 14 • Exported Receivers: 2 / 11 • Exported Providers: 0 / 4 Application Permissions These are the permissions the app requests from Android. Most are normal network-related permissions, but a couple stand out: – It can query all installed apps. – It can post notifications. Overall, it leans heavily into advertising-related permissions. Android API Usage This shows which Android APIs the app actively uses. It uses crypto, Base64 encoding/decoding, certificate handling, dynamic class loading, OS command execution, and advertising ID access — a mix often seen in VPNs but also potentially in shady apps. Certificate Analysis The APK is signed using outdated or weak signature schemes. Two warnings appear: – Vulnerable to the Janus signing flaw. – Uses SHA-1 in parts of the signing chain, which is collision-prone. This doesn’t prove malicious intent, but it weakens trust in the app’s integrity. App Issues These are weaknesses found in the app’s configuration. It shows outdated Android support, clear-text traffic enabled, backup enabled, and several components that expose permissions incorrectly — all signs of sloppy security hygiene. Code Analysis These findings come from scanning the app’s source code. Issues include weak cryptography, insecure random numbers, logging sensitive data, hardcoded secrets, SQL injection risk, and MD5 usage. Only a couple of items are marked secure, like SSL pinning. Behaviour Analysis This lists what the app actually does during execution. It reads files, parses JSON, opens local files, connects to remote URLs, queries installed apps, uses reflection, triggers implicit intents, and retrieves SIM country data. The behaviors match a networked app with analytics and tracking capabilities. Conclusion The whole picture points to an app that works, but cuts corners everywhere that matters for security. The permissions are broad, the APIs used lean toward tracking and dynamic loading, the certificates are outdated, and the code base mixes weak crypto, insecure randomness, clear-text traffic, and sloppy logging. On top of that, several components expose permissions incorrectly, making them accessible to other apps. The behavior analysis shows it probes the device, reaches out to remote hosts, inspects installed packages, and pulls SIM data. None of these items alone guarantee malicious intent, but together they form a pattern: the app is built with minimal security discipline and maximum data access. It’s the kind of VPN product that may function on the surface but introduces more risk than it removes. A secure VPN should reduce your exposure; this one expands it. For someone studying or analyzing app security, it’s a solid example of how technical debt, weak crypto, and unnecessary permissions converge into a high-risk footprint.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 13
  },
  "ad05d15c-6de1-4b75-865c-077ac8a672dd": {
    "filename": "Lab Exam 1 - Static Analysis of Malware .pdf",
    "text": "LAB EXAM MODEL 1 Static Analysis of Malware The Downloaded malware file is DanaBot.exe 1. File Information: a. file type : executable b. Size : 2795520 bytes c. MD5 hash : 4D 5A 90 00 03 00 00 00 04 00 00 00 FF FF 00 00 B8 00 00 00 00 00 00 00 40 00 00 00 00 00 00 00 00 d. SHA256 hash: DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 e. creation date : Thu Mar 14 19:36:48 2019 | UTC The analyzed file, DanaBot.exe , is a 32-bit Windows executable with a size of approximately 2.79 MB. The compilation timestamp indicates it was created on March 14, 2019. The SHA256 hash, which uniquely identifies the file, is DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 Tools used: Pestudio 2. String Analysis: a. Use a string extraction tool to identify readable text. Tool Used: FLOSS 3. Categories useful strings a. URLs b. IP addresses c. Domains d. file paths e. function names f. suspicious strings g. DLL information Analysis with the FLOSS tool revealed a large number of static strings (33,058), but a conspicuous absence of decoded strings. Further inspection of the output showed no clear-text URLs, IP addresses, or domains. The strings consist mostly of standard library and function names, with no immediately obvious suspicious indicators. This lack of meaningful, human-readable indicators is a strong sign that the malware's configuration and critical data (like C2 server addresses) are obfuscated or encrypted within the binary. 4. PE Header Analysis (for PE files): a. Check whether the sample is a Portable Executable (PE). The file is confirmed to be a valid Portable Executable (PE), as identified by the \"MZ\" magic bytes at the beginning of the file. The PE header defines five sections: .text, .rdata, .data, .tls, and .rsrc. Analysis of the section properties reveals significant anomalies. The .text, .rdata, and .data sections all show a Virtual Size that is substantially larger than their Raw Size. For example, the .text section has a raw size of F800 bytes on disk but expands to F7FD bytes in memory. This discrepancy is a classic indicator of a packed executable. The malware code is likely compressed or encrypted on disk and will be unpacked into its larger, true form in memory during execution. Tools Used: PE-Bear It loads, it's a Portable Executable (PE). The first two letters you see in the hex view should be \"MZ\"—the signature of a DOS header, which is part of the PE format. 5. Analyse the PE header. Identify sections a. .text : b. .data c. .rdata d. inspect their sizes and properties. A large difference where the VirtualSize is much bigger than the SizeOfRawData can be a red flag. It often means the section is compressed or un-initialized and will be expanded in memory when the program runs, a common sign of packing. 6. Extract information about imported and exported functions. Exports Imports The malware's export table contains three non-descript functions, which is unusual for a malicious executable and likely a byproduct of its packer. The import table, however, is highly revealing. It shows the program imports numerous functions from standard Windows libraries like KERNEL32.dll and ADVAPI32.dll. Pestudio flags over 88 imported functions as suspicious. These imports provide critical clues about the malware's capabilities, including reconnaissance (GetVersionExA), memory manipulation (HeapAlloc), and system information gathering (GetVolumeNameForVolumeMountPointA). These functions will be analyzed in greater detail to identify behavioral patterns. Tools: PE-bear for structure, Pestudio for a clean view of imports/exports. 7. Obfuscation/Encryption Detection: a. Identify any signs of code obfuscation or encryption within the binary. If the file is not packed (or DiE can see through the packing), it will identify the compiler used to create the program, such as Compiler: Microsoft Visual C++. 8. Section Entropy Analysis a. Use a PE analysis tool to calculate the entropy of each section (.text, .data, .rdata, etc.). b. Interpret the entropy values to determine whether any section indicates potential packing or encryption. A high, flat line indicates high entropy, which is a strong sign of packed or encrypted data, even if DiE can't name the specific packer. A high entropy value (generally above 7.0) suggests the data is compressed or encrypted. Normal code and data have lower entropy. If you see a section with high entropy, it's likely packed. Tools: Detect It Easy (DiE), Pestudio. While Detect It Easy (DiE) identifies the compiler as Microsoft Visual C++ and does not flag a specific packer, the entropy analysis provides definitive evidence of obfuscation. The overall entropy of the file as calculated by Pestudio is 7.950, which is extremely high. An entropy value above 7.0 strongly suggests that the data is not normal code but is instead compressed or encrypted. The entropy graph from PE-bear confirms this, showing a high and uniform entropy level across the majority of the file, which is characteristic of a packed binary. This confirms that the malware's true code is hidden 10.Export Table Review ● Analyze the Export Table to check if the malware exposes functions. If yes, identify their purpose and potential use in malicious operations. 11. Section Anomalies Detection ● Check for non-standard section names, irregular section alignment, or sections with both read/write/execute permissions, which may indicate packing or code injection. The PE sections have standard names. However, a critical anomaly is found in the section permissions. Both the .data and .tls sections have characteristics that allow them to be written to and executed. A section should never be both writable and executable at the same time. This Write + Execute permission is a major red flag and a common technique used by packers. The packer writes the unpacked, malicious code into this memory region and then instructs the CPU to execute it, a clear sign of code injection or in-memory unpacking 12. Suspicious API Pattern Identification Analysis of the imported functions reveals several combinations that indicate malicious functionality. The APIs can be grouped into distinct behavioral patterns: ● Process Injection: The malware imports OpenProcess, VirtualAllocEx, WriteProcessMemory, and CreateRemoteThread. This is the complete set of functions required to inject malicious code into another running process, a core technique for stealth. ● Persistence: Imports like RegCreateKeyExA and RegSetValueExA indicate the ability to manipulate the Windows Registry. This is commonly used to create startup entries to ensure the malware runs every time the system boots. ● Anti-Analysis & Evasion: The presence of IsDebuggerPresent is a direct check to see if the malware is being analyzed. Furthermore, the use of GetModuleHandleA and GetProcAddress suggests the malware dynamically resolves function addresses to make static analysis more difficult. 13. Timestamp and Compiler Information Analysis Examine the PE header’s timestamp and compiler information. Check for anomalies such as future dates, missing timestamps, or uncommon compiler signatures. The PE header's compiler stamp indicates the file was created on Thursday, March 14, 2019. While not as overtly suspicious as a future date or a generic 1992 timestamp, this date could still be manipulated by the author. The compiler is identified as Visual Studio 2013. This information suggests the malware was developed using a standard, albeit older, development environment. The most significant finding remains the evidence of packing, rather than any specific compiler anomaly.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 19
  },
  "f4184f95-be80-4f7e-bf85-f11f7f5d26c6": {
    "filename": "Lab Exam 1 - Static Analysis of Malware .pdf",
    "text": "LAB EXAM MODEL 1 Static Analysis of Malware The Downloaded malware file is DanaBot.exe 1. File Information: a. file type : executable b. Size : 2795520 bytes c. MD5 hash : 4D 5A 90 00 03 00 00 00 04 00 00 00 FF FF 00 00 B8 00 00 00 00 00 00 00 40 00 00 00 00 00 00 00 00 d. SHA256 hash: DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 e. creation date : Thu Mar 14 19:36:48 2019 | UTC The analyzed file, DanaBot.exe , is a 32-bit Windows executable with a size of approximately 2.79 MB. The compilation timestamp indicates it was created on March 14, 2019. The SHA256 hash, which uniquely identifies the file, is DB0D72BC7D10209F7FA354EC100D57ABBB9FE2E57CE72789F5F88257C5D3EBD1 Tools used: Pestudio 2. String Analysis: a. Use a string extraction tool to identify readable text. Tool Used: FLOSS 3. Categories useful strings a. URLs b. IP addresses c. Domains d. file paths e. function names f. suspicious strings g. DLL information Analysis with the FLOSS tool revealed a large number of static strings (33,058), but a conspicuous absence of decoded strings. Further inspection of the output showed no clear-text URLs, IP addresses, or domains. The strings consist mostly of standard library and function names, with no immediately obvious suspicious indicators. This lack of meaningful, human-readable indicators is a strong sign that the malware's configuration and critical data (like C2 server addresses) are obfuscated or encrypted within the binary. 4. PE Header Analysis (for PE files): a. Check whether the sample is a Portable Executable (PE). The file is confirmed to be a valid Portable Executable (PE), as identified by the \"MZ\" magic bytes at the beginning of the file. The PE header defines five sections: .text, .rdata, .data, .tls, and .rsrc. Analysis of the section properties reveals significant anomalies. The .text, .rdata, and .data sections all show a Virtual Size that is substantially larger than their Raw Size. For example, the .text section has a raw size of F800 bytes on disk but expands to F7FD bytes in memory. This discrepancy is a classic indicator of a packed executable. The malware code is likely compressed or encrypted on disk and will be unpacked into its larger, true form in memory during execution. Tools Used: PE-Bear It loads, it's a Portable Executable (PE). The first two letters you see in the hex view should be \"MZ\"—the signature of a DOS header, which is part of the PE format. 5. Analyse the PE header. Identify sections a. .text : b. .data c. .rdata d. inspect their sizes and properties. A large difference where the VirtualSize is much bigger than the SizeOfRawData can be a red flag. It often means the section is compressed or un-initialized and will be expanded in memory when the program runs, a common sign of packing. 6. Extract information about imported and exported functions. Exports Imports The malware's export table contains three non-descript functions, which is unusual for a malicious executable and likely a byproduct of its packer. The import table, however, is highly revealing. It shows the program imports numerous functions from standard Windows libraries like KERNEL32.dll and ADVAPI32.dll. Pestudio flags over 88 imported functions as suspicious. These imports provide critical clues about the malware's capabilities, including reconnaissance (GetVersionExA), memory manipulation (HeapAlloc), and system information gathering (GetVolumeNameForVolumeMountPointA). These functions will be analyzed in greater detail to identify behavioral patterns. Tools: PE-bear for structure, Pestudio for a clean view of imports/exports. 7. Obfuscation/Encryption Detection: a. Identify any signs of code obfuscation or encryption within the binary. If the file is not packed (or DiE can see through the packing), it will identify the compiler used to create the program, such as Compiler: Microsoft Visual C++. 8. Section Entropy Analysis a. Use a PE analysis tool to calculate the entropy of each section (.text, .data, .rdata, etc.). b. Interpret the entropy values to determine whether any section indicates potential packing or encryption. A high, flat line indicates high entropy, which is a strong sign of packed or encrypted data, even if DiE can't name the specific packer. A high entropy value (generally above 7.0) suggests the data is compressed or encrypted. Normal code and data have lower entropy. If you see a section with high entropy, it's likely packed. Tools: Detect It Easy (DiE), Pestudio. While Detect It Easy (DiE) identifies the compiler as Microsoft Visual C++ and does not flag a specific packer, the entropy analysis provides definitive evidence of obfuscation. The overall entropy of the file as calculated by Pestudio is 7.950, which is extremely high. An entropy value above 7.0 strongly suggests that the data is not normal code but is instead compressed or encrypted. The entropy graph from PE-bear confirms this, showing a high and uniform entropy level across the majority of the file, which is characteristic of a packed binary. This confirms that the malware's true code is hidden 10.Export Table Review ● Analyze the Export Table to check if the malware exposes functions. If yes, identify their purpose and potential use in malicious operations. 11. Section Anomalies Detection ● Check for non-standard section names, irregular section alignment, or sections with both read/write/execute permissions, which may indicate packing or code injection. The PE sections have standard names. However, a critical anomaly is found in the section permissions. Both the .data and .tls sections have characteristics that allow them to be written to and executed. A section should never be both writable and executable at the same time. This Write + Execute permission is a major red flag and a common technique used by packers. The packer writes the unpacked, malicious code into this memory region and then instructs the CPU to execute it, a clear sign of code injection or in-memory unpacking 12. Suspicious API Pattern Identification Analysis of the imported functions reveals several combinations that indicate malicious functionality. The APIs can be grouped into distinct behavioral patterns: ● Process Injection: The malware imports OpenProcess, VirtualAllocEx, WriteProcessMemory, and CreateRemoteThread. This is the complete set of functions required to inject malicious code into another running process, a core technique for stealth. ● Persistence: Imports like RegCreateKeyExA and RegSetValueExA indicate the ability to manipulate the Windows Registry. This is commonly used to create startup entries to ensure the malware runs every time the system boots. ● Anti-Analysis & Evasion: The presence of IsDebuggerPresent is a direct check to see if the malware is being analyzed. Furthermore, the use of GetModuleHandleA and GetProcAddress suggests the malware dynamically resolves function addresses to make static analysis more difficult. 13. Timestamp and Compiler Information Analysis Examine the PE header’s timestamp and compiler information. Check for anomalies such as future dates, missing timestamps, or uncommon compiler signatures. The PE header's compiler stamp indicates the file was created on Thursday, March 14, 2019. While not as overtly suspicious as a future date or a generic 1992 timestamp, this date could still be manipulated by the author. The compiler is identified as Visual Studio 2013. This information suggests the malware was developed using a standard, albeit older, development environment. The most significant finding remains the evidence of packing, rather than any specific compiler anomaly.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 19
  },
  "61aa5ed0-c336-4a33-8a34-eef2f8b344dc": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "6f71da35-5fdc-4ca8-a805-7fa9ba1dd99d": {
    "filename": "Lecture 12.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 12 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Unsupervised Learning Introduction • Regression and Classification problems belong to a class of problems called Supervised Learning Problems. • In both regression and classification, the historical or training data provided to build a predictive model includes data on the variable to be predicted, i.e., the outcome or target variables. • When the data is provided on the variable to be predicted then the problem is a Supervised Learning problem. • Unsupervised learning problems are those where the dataset does not have any target or outcome variables. If there is no data on what needs to be predicted, then what can such algorithms do? • Suppose a customer has selected a few products to purchase. • We would like to provide suggestions about other products that he should or could be buying. • The data we have is historical data of what customers have bought. • In the historical data, if we find repeat patterns of milk being bought with bread, and • If customer who is currently shopping has bought milk, then our algorithm can suggest buying bread. • We can not say that if milk is the value of a feature , then bread is the value to be predicted because vice versa, i.e., if the customer has bought bread, he could buy milk. Contd… • So, the role of feature and predictor quickly gets changed. • Rather, in this case, we have patterns in our historical data, and we define rules saying that if certain patterns occur very often, then they can be used for making certain recommendations. • This problem formulation goes by the name of Associate Rule Mining and is an Unsupervised Learning problem. • We do not discuss associate rule mining in this course • Rules in Associate rule mining are automatically inferred from the data (i.e machine learned) and are not created manually by human beings. Contd.. • In unsupervised learning , the models still perform predictions. • But there is no prior data on the outcomes. • We need to define the outcome or prediction we desire and accordingly come up with an algorithm for doing so. Clustering • Clustering is an unsupervised machine learning technique that groups similar data points together into clusters based on their characteristics, without using any labeled data. • The objective is to ensure that data points within the same cluster are more similar to each other than to those in different clusters, enabling the discovery of natural groupings and hidden patterns in complex datasets. • Goal: Discover the natural grouping or structure in unlabeled data without predefined categories. • How: Data points are assigned to clusters based on similarity or distance measures. • Similarity Measures: Can include Euclidean distance, cosine similarity or other metrics depending on data type and clustering method. • Output: Each group is assigned a cluster ID, representing shared characteristics within the cluster. • For example, if we have customer purchase data, clustering can group customers with similar shopping habits. • These clusters can then be used for targeted marketing, personalized recommendations or customer segmentation. Types of Clustering • Hard Clustering: In hard clustering, each data point strictly belongs to exactly one cluster, no overlap is allowed. This approach assigns a clear membership, making it easier to interpret and use for definitive segmentation tasks. • Example: If clustering customer data into 2 segments, each customer belongs fully to either Cluster 1 or Cluster 2 without partial memberships. • Use cases: Market segmentation, customer grouping, document clustering. • Limitations: Cannot represent ambiguity or overlap between groups; boundaries are crisp. • Soft Clustering: Soft clustering assigns each data point a probability or degree of membership to multiple clusters simultaneously, allowing data points to partially belong to several groups. • Example: A data point may have a 70% membership in Cluster 1 and 30% in Cluster 2, reflecting uncertainty or overlap in group characteristics. • Use cases: Situations with overlapping class boundaries, fuzzy categories like customer personas or medical diagnosis. • Benefits: Captures ambiguity in data, models gradual transitions between clusters. Types of Clustering Methods • Clustering methods can be classified on the basis of how they for clusters, 1. Centroid-based Clustering (Partitioning Methods) • Centroid-based clustering organizes data points around central prototypes called centroids, where each cluster is represented by the mean (or medoid) of its members. • The number of clusters is specified in advance and the algorithm allocates points to the nearest centroid, making this technique efficient for spherical and similarly sized clusters but sensitive to outliers and initialization. Algorithms • K-means: Iteratively assigns points to nearest centroid and recalculates centroids to minimize intra-cluster variance. • K-medoids : Similar to K-means but uses actual data points (medoids) as centers, robust to outliers. Pros: • Fast and scalable for large datasets. • Simple to implement and interpret. Cons: • Requires pre-knowledge of k. • Sensitive to initialization and outliers. • Not suitable for non-spherical clusters. Density-based Clustering (Model-based Methods) • Density-based clustering defines clusters as contiguous regions of high data density separated by areas of lower density. • This approach can identify clusters of arbitrary shapes, handles noise well and does not require predefining the number of clusters, though its effectiveness depends on chosen density parameters. Algorithms • DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points with sufficient neighbors; labels sparse points as noise. • OPTICS (Ordering Points To Identify Clustering Structure): Extends DBSCAN to handle varying densities. Pros/Cons Pros: • Handles clusters of varying shapes and sizes. • Does not require cluster count upfront. • Effective in noisy datasets. Cons: • Difficult to choose parameters like epsilon and min points. • Less effective for varying density clusters (except OPTICS). Connectivity-based Clustering (Hierarchical Clustering) • Connectivity-based (or hierarchical) clustering builds nested groupings of data by evaluating how data points are connected to their neighbors. • It creates a dendrogram—a tree-like structure—that reflects relationships at various granularity levels and does not require specifying cluster numbers in advance, but can be computationally intensive. Approaches: • Agglomerative (Bottom-up): Start with each point as a cluster; iteratively merge closest clusters. • Divisive (Top-down): Start with one cluster; iteratively split into smaller clusters. Pros • Provides a full hierarchy, easy to visualize. • No need to specify number of clusters upfront. Cons • Computationally intensive for large datasets. • Merging/splitting decisions are irreversible. Distribution-based Clustering • Distribution-based clustering assumes data is generated from a mixture of probability distributions, such as Gaussian distributions and assigns points to clusters based on statistical likelihood. • This method supports clusters with flexible shapes and overlaps, but usually requires specifying the number of distributions. Algorithm: • Gaussian Mixture Model (GMM): Fits data as a weighted mixture of Gaussian distributions; assigns data points based on likelihood. Pros: • Flexible cluster shapes. • Provides probabilistic memberships. • Suitable for overlapping clusters. Cons: • Requires specifying number of components. • Computationally more expensive. • Sensitive to initialization. Fuzzy Clustering • Fuzzy clustering extends traditional methods by allowing each data point to belong to multiple clusters with varying degrees of membership. • This approach captures ambiguity and soft boundaries in data and is particularly useful when the clusters overlap or boundaries are not clear-cut. Algorithm: • Fuzzy C-Means: Similar to K-means but with fuzzy memberships updated iteratively. Pros: • Models data ambiguity explicitly. • Useful for complex or imprecise data. Cons: • Choosing fuzziness parameter can be tricky. • Computational overhead compared to hard clustering. Use Cases • Customer Segmentation: Grouping customers based on behavior or demographics for targeted marketing and personalized services. • Anomaly Detection: Identifying outliers or fraudulent activities in finance, network security and sensor data. • Image Segmentation: Dividing images into meaningful parts for object detection, medical diagnostics or computer vision tasks. • Recommendation Systems: Clustering user preferences to recommend movies, products or content tailored to different groups. • Market Basket Analysis: Discovering products frequently bought together to optimize store layouts and promotions. K means Clustering • K-Means Clustering is an unsupervised machine learning algorithm that helps group data points into clusters based on their inherent similarity. • Unlike supervised learning, where we train models using labeled data, K-Means is used when we have data that is not labeled and the goal is to uncover hidden patterns or structures. • For example, an online store can use K-Means to segment customers into groups like \"Budget Shoppers,\" \"Frequent Buyers,\" and \"Big Spenders\" based on their purchase history Working of K-Means Clustering • Suppose we are given a data set of items with certain features and values for these features like a vector. • The task is to categorize those items into groups. • To achieve this, we will use the K-means algorithm. 𝑘 • \" \" represents the number of groups or clusters we want to classify our items into. K-Means Clustering Algorithm • The algorithm will categorize the items into \"𝑘 \" groups or clusters of similarity. • To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows: 1. Initialization: We begin by randomly selecting k cluster centroids. 2. Assignment Step: Each data point is assigned to the nearest centroid, forming clusters. 3. Update Step: After the assignment, we recalculate the centroid of each cluster by averaging the points within it. 4. Repeat: This process repeats until the centroids no longer change or the maximum number of iterations is reached. • The goal is to partition the dataset into 𝑘 clusters such that data points within each cluster are more similar to each other than to those in other clusters. Euclidean distance Elbow Method for optimal value of k in KMeans • Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means. • However, deciding the ideal k is not straightforward. • The Elbow Method helps by plotting the Within-Cluster Sum of Squares (WCSS) against increasing k values and looking for a point where the improvement slows down, this point is called the \"elbow.\" Working of Elbow Point The Elbow Method works in the following steps: 1) We iterate over a range of k values, typically from 1 to n (where n is a hyperparameter you choose). 2) For each k, we calculate a distance measure called WCSS (Within-Cluster Sum of Squares). • This tells us how spread out the data points are within each cluster. • WCSS measures how well the data points are clustered around their respective centroids. 3) We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS. 4) We plot a graph with k on the X-axis and WCSS on the Y-axis. 5) As we increase k, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph. Within-Cluster Sum of Squares(WCSS) 𝑘 𝑘 • Where (𝑥 , 𝑦 ) is the centroid of the points in the cluster 𝐶 , and | 𝐶 | is 𝑘 𝑘 the number of points in the cluster. • WCSS is essentially the sum of the square of the distances between the points in the cluster and its centroid. • It should be obvious that if the points are tightly clustered ,then WCSS will be a small number and vice versa. • Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability. • After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting. • The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. • This \"elbow\" point suggests the optimal number of clusters. Inertia • Inertia simply as the sum of the WCSs values for a group of clusters. • If there are K clusters ,then inertia of the clusters is given by, • Given K ,the number of clusters and n points , there are many ways to cluster the n points into K clusters. • The K-means algorithm creates K-Clusters such that the sum of the WCSS values for the K-clusters is minimal, i.e., it tries to create K clusters such that the inertia of the K clusters created is less than the inertia of clustering of the n points into some other K clusters. • It should be mentioned at the onset that K-means is based on heuristics and does not guarantee a clustering with minimum inertia. • It merely finds a clustering with low inertia • Lower Inertia suggests better clustering Why Use K-Means Clustering? K-Means is popular in a wide variety of applications due to its simplicity, efficiency and effectiveness. Data Segmentation: One of the most common uses of K-Means is segmenting data into distinct groups. For example, businesses use K-Means to group customers based on behavior, such as purchasing patterns or website interaction. Image Compression: K-Means can be used to reduce the complexity of images by grouping similar pixels into clusters, effectively compressing the image. This is useful for image storage and processing. Anomaly Detection: K-Means can be applied to detect anomalies or outliers by identifying data points that do not belong to any of the clusters. Document Clustering: In natural language processing (NLP), K-Means is used to group similar documents or articles together. It’s often used in applications like recommendation systems or news categorization. Organizing Large Datasets: When dealing with large datasets, K-Means can help in organizing the data into smaller, more manageable chunks based on similarities, improving the efficiency of data analysis. Challenges with K-Means Clustering • Choosing the Right Number of Clusters (𝑘 ): One of the biggest challenges is deciding how many clusters to use. • Sensitive to Initial Centroids: The final clusters can vary depending on the initial random placement of centroids. • Non-Spherical Clusters: K-Means assumes that the clusters are spherical and equally sized. This can be a problem when the actual clusters in the data are of different shapes or densities. • Outliers: K-Means is sensitive to outliers, which can distort the centroid and, ultimately, the clusters.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 39
  },
  "81c599dc-39ab-4bd5-b602-89f04f02625c": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "f5a27217-2c68-42de-94e8-7f6ba6ccfb9e": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "82dcd426-66c8-46b3-ba25-d0c8d16f8676": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "05aed1a5-9a3b-4963-956e-186af991b026": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "48ae5370-d72b-4d97-b961-eb6f6fb8fc3d": {
    "filename": "Lecture 12.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 12 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Unsupervised Learning Introduction • Regression and Classification problems belong to a class of problems called Supervised Learning Problems. • In both regression and classification, the historical or training data provided to build a predictive model includes data on the variable to be predicted, i.e., the outcome or target variables. • When the data is provided on the variable to be predicted then the problem is a Supervised Learning problem. • Unsupervised learning problems are those where the dataset does not have any target or outcome variables. If there is no data on what needs to be predicted, then what can such algorithms do? • Suppose a customer has selected a few products to purchase. • We would like to provide suggestions about other products that he should or could be buying. • The data we have is historical data of what customers have bought. • In the historical data, if we find repeat patterns of milk being bought with bread, and • If customer who is currently shopping has bought milk, then our algorithm can suggest buying bread. • We can not say that if milk is the value of a feature , then bread is the value to be predicted because vice versa, i.e., if the customer has bought bread, he could buy milk. Contd… • So, the role of feature and predictor quickly gets changed. • Rather, in this case, we have patterns in our historical data, and we define rules saying that if certain patterns occur very often, then they can be used for making certain recommendations. • This problem formulation goes by the name of Associate Rule Mining and is an Unsupervised Learning problem. • We do not discuss associate rule mining in this course • Rules in Associate rule mining are automatically inferred from the data (i.e machine learned) and are not created manually by human beings. Contd.. • In unsupervised learning , the models still perform predictions. • But there is no prior data on the outcomes. • We need to define the outcome or prediction we desire and accordingly come up with an algorithm for doing so. Clustering • Clustering is an unsupervised machine learning technique that groups similar data points together into clusters based on their characteristics, without using any labeled data. • The objective is to ensure that data points within the same cluster are more similar to each other than to those in different clusters, enabling the discovery of natural groupings and hidden patterns in complex datasets. • Goal: Discover the natural grouping or structure in unlabeled data without predefined categories. • How: Data points are assigned to clusters based on similarity or distance measures. • Similarity Measures: Can include Euclidean distance, cosine similarity or other metrics depending on data type and clustering method. • Output: Each group is assigned a cluster ID, representing shared characteristics within the cluster. • For example, if we have customer purchase data, clustering can group customers with similar shopping habits. • These clusters can then be used for targeted marketing, personalized recommendations or customer segmentation. Types of Clustering • Hard Clustering: In hard clustering, each data point strictly belongs to exactly one cluster, no overlap is allowed. This approach assigns a clear membership, making it easier to interpret and use for definitive segmentation tasks. • Example: If clustering customer data into 2 segments, each customer belongs fully to either Cluster 1 or Cluster 2 without partial memberships. • Use cases: Market segmentation, customer grouping, document clustering. • Limitations: Cannot represent ambiguity or overlap between groups; boundaries are crisp. • Soft Clustering: Soft clustering assigns each data point a probability or degree of membership to multiple clusters simultaneously, allowing data points to partially belong to several groups. • Example: A data point may have a 70% membership in Cluster 1 and 30% in Cluster 2, reflecting uncertainty or overlap in group characteristics. • Use cases: Situations with overlapping class boundaries, fuzzy categories like customer personas or medical diagnosis. • Benefits: Captures ambiguity in data, models gradual transitions between clusters. Types of Clustering Methods • Clustering methods can be classified on the basis of how they for clusters, 1. Centroid-based Clustering (Partitioning Methods) • Centroid-based clustering organizes data points around central prototypes called centroids, where each cluster is represented by the mean (or medoid) of its members. • The number of clusters is specified in advance and the algorithm allocates points to the nearest centroid, making this technique efficient for spherical and similarly sized clusters but sensitive to outliers and initialization. Algorithms • K-means: Iteratively assigns points to nearest centroid and recalculates centroids to minimize intra-cluster variance. • K-medoids : Similar to K-means but uses actual data points (medoids) as centers, robust to outliers. Pros: • Fast and scalable for large datasets. • Simple to implement and interpret. Cons: • Requires pre-knowledge of k. • Sensitive to initialization and outliers. • Not suitable for non-spherical clusters. Density-based Clustering (Model-based Methods) • Density-based clustering defines clusters as contiguous regions of high data density separated by areas of lower density. • This approach can identify clusters of arbitrary shapes, handles noise well and does not require predefining the number of clusters, though its effectiveness depends on chosen density parameters. Algorithms • DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points with sufficient neighbors; labels sparse points as noise. • OPTICS (Ordering Points To Identify Clustering Structure): Extends DBSCAN to handle varying densities. Pros/Cons Pros: • Handles clusters of varying shapes and sizes. • Does not require cluster count upfront. • Effective in noisy datasets. Cons: • Difficult to choose parameters like epsilon and min points. • Less effective for varying density clusters (except OPTICS). Connectivity-based Clustering (Hierarchical Clustering) • Connectivity-based (or hierarchical) clustering builds nested groupings of data by evaluating how data points are connected to their neighbors. • It creates a dendrogram—a tree-like structure—that reflects relationships at various granularity levels and does not require specifying cluster numbers in advance, but can be computationally intensive. Approaches: • Agglomerative (Bottom-up): Start with each point as a cluster; iteratively merge closest clusters. • Divisive (Top-down): Start with one cluster; iteratively split into smaller clusters. Pros • Provides a full hierarchy, easy to visualize. • No need to specify number of clusters upfront. Cons • Computationally intensive for large datasets. • Merging/splitting decisions are irreversible. Distribution-based Clustering • Distribution-based clustering assumes data is generated from a mixture of probability distributions, such as Gaussian distributions and assigns points to clusters based on statistical likelihood. • This method supports clusters with flexible shapes and overlaps, but usually requires specifying the number of distributions. Algorithm: • Gaussian Mixture Model (GMM): Fits data as a weighted mixture of Gaussian distributions; assigns data points based on likelihood. Pros: • Flexible cluster shapes. • Provides probabilistic memberships. • Suitable for overlapping clusters. Cons: • Requires specifying number of components. • Computationally more expensive. • Sensitive to initialization. Fuzzy Clustering • Fuzzy clustering extends traditional methods by allowing each data point to belong to multiple clusters with varying degrees of membership. • This approach captures ambiguity and soft boundaries in data and is particularly useful when the clusters overlap or boundaries are not clear-cut. Algorithm: • Fuzzy C-Means: Similar to K-means but with fuzzy memberships updated iteratively. Pros: • Models data ambiguity explicitly. • Useful for complex or imprecise data. Cons: • Choosing fuzziness parameter can be tricky. • Computational overhead compared to hard clustering. Use Cases • Customer Segmentation: Grouping customers based on behavior or demographics for targeted marketing and personalized services. • Anomaly Detection: Identifying outliers or fraudulent activities in finance, network security and sensor data. • Image Segmentation: Dividing images into meaningful parts for object detection, medical diagnostics or computer vision tasks. • Recommendation Systems: Clustering user preferences to recommend movies, products or content tailored to different groups. • Market Basket Analysis: Discovering products frequently bought together to optimize store layouts and promotions. K means Clustering • K-Means Clustering is an unsupervised machine learning algorithm that helps group data points into clusters based on their inherent similarity. • Unlike supervised learning, where we train models using labeled data, K-Means is used when we have data that is not labeled and the goal is to uncover hidden patterns or structures. • For example, an online store can use K-Means to segment customers into groups like \"Budget Shoppers,\" \"Frequent Buyers,\" and \"Big Spenders\" based on their purchase history Working of K-Means Clustering • Suppose we are given a data set of items with certain features and values for these features like a vector. • The task is to categorize those items into groups. • To achieve this, we will use the K-means algorithm. 𝑘 • \" \" represents the number of groups or clusters we want to classify our items into. K-Means Clustering Algorithm • The algorithm will categorize the items into \"𝑘 \" groups or clusters of similarity. • To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows: 1. Initialization: We begin by randomly selecting k cluster centroids. 2. Assignment Step: Each data point is assigned to the nearest centroid, forming clusters. 3. Update Step: After the assignment, we recalculate the centroid of each cluster by averaging the points within it. 4. Repeat: This process repeats until the centroids no longer change or the maximum number of iterations is reached. • The goal is to partition the dataset into 𝑘 clusters such that data points within each cluster are more similar to each other than to those in other clusters. Euclidean distance Elbow Method for optimal value of k in KMeans • Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means. • However, deciding the ideal k is not straightforward. • The Elbow Method helps by plotting the Within-Cluster Sum of Squares (WCSS) against increasing k values and looking for a point where the improvement slows down, this point is called the \"elbow.\" Working of Elbow Point The Elbow Method works in the following steps: 1) We iterate over a range of k values, typically from 1 to n (where n is a hyperparameter you choose). 2) For each k, we calculate a distance measure called WCSS (Within-Cluster Sum of Squares). • This tells us how spread out the data points are within each cluster. • WCSS measures how well the data points are clustered around their respective centroids. 3) We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS. 4) We plot a graph with k on the X-axis and WCSS on the Y-axis. 5) As we increase k, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph. Within-Cluster Sum of Squares(WCSS) 𝑘 𝑘 • Where (𝑥 , 𝑦 ) is the centroid of the points in the cluster 𝐶 , and | 𝐶 | is 𝑘 𝑘 the number of points in the cluster. • WCSS is essentially the sum of the square of the distances between the points in the cluster and its centroid. • It should be obvious that if the points are tightly clustered ,then WCSS will be a small number and vice versa. • Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability. • After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting. • The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. • This \"elbow\" point suggests the optimal number of clusters. Inertia • Inertia simply as the sum of the WCSs values for a group of clusters. • If there are K clusters ,then inertia of the clusters is given by, • Given K ,the number of clusters and n points , there are many ways to cluster the n points into K clusters. • The K-means algorithm creates K-Clusters such that the sum of the WCSS values for the K-clusters is minimal, i.e., it tries to create K clusters such that the inertia of the K clusters created is less than the inertia of clustering of the n points into some other K clusters. • It should be mentioned at the onset that K-means is based on heuristics and does not guarantee a clustering with minimum inertia. • It merely finds a clustering with low inertia • Lower Inertia suggests better clustering Why Use K-Means Clustering? K-Means is popular in a wide variety of applications due to its simplicity, efficiency and effectiveness. Data Segmentation: One of the most common uses of K-Means is segmenting data into distinct groups. For example, businesses use K-Means to group customers based on behavior, such as purchasing patterns or website interaction. Image Compression: K-Means can be used to reduce the complexity of images by grouping similar pixels into clusters, effectively compressing the image. This is useful for image storage and processing. Anomaly Detection: K-Means can be applied to detect anomalies or outliers by identifying data points that do not belong to any of the clusters. Document Clustering: In natural language processing (NLP), K-Means is used to group similar documents or articles together. It’s often used in applications like recommendation systems or news categorization. Organizing Large Datasets: When dealing with large datasets, K-Means can help in organizing the data into smaller, more manageable chunks based on similarities, improving the efficiency of data analysis. Challenges with K-Means Clustering • Choosing the Right Number of Clusters (𝑘 ): One of the biggest challenges is deciding how many clusters to use. • Sensitive to Initial Centroids: The final clusters can vary depending on the initial random placement of centroids. • Non-Spherical Clusters: K-Means assumes that the clusters are spherical and equally sized. This can be a problem when the actual clusters in the data are of different shapes or densities. • Outliers: K-Means is sensitive to outliers, which can distort the centroid and, ultimately, the clusters.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 39
  },
  "51ae9041-e41f-41e5-94b5-2ea635ecfec4": {
    "filename": "Lecture 12.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 12 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Unsupervised Learning Introduction • Regression and Classification problems belong to a class of problems called Supervised Learning Problems. • In both regression and classification, the historical or training data provided to build a predictive model includes data on the variable to be predicted, i.e., the outcome or target variables. • When the data is provided on the variable to be predicted then the problem is a Supervised Learning problem. • Unsupervised learning problems are those where the dataset does not have any target or outcome variables. If there is no data on what needs to be predicted, then what can such algorithms do? • Suppose a customer has selected a few products to purchase. • We would like to provide suggestions about other products that he should or could be buying. • The data we have is historical data of what customers have bought. • In the historical data, if we find repeat patterns of milk being bought with bread, and • If customer who is currently shopping has bought milk, then our algorithm can suggest buying bread. • We can not say that if milk is the value of a feature , then bread is the value to be predicted because vice versa, i.e., if the customer has bought bread, he could buy milk. Contd… • So, the role of feature and predictor quickly gets changed. • Rather, in this case, we have patterns in our historical data, and we define rules saying that if certain patterns occur very often, then they can be used for making certain recommendations. • This problem formulation goes by the name of Associate Rule Mining and is an Unsupervised Learning problem. • We do not discuss associate rule mining in this course • Rules in Associate rule mining are automatically inferred from the data (i.e machine learned) and are not created manually by human beings. Contd.. • In unsupervised learning , the models still perform predictions. • But there is no prior data on the outcomes. • We need to define the outcome or prediction we desire and accordingly come up with an algorithm for doing so. Clustering • Clustering is an unsupervised machine learning technique that groups similar data points together into clusters based on their characteristics, without using any labeled data. • The objective is to ensure that data points within the same cluster are more similar to each other than to those in different clusters, enabling the discovery of natural groupings and hidden patterns in complex datasets. • Goal: Discover the natural grouping or structure in unlabeled data without predefined categories. • How: Data points are assigned to clusters based on similarity or distance measures. • Similarity Measures: Can include Euclidean distance, cosine similarity or other metrics depending on data type and clustering method. • Output: Each group is assigned a cluster ID, representing shared characteristics within the cluster. • For example, if we have customer purchase data, clustering can group customers with similar shopping habits. • These clusters can then be used for targeted marketing, personalized recommendations or customer segmentation. Types of Clustering • Hard Clustering: In hard clustering, each data point strictly belongs to exactly one cluster, no overlap is allowed. This approach assigns a clear membership, making it easier to interpret and use for definitive segmentation tasks. • Example: If clustering customer data into 2 segments, each customer belongs fully to either Cluster 1 or Cluster 2 without partial memberships. • Use cases: Market segmentation, customer grouping, document clustering. • Limitations: Cannot represent ambiguity or overlap between groups; boundaries are crisp. • Soft Clustering: Soft clustering assigns each data point a probability or degree of membership to multiple clusters simultaneously, allowing data points to partially belong to several groups. • Example: A data point may have a 70% membership in Cluster 1 and 30% in Cluster 2, reflecting uncertainty or overlap in group characteristics. • Use cases: Situations with overlapping class boundaries, fuzzy categories like customer personas or medical diagnosis. • Benefits: Captures ambiguity in data, models gradual transitions between clusters. Types of Clustering Methods • Clustering methods can be classified on the basis of how they for clusters, 1. Centroid-based Clustering (Partitioning Methods) • Centroid-based clustering organizes data points around central prototypes called centroids, where each cluster is represented by the mean (or medoid) of its members. • The number of clusters is specified in advance and the algorithm allocates points to the nearest centroid, making this technique efficient for spherical and similarly sized clusters but sensitive to outliers and initialization. Algorithms • K-means: Iteratively assigns points to nearest centroid and recalculates centroids to minimize intra-cluster variance. • K-medoids : Similar to K-means but uses actual data points (medoids) as centers, robust to outliers. Pros: • Fast and scalable for large datasets. • Simple to implement and interpret. Cons: • Requires pre-knowledge of k. • Sensitive to initialization and outliers. • Not suitable for non-spherical clusters. Density-based Clustering (Model-based Methods) • Density-based clustering defines clusters as contiguous regions of high data density separated by areas of lower density. • This approach can identify clusters of arbitrary shapes, handles noise well and does not require predefining the number of clusters, though its effectiveness depends on chosen density parameters. Algorithms • DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points with sufficient neighbors; labels sparse points as noise. • OPTICS (Ordering Points To Identify Clustering Structure): Extends DBSCAN to handle varying densities. Pros/Cons Pros: • Handles clusters of varying shapes and sizes. • Does not require cluster count upfront. • Effective in noisy datasets. Cons: • Difficult to choose parameters like epsilon and min points. • Less effective for varying density clusters (except OPTICS). Connectivity-based Clustering (Hierarchical Clustering) • Connectivity-based (or hierarchical) clustering builds nested groupings of data by evaluating how data points are connected to their neighbors. • It creates a dendrogram—a tree-like structure—that reflects relationships at various granularity levels and does not require specifying cluster numbers in advance, but can be computationally intensive. Approaches: • Agglomerative (Bottom-up): Start with each point as a cluster; iteratively merge closest clusters. • Divisive (Top-down): Start with one cluster; iteratively split into smaller clusters. Pros • Provides a full hierarchy, easy to visualize. • No need to specify number of clusters upfront. Cons • Computationally intensive for large datasets. • Merging/splitting decisions are irreversible. Distribution-based Clustering • Distribution-based clustering assumes data is generated from a mixture of probability distributions, such as Gaussian distributions and assigns points to clusters based on statistical likelihood. • This method supports clusters with flexible shapes and overlaps, but usually requires specifying the number of distributions. Algorithm: • Gaussian Mixture Model (GMM): Fits data as a weighted mixture of Gaussian distributions; assigns data points based on likelihood. Pros: • Flexible cluster shapes. • Provides probabilistic memberships. • Suitable for overlapping clusters. Cons: • Requires specifying number of components. • Computationally more expensive. • Sensitive to initialization. Fuzzy Clustering • Fuzzy clustering extends traditional methods by allowing each data point to belong to multiple clusters with varying degrees of membership. • This approach captures ambiguity and soft boundaries in data and is particularly useful when the clusters overlap or boundaries are not clear-cut. Algorithm: • Fuzzy C-Means: Similar to K-means but with fuzzy memberships updated iteratively. Pros: • Models data ambiguity explicitly. • Useful for complex or imprecise data. Cons: • Choosing fuzziness parameter can be tricky. • Computational overhead compared to hard clustering. Use Cases • Customer Segmentation: Grouping customers based on behavior or demographics for targeted marketing and personalized services. • Anomaly Detection: Identifying outliers or fraudulent activities in finance, network security and sensor data. • Image Segmentation: Dividing images into meaningful parts for object detection, medical diagnostics or computer vision tasks. • Recommendation Systems: Clustering user preferences to recommend movies, products or content tailored to different groups. • Market Basket Analysis: Discovering products frequently bought together to optimize store layouts and promotions. K means Clustering • K-Means Clustering is an unsupervised machine learning algorithm that helps group data points into clusters based on their inherent similarity. • Unlike supervised learning, where we train models using labeled data, K-Means is used when we have data that is not labeled and the goal is to uncover hidden patterns or structures. • For example, an online store can use K-Means to segment customers into groups like \"Budget Shoppers,\" \"Frequent Buyers,\" and \"Big Spenders\" based on their purchase history Working of K-Means Clustering • Suppose we are given a data set of items with certain features and values for these features like a vector. • The task is to categorize those items into groups. • To achieve this, we will use the K-means algorithm. 𝑘 • \" \" represents the number of groups or clusters we want to classify our items into. K-Means Clustering Algorithm • The algorithm will categorize the items into \"𝑘 \" groups or clusters of similarity. • To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows: 1. Initialization: We begin by randomly selecting k cluster centroids. 2. Assignment Step: Each data point is assigned to the nearest centroid, forming clusters. 3. Update Step: After the assignment, we recalculate the centroid of each cluster by averaging the points within it. 4. Repeat: This process repeats until the centroids no longer change or the maximum number of iterations is reached. • The goal is to partition the dataset into 𝑘 clusters such that data points within each cluster are more similar to each other than to those in other clusters. Euclidean distance Elbow Method for optimal value of k in KMeans • Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means. • However, deciding the ideal k is not straightforward. • The Elbow Method helps by plotting the Within-Cluster Sum of Squares (WCSS) against increasing k values and looking for a point where the improvement slows down, this point is called the \"elbow.\" Working of Elbow Point The Elbow Method works in the following steps: 1) We iterate over a range of k values, typically from 1 to n (where n is a hyperparameter you choose). 2) For each k, we calculate a distance measure called WCSS (Within-Cluster Sum of Squares). • This tells us how spread out the data points are within each cluster. • WCSS measures how well the data points are clustered around their respective centroids. 3) We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS. 4) We plot a graph with k on the X-axis and WCSS on the Y-axis. 5) As we increase k, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph. Within-Cluster Sum of Squares(WCSS) 𝑘 𝑘 • Where (𝑥 , 𝑦 ) is the centroid of the points in the cluster 𝐶 , and | 𝐶 | is 𝑘 𝑘 the number of points in the cluster. • WCSS is essentially the sum of the square of the distances between the points in the cluster and its centroid. • It should be obvious that if the points are tightly clustered ,then WCSS will be a small number and vice versa. • Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability. • After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting. • The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. • This \"elbow\" point suggests the optimal number of clusters. Inertia • Inertia simply as the sum of the WCSs values for a group of clusters. • If there are K clusters ,then inertia of the clusters is given by, • Given K ,the number of clusters and n points , there are many ways to cluster the n points into K clusters. • The K-means algorithm creates K-Clusters such that the sum of the WCSS values for the K-clusters is minimal, i.e., it tries to create K clusters such that the inertia of the K clusters created is less than the inertia of clustering of the n points into some other K clusters. • It should be mentioned at the onset that K-means is based on heuristics and does not guarantee a clustering with minimum inertia. • It merely finds a clustering with low inertia • Lower Inertia suggests better clustering Why Use K-Means Clustering? K-Means is popular in a wide variety of applications due to its simplicity, efficiency and effectiveness. Data Segmentation: One of the most common uses of K-Means is segmenting data into distinct groups. For example, businesses use K-Means to group customers based on behavior, such as purchasing patterns or website interaction. Image Compression: K-Means can be used to reduce the complexity of images by grouping similar pixels into clusters, effectively compressing the image. This is useful for image storage and processing. Anomaly Detection: K-Means can be applied to detect anomalies or outliers by identifying data points that do not belong to any of the clusters. Document Clustering: In natural language processing (NLP), K-Means is used to group similar documents or articles together. It’s often used in applications like recommendation systems or news categorization. Organizing Large Datasets: When dealing with large datasets, K-Means can help in organizing the data into smaller, more manageable chunks based on similarities, improving the efficiency of data analysis. Challenges with K-Means Clustering • Choosing the Right Number of Clusters (𝑘 ): One of the biggest challenges is deciding how many clusters to use. • Sensitive to Initial Centroids: The final clusters can vary depending on the initial random placement of centroids. • Non-Spherical Clusters: K-Means assumes that the clusters are spherical and equally sized. This can be a problem when the actual clusters in the data are of different shapes or densities. • Outliers: K-Means is sensitive to outliers, which can distort the centroid and, ultimately, the clusters.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 39
  },
  "a5aa2397-f0d9-445c-ac46-797f7e357a52": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "2c1b1309-56ce-4f42-b3ab-f3614ffb3ed2": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "00e60013-b499-414d-a5e4-8d676fa9180f": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "3d96d4fe-d3ef-40a5-8da3-9f30bf41de4f": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "5f628564-bd78-4784-84ac-ff138b58de37": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "dd738292-9192-4f13-a1df-cd4002135e5d": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "b8fedb40-7804-4622-99fe-f66037fa6312": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "bfad01d0-e7e2-41a1-bf3c-626d85695a38": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "c26da295-d69c-482e-8ce5-664a9632d4a8": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "74271c0c-9082-4f27-9149-887262bf896c": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "4e3bf599-a588-4a9a-beae-79a58a2e3f83": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "fc73801a-8607-4bfd-8779-1d20a9d6d26c": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "963568d1-068b-448b-828e-f27f3590192b": {
    "filename": "Lecture 13-1.pdf",
    "text": "AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. Contd… Once the standardization is done, all the variables will be transformed to the same scale. STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). Scatter plot Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 29
  },
  "58ce5870-fb40-40ac-a563-40b667fd6708": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "62f3ffc5-3137-4650-960b-ac5715ecc9dc": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "cc7d1ab3-6521-4241-9f40-84f8bf4284ab": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "1f1b76e1-2ac8-424c-b672-0feccf27b969": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "12c5aa9d-22ce-45c5-b736-d0b18b5acf5f": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "dc35d848-011c-44ab-ae39-e52e92ee4109": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  },
  "e93aa26c-d324-4346-a6bf-cfa94a0c86ae": {
    "filename": "Lecture 13-1.pdf",
    "text": "<<PAGE_1>> AI for Cyber Security M5010018/M5020018 Lecture 13 School of Computer Science and Engineering (SoCSE) Digital University Kerala ,Thiruvananthapuram <<PAGE_2>> Principal Components Analysis(PCA) • Principal Component Analysis, or PCA, is a dimensionality- reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. • Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. • Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. • So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. <<PAGE_3>> Step by Step Explanation of PCA • STEP 1: STANDARDIZATION • STEP 2: COVARIANCE MATRIX COMPUTATION • STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • STEP 4: FEATURE VECTOR • STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES <<PAGE_4>> STEP 1: STANDARDIZATION (MEAN = 0 & S.D = 1) • The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. • More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. • That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. • So, transforming the data to comparable scales can prevent this problem. • Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable. <<PAGE_5>> Contd… Once the standardization is done, all the variables will be transformed to the same scale. <<PAGE_6>> STEP 2: COVARIANCE MATRIX COMPUTATION • The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or • In other words, to see if there is any relationship between them. • Because sometimes, variables are highly correlated in such a way that they contain redundant information. • So, in order to identify these correlations, we compute the covariance matrix. <<PAGE_7>> COVARIANCE MATRIX • The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. • For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: <<PAGE_8>> Contd… • Since the covariance of a variable with itself is its variance (Cov(a , a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. • And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. • So , Covariance Matrix is a Symmetric Matrix. <<PAGE_9>> What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? • It’s actually the sign of the covariance that matters : • if positive then : the two variables increase or decrease together (correlated) • if negative then : One increases when the other decreases (Inversely correlated) • Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step. <<PAGE_10>> STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS • Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data. • Before getting to the explanation of these concepts, let’s first understand what do we mean by principal components. • Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. • These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. <<PAGE_11>> Scree Plot • So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below. <<PAGE_12>> Contd… • Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. • An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables. • Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. • The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. • To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible. <<PAGE_13>> How PCA Constructs the Principal Components? • As there are as many principal components as there are variables in the data, • Principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. • For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? • Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. • Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). <<PAGE_14>> Scatter plot <<PAGE_15>> Contd… • The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance. • This continues until a total of p principal components have been calculated, equal to the original number of variables. • Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. <<PAGE_16>> Eigenvectors and Eigenvalues • What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. • And their number is equal to the number of dimensions of the data. • For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. • It is eigenvectors and eigenvalues who are behind all the magic explained above, • Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. • And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. • By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. <<PAGE_17>> Example: • Let’s suppose that our data set is 2-dimensional with 2 variables x , y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: • If we rank the eigenvalues in descending order, we get λ1>λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. <<PAGE_18>> Percentage of PC • After having the principal components, to compute the percentage of variance (information) accounted for by each component, • we divide the eigenvalue of each component by the sum of eigenvalues. • If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. <<PAGE_19>> STEP 4: FEATURE VECTOR • As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. • In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector. • So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. • This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions. <<PAGE_20>> Example: • Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors v1 and v2: • Or discard the eigenvector v2, which is the one of lesser significance, and form a feature vector with v1 only: • Discarding the eigenvector v2 will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set • But given that v2 was carrying only 4% of the information, the loss will be therefore not important and we will still have 96% of the information that is carried by v1. <<PAGE_21>> Contd… • So, as we saw in the example, it’s up to you to choose whether to keep all the components or discard the ones of lesser significance, depending on what you are looking for. • Because if you just want to describe your data in terms of new variables (principal components) that are uncorrelated without seeking to reduce dimensionality, leaving out lesser significant components is not needed. • <<PAGE_22>> STEP 5: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES • In the previous steps, apart from standardization, you do not make any changes on the data, you just select the principal components and form the feature vector, but the input data set remains always in terms of the original axes (i.e., in terms of the initial variables). • In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). • This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.",
    "chunk_size": 500,
    "overlap": 100,
    "chunks": 30
  }
}